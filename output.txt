[0.00s - 87.12s]  Aujourd'hui, on reçoit une légende de la tech française, Arthur Mench, cofondateur de Mistral AI, la seule entreprise d'Europe capable de tenir tête à OpenAI et aux GAFAM dans leur course à l'intelligence artificielle. En à peine un an, lui et ses deux associés ont réussi l'impossible, lever plus d'un milliard d'euros, développer des modèles d'IA qui rivalisent avec Chagipity et transformer leur start-up parisienne en une entreprise valorisée 6 milliards d'euros. Dans cet épisode exceptionnel, Arthur va nous dévoiler les secrets de cette success story, comment trois Français ont quitté leur job en or chez Google et Meta pour se lancer dans cette aventure folle, comment ils rivalisent avec des géants qui ont 100 fois plus de puissance de calcul qu'eux et surtout la guerre des talents qui fait rage en coulisses entre Mistral et les GAFAM pour attirer les meilleurs ingénieurs. On va aussi lui demander si d'après lui les modèles d'IA ont atteint un plafond et qu'est-ce qu'ils nous réservent pour la suite. Je suis très excité et honoré de pouvoir vous partager cette conversation avec Arthur Mech. Mais juste avant, j'ai un message pour tous ceux qui hésitent à prendre un abonnement chat GPT. Notre partenaire du jour, Mammouth Ayaï eu une idée assez brillante. Rassembler tous les meilleurs modèles d'IA dans une seule interface et derrière un unique abonnement. Pour 10 euros par mois, vous avez accès aux modèles de langages les plus récents, O1, Grock, DeepSync, et même des modèles de génération d'images comme Midjournay ou Flux. Quand on sait que cumuler tous ces abonnements, ça coûterait dans les 80-100 euros, c'est assez imbattable. Si vous avez besoin de générer beaucoup d'images par mois, ils ont aussi des plans
[87.12s - 88.12s]  un peu plus chers.
[88.12s - 90.12s]  Le truc cool, c'est qu'ils sont toujours à l'affût des nouvelles sorties.
[90.12s - 92.62s]  Par exemple, ils ont déjà flux pour la génération d'images.
[92.62s - 95.76s]  Et globalement, c'est juste agréable de ne pas avoir à changer d'interface tout
[95.76s - 96.76s]  le temps.
[96.76s - 98.76s]  Je vous mets le lien vers leurs différentes formules en description et on reprend.
[98.76s - 102.14s]  C'est quoi l'élément déclencheur déjà pour se dire « on va créer notre propre
[102.14s - 105.34s]  société face à ces géants » quand on est déjà bien installé.

[0.00s - 0.50s]  confortable.

[0.00s - 5.94s]  Je pense qu'il y a deux conversations, une en septembre 2022 avec Timothée et une en novembre 2022 à NeurIPS,
[5.94s - 9.48s]  qui est la grosse conférence de machine learning avec Guillaume,
[9.84s - 13.94s]  où on s'est rendu compte qu'on avait des aspirations similaires de lancer une entreprise...

[0.00s - 3.08s]  en France et qu'on connaissait pas mal de gens que ça intéresserait.

[0.00s - 3.12s]  Et donc à partir de là, c'est un peu le début de l'engrenage.
[3.24s - 5.64s]  C'est-à-dire qu'au début, tu te dis « Ah, c'est peut-être une bonne idée ».
[5.64s - 7.74s]  Et puis au fur et à mesure, chaque jour qui passe,
[8.00s - 10.30s]  tu t'impliques de plus en plus émotionnellement dans cette idée.
[10.74s - 12.38s]  Puis à un moment donné, tu as un peu un point de non-retour
[12.38s - 15.92s]  parce qu'en fait, tu es plus dans l'idée que dans le travail dans ton entreprise actuelle.

[0.00s - 6.04s]  Mais à partir de février, c'est vrai qu'on s'est dit, là on peut avoir 15 personnes, on peut aller vite, on sait le faire,
[6.34s - 11.20s]  on peut démontrer que l'Europe peut faire des choses intéressantes dans le domaine et peut reprendre une position de leadership.
[11.66s - 15.26s]  Et donc c'est comme ça que ça s'est fait, et à partir d'avril, on s'est lancé, quoi.

[0.00s - 5.22s]  Donc, Tau, il y a déjà cette idée que le projet, c'est de faire de l'IA très performante européenne

[0.00s - 0.96s]  plus ça que

[0.00s - 7.40s]  Juste, on se sent un petit peu peut-être ralenti par une grosse structure au-dessus de nous, donc Meta ou Google, et on pense aller plus vite tout seul. C'était quoi ?

[0.00s - 6.44s]  Non, tu avais les deux. En fait, Guillaume, Timothée et moi, on travaillait sur ce sujet depuis à peu près 2020.
[6.74s - 10.38s]  Et on a vu ce qu'on pouvait faire avec des petites équipes très concentrées.
[10.90s - 16.48s]  C'est vrai qu'en 2022, ces équipes sont devenues moins concentrées parce que c'est le moment où le monde a réalisé
[16.48s - 19.54s]  qu'il y avait une opportunité économique autour des modèles de langue.
[19.54s - 26.30s]  Et donc, on s'est dit qu'on pouvait bénéficier aussi de cet aspect de désorganisation pour nous être mieux organisés
[26.30s - 27.66s]  et fournir des choses plus rapidement.

[0.00s - 1.14s]  Ça se passe comment le tout début ?

[0.00s - 3.34s]  Vous avez chacun une spécialité ?
[3.34s - 5.28s]  Comment vous vous organisez au tout début de la boîte ?

[0.00s - 1.32s]  On vient tous les trois du même...

[0.00s - 0.42s]  Même formation.

[0.00s - 3.50s]  De la même formation, on a fait la même chose, on a tous les trois des thèses en machine learning.
[4.24s - 9.56s]  C'est vrai qu'on s'est rapidement spécialisé avec Guillaume, qui est le scientifique le plus fort d'entre nous,
[9.62s - 11.08s]  qui a pris la partie scientifique.
[11.72s - 15.16s]  Timothée, qui est plus un ingénieur et qui s'est occupé de faire toute l'infrastructure
[15.16s - 18.76s]  et de monter l'équipe d'ingénieurs produits aussi.
[19.38s - 25.44s]  Et moi, j'ai assez vite fait l'aspect lever de fond, l'aspect parler avec des clients...

[0.00s - 4.44s]  C'est des choses que j'aimais bien faire, on s'est répartis comme ça assez vite.
[5.00s - 7.86s]  Et pour revenir à comment ça démarre, ça démarre par une levée de fonds,
[8.02s - 13.00s]  parce qu'il faut la capacité de calcul et il faut la capacité humaine pour avancer assez vite.
[13.44s - 16.28s]  Et donc on a fait une levée de fonds en quelques semaines,
[16.78s - 20.14s]  et à partir de là on était partis pour faire le premier modèle en septembre.

[0.00s - 2.36s]  C'est-à-dire qu'il n'y a pas une seule ligne de code, en fait ?

[0.00s - 2.76s]  Avant même de savoir que c'est bon, il y a une levée de fonds qui va se faire.
[3.04s - 5.58s]  C'est un domaine où il faut forcément attendre la levée de fonds.
[5.64s - 7.16s]  Si on veut commencer la première...

[0.00s - 2.10s]  On peut un peu paralléliser, commencer à faire du code.

[0.00s - 0.92s]  un peu paralléliser ?

[0.00s - 1.10s]  À trois, tu n'as pas beaucoup de levier.
[1.32s - 2.80s]  Il vaut mieux avoir une petite équipe
[2.80s - 4.94s]  d'une dizaine de personnes pour aller plus vite.
[5.40s - 7.28s]  On a commencé par la data, parce qu'il faut la donner
[7.28s - 8.40s]  pour entraîner les modèles.
[8.62s - 10.34s]  Il y a beaucoup de travail manuel là-dessus.

[0.00s - 3.16s]  à faire et Guillaume, Timothée essentiellement avaient commencé

[0.00s - 1.08s]  pendant qu'on finissait la levée.

[0.00s - 0.16s]  OK.

[0.00s - 1.40s]  on parle des levées de fonds.
[1.48s - 4.40s]  Toi, tu as fait Polytechnique, Centra de Paris, l'UNS et un doctorat.
[4.84s - 7.10s]  Est-ce que ça aide quand même à lever des fonds
[7.10s - 8.72s]  alors que vous n'êtes que trois ?
[8.72s - 12.06s]  Ou alors, est-ce que c'est encore plus le non-méta Google ?
[12.06s - 13.40s]  Tu dirais que c'est quoi qui aide le plus ?

[0.00s - 6.36s]  Je pense que ce qui a aidé au démarrage, c'est qu'on était crédibles sur le domaine le plus chaud du moment en 2023
[6.36s - 11.50s]  et qu'on avait plutôt des papiers qui étaient liés à ce domaine-là.
[11.66s - 14.10s]  C'est-à-dire que moi, j'étais dans l'équipe qui travaillait à DeepMind là-dessus.
[14.64s - 17.18s]  Guillaume et Timothée, ils étaient à Meta, c'est eux qui ont fait de la mâle le premier.
[18.06s - 22.30s]  Et donc cette crédibilité-là, ce n'est pas ce qu'on a fait dans notre jeunesse à l'école.
[22.30s - 26.56s]  C'est plutôt une crédibilité scientifique qu'on a construite dans notre première partie de carrière, on va dire.

[0.00s - 2.98s]  Un alignement de planète avec ce qui intéresse le plus
[2.98s - 5.08s]  et les meilleures personnes pour le développer, en fait.

[0.00s - 5.78s]  Oui, effectivement, notre crédibilité venait aussi du fait qu'on avait une excellente équipe de démarrage
[5.78s - 9.68s]  et qu'on pouvait démontrer qu'on saurait la recruter.

[0.00s - 3.20s]  Et il y a ce jour qui arrive, c'est le 27 septembre 2023,
[3.42s - 7.46s]  où vous postez un lien sur votre compte Twitter parfaitement inactif,
[7.58s - 8.84s]  et c'est votre premier modèle en fait.
[9.18s - 10.78s]  Donc Mistral 7B.

[0.00s - 1.98s]  Le tweet est vu plus d'un million de fois.
[2.12s - 3.84s]  Vous êtes repris par tous les médias américains,
[3.94s - 4.68s]  tout le monde de l'IA.

[0.00s - 2.00s]  et en effervescence et s'amuse avec le modèle.
[2.00s - 4.00s]  Il est téléchargé un million de fois, mais super vite.

[0.00s - 1.14s]  Nous, on a vu ça de l'extérieur.

[0.00s - 1.54s]  on a vu cet engouement. Vous,

[0.00s - 1.28s]  De l'intérieur, c'était comment ?

[0.00s - 45.48s]  Le tweet, c'était une idée de Guillaume, le chief scientist, pour rendre à César ce qui appartient à César. Parce que vous ne le publiez pas comme les autres. On ne le publie pas comme les autres. Effectivement, on a mis à disposition un magnet link qui permet de le télécharger en BitTorrent. C'est comme ça qu'on a parlé la première fois et c'était une excellente idée. C'était une journée où on avait aussi prévu de faire de la communication plus habituelle. Moi, j'étais allé parler aux journalistes Figaro etc et donc il fallait mettre le torrent le matin et puis l'embargo était vers 16h donc il y avait cette période où en fait on avait rompu l'embargo mais bon les journalistes a priori allaient pas comprendre ce qui se passait donc ça se passait bien c'est moi qui postais, je crois c'était à 5h du mat donc j'avais mis un petit réveil parce que j'étais pas sûr du schedule send de Twitter
[45.48s - 46.94s]  ça s'appelait encore Twitter à l'époque
[46.94s - 48.20s]  et je l'ai mis
[48.20s - 49.30s]  et après je suis allé me recoucher
[49.30s - 50.12s]  et ensuite on a vu
[50.12s - 51.32s]  que ça avait bien démarré au démarrage
[51.32s - 52.08s]  c'est un truc où
[52.08s - 53.76s]  vous vous y attendiez un petit peu
[53.76s - 54.22s]  ou quand

[0.00s - 1.74s]  On savait que le modèle était bon
[1.74s - 4.68s]  On savait qu'on était largement au-dessus
[4.68s - 6.58s]  Des meilleurs modèles open source
[6.58s - 10.18s]  Qu'on avait visé explicitement cette taille-là
[10.18s - 11.76s]  Parce qu'on savait que ça tournait sur des laptops aussi
[11.76s - 13.88s]  Donc ça voulait dire que
[13.88s - 15.84s]  Tous les hobbyistes allaient pouvoir jouer avec
[15.84s - 17.30s]  Et ça n'a pas manqué, ça a fonctionné
[17.30s - 20.24s]  Donc on se doutait qu'on allait être remarqué
[20.24s - 21.56s]  Ce qu'on ne se doutait pas
[21.56s - 23.46s]  C'est que les gens allaient le mettre dans des perroquies en peluche
[23.46s - 25.64s]  Et ce genre de choses en un mois
[25.64s - 28.00s]  La réception était plus grande que ce qu'on espérait
[28.00s - 28.74s]  On était très contents

[0.06s - 0.82s]  Il y a un autre truc
[0.82s - 1.96s]  qui s'est passé nécessairement
[1.96s - 3.12s]  en publiant des modèles
[3.12s - 4.24s]  avec des poids ouverts comme ça.
[4.36s - 5.76s]  C'est que ça laisse la porte
[5.76s - 6.98s]  à tout ce qui est
[6.98s - 8.46s]  entraînement, fine tuning.
[8.84s - 9.68s]  Et tout le monde
[9.68s - 10.42s]  s'en est donné à cœur joie.
[10.50s - 11.90s]  Je pense que c'était déjà
[11.90s - 12.54s]  un peu le cas
[12.54s - 13.40s]  sur les modèles de l'Yama.
[13.60s - 14.42s]  Mais je me souviens
[14.42s - 14.82s]  que c'est un modèle
[14.82s - 15.60s]  qui a été beaucoup,
[15.72s - 16.50s]  beaucoup réentraîné.
[16.72s - 17.28s]  C'est quoi les...

[0.00s - 0.42s]  Les...

[0.00s - 0.48s]  fine-tuning
[0.48s - 1.66s]  un peu étonnant
[1.66s - 2.60s]  ou curieux
[2.60s - 3.14s]  dont tu te souviens
[3.14s - 3.70s]  de ce modèle-là
[3.70s - 4.08s]  ou d'autres ?
[4.08s - 4.78s]  Il y a quelqu'un
[4.78s - 4.88s]  qui s'est

[0.00s - 2.94s]  qui nous avait entraîné ce modèle pour parler aux morts.

[0.00s - 5.24s]  Je ne sais plus comment ça s'appelait mais il avait fait un fine tuning un petit peu
[5.24s - 10.40s]  ésotérique et le truc marchait relativement bien donc c'était assez marrant.
[10.40s - 14.64s]  C'est vrai que cette taille là c'est aussi une taille où tu peux fin tuner même sur
[14.64s - 18.60s]  des gros PC de gaming éventuellement et puis ça ne coûte pas très cher et ça permet
[18.60s - 22.96s]  de rentrer du style, ça permet de faire du roleplay et donc les gens s'en sont donné
[22.96s - 24.26s]  à coeur de joie effectivement.

[0.00s - 0.82s]  pour expliquer.

[0.00s - 3.66s]  Il y a le modèle de fondation qui est lui le plus coûteux et le plus compliqué.
[3.66s - 5.82s]  Et j'imagine qu'il contient en gros l'information.
[5.82s - 10.34s]  Et après, le fine tuning, c'est conversationnel, c'est en faire un bon agent de discussion.

[0.00s - 3.06s]  Oui, il faut voir la première phase comme une compression de la connaissance humaine,
[3.22s - 8.12s]  et la deuxième phase comme une manière d'instruire le modèle à suivre ce qu'on lui demande de faire.
[8.24s - 11.08s]  Donc on le rend contrôlable, et une manière de le contrôler, c'est de le rendre...

[0.00s - 0.94s]  de le rendre conversationnel.

[0.00s - 1.64s]  Donc ces deux phases-là sont assez distinctes, effectivement.

[0.00s - 0.98s]  Et est-ce que sur cette deuxième

[0.00s - 4.74s]  phase, il y a des trucs d'indépendants tout seuls qui ont testé des choses sur du fine
[4.74s - 7.06s]  tuning et ont découvert des bonnes personnes.

[0.00s - 4.48s]  technique ? Ouais, on a appris des trucs, je ne vais pas rentrer dans les détails, mais il y avait
[4.48s - 9.64s]  de direct preference optimization, c'est un peu du jargon, mais qu'on n'avait pas fait sur le
[9.64s - 13.30s]  premier modèle et on a vu des gens le faire, on s'est dit ça devrait bien marcher sur le deuxième
[13.30s - 16.22s]  modèle et ça a bien marché sur le deuxième modèle. Maintenant on fait d'autres choses,
[16.92s - 21.04s]  mais effectivement une des raisons pour lesquelles on a lancé la boîte au-delà de l'Europe, c'est
[21.04s - 25.24s]  aussi l'aspect ouvert et l'aspect contribution de la communauté. C'est

[0.00s - 7.82s]  En fait, l'EI entre 2012 et 2022, ça s'est construit les uns par-dessus les autres pendant les conférences, les grosses boîtes par-dessus les grosses boîtes.
[8.10s - 9.58s]  Puis soudain, quand c'est devenu...

[0.00s - 1.98s]  un modèle économique intéressant
[1.98s - 2.90s]  les gens ont arrêté
[2.90s - 4.08s]  les grosses entreprises
[4.08s - 4.30s]  ont arrêté
[4.30s - 5.52s]  et donc
[5.52s - 7.14s]  on a essayé de prolonger ça
[7.14s - 7.74s]  un peu
[7.74s - 8.76s]  avec ce qu'on a fait

[0.00s - 0.22s]  Réalisé.

[0.00s - 2.72s]  Oui, aujourd'hui, tu as vraiment deux camps distincts.
[2.72s - 9.02s]  C'est assez particulier sur, d'un côté, les Anthropik, OpenAI et compagnie qui ne publient plus grand-chose.
[9.14s - 11.82s]  Google aussi, j'ai l'impression, a beaucoup ralenti les publications.
[12.34s - 14.12s]  Et de l'autre côté, les Chinois, bizarrement.

[0.00s - 0.16s]  Donc...

[0.00s - 3.18s]  Pourquoi les Chinois, ils sont autant à fond dans les modèles open source ?
[3.18s - 3.86s]  C'est curieux, non ?

[0.00s - 1.40s]  Je pense qu'ils sont en position de challenger.
[2.06s - 4.86s]  Est-ce que l'open source, c'est une bonne stratégie de challenger ?
[4.86s - 7.70s]  On en est, je pense, la bonne illustration.
[8.10s - 9.66s]  Je pense qu'ils ont des bonnes techniques,
[9.78s - 11.78s]  ils ont des bons renseignements aussi, c'est vrai.

[0.00s - 3.52s]  Mais ils ont beaucoup fait avancer la science, les nouvelles techniques.
[3.72s - 4.98s]  C'est clairement ceux qui publient le plus, effectivement.

[0.00s - 3.36s]  Et tu parlais de la position de challenger, est-ce que Meta, quand il publie Yama pour la première fois,
[3.50s - 5.08s]  ils sont en position de challenger à ce moment-là ?

[0.00s - 1.22s]  C'est les Timothée et Guillaume.
[1.40s - 3.64s]  Je pense qu'ils sont dans la position de challenge parce qu'ils n'ont pas encore parlé.

[0.00s - 7.96s]  Et je pense qu'avec le mouvement qu'on a perpétué avec nos modèles en septembre et en décembre en particulier,
[8.26s - 12.74s]  donc Mistral 7B, Mistral 8X 7B, je pense qu'on a lancé cette route l'open source.
[12.90s - 17.00s]  Et donc il y a aussi un peu une concurrence sur qui fait les meilleurs modèles open source.
[17.08s - 18.20s]  Je pense que ça bénéficie à tout le monde.
[18.40s - 20.44s]  Et donc on est ravi d'avoir bien participé à ça.

[0.00s - 0.44s]  On va nous avoir

[0.00s - 1.22s]  Oui, c'est vrai.

[0.00s - 0.22s]  C'est...

[0.00s - 0.16s]  ...

[0.00s - 0.18s]  ...

[0.00s - 0.48s]  La régalade.
[0.68s - 2.32s]  Qu'est-ce qui fait, tu penses qu'à ce moment-là,
[2.58s - 3.94s]  vous avez autant d'avance, vous ?
[3.94s - 6.26s]  Après, il y a un yo-yo avec tout le monde qui se produit,
[6.74s - 9.10s]  mais là, il y a une vraie avance sociale.

[0.00s - 0.72s]  Indiscutable quoi.

[0.00s - 4.92s]  Je pense qu'on connaissait l'importance de la donnée et on a beaucoup travaillé là-dessus.
[5.48s - 12.00s]  On savait aussi comment entraîner le modèle de manière efficace parce qu'on avait trois
[12.00s - 13.62s]  ronds d'expérience chacun dans ce domaine.

[0.00s - 1.06s]  des bonnes connaissances
[1.06s - 1.90s]  et on a insisté
[1.90s - 3.04s]  sur les aspects
[3.04s - 3.50s]  de l'entraînement
[3.50s - 4.30s]  qui ont le plus de leviers
[4.30s - 4.90s]  c'est-à-dire la qualité
[4.90s - 5.30s]  de la donnée.

[0.00s - 1.90s]  Effectivement, c'est derrière un peu tout...

[0.00s - 2.50s]  l'évolution de la recherche, j'ai l'impression de temps vert.
[2.50s - 4.86s]  En fait, il n'y a que la donnée qui compte.

[0.00s - 0.92s]  Grosse partie.

[0.00s - 1.52s]  la donnée et la quantité de calcul.

[0.00s - 3.58s]  Il y a aussi le compute, et c'est lié à un autre sujet très important, c'est

[0.00s - 0.96s]  les fonds, tout simplement.
[1.52s - 3.10s]  En un an, vous avez en tout levé quand même
[3.10s - 5.76s]  un milliard d'euros, ce qui est vertigineux.
[5.92s - 7.54s]  Vous avez aussi sorti plein de nouveaux
[7.54s - 9.64s]  modèles, des Pixral, par exemple,
[9.76s - 11.38s]  des modèles un peu différents, multimodaux, etc.
[11.76s - 13.10s]  Comment vous approchez le fait que, justement,
[13.16s - 15.36s]  en termes de quantité de compute, par rapport
[15.36s - 16.54s]  à un méta, par exemple, qui a

[0.00s - 2.68s]  qui aura à la fin de l'année 350 000 H...

[0.00s - 0.82s]  100, c'est ça ?
[0.82s - 0.92s]  Oui.
[1.16s - 2.02s]  Si je ne dis pas de bêtises.
[2.06s - 2.32s]  En GPU.
[2.88s - 6.02s]  Est-ce que justement, il n'y a pas le choix que de passer par des très grosses levées de fonds ?

[0.00s - 3.30s]  Mais après, comme on pérennise le truc, c'est quoi un peu ta vision du compute ?

[0.00s - 3.98s]  Nous, notre vision, c'est qu'on a besoin de compute, mais on n'a pas besoin de 350 000 à champs.

[0.00s - 10.86s]  Et donc ça a été toujours notre test qu'on pouvait être plus efficace, qu'on pouvait en étant focalisé sur le fait de faire des excellents produits et ne pas faire plein d'autres choses à côté.
[11.02s - 14.74s]  Parce que nos concurrents américains ont tendance à faire beaucoup de choses à côté.
[15.06s - 17.98s]  L'allocation de ressources, c'est une question importante.

[0.00s - 0.84s]  constante chez nous.
[1.02s - 1.30s]  C'est un peu le nerf
[1.30s - 1.82s]  de l'aïr quoi.
[2.08s - 3.82s]  C'est arriver à tenir
[3.82s - 4.80s]  l'amélioration des modèles
[4.80s - 5.28s]  dans le temps
[5.28s - 7.04s]  versus le cramage
[7.04s - 8.08s]  du compute.

[0.00s - 4.16s]  Il faut gérer le budget, il faut être intelligent pour ne pas dépenser trop.
[5.98s - 9.74s]  Tout est une question de mettre le curseur au bon endroit et d'avoir les bons compromis.
[10.04s - 12.58s]  Ce n'est pas facile, mais je pense que pour le moment, on a bien réussi.
[12.90s - 14.90s]  On a réussi à avoir des modèles qui sont très performants,
[15.42s - 18.80s]  avec un niveau de dépense de capital qui est quand même très contrôlé.

[0.00s - 0.10s]  Ouais.

[0.00s - 4.26s]  Est-ce que justement, j'ai vu que parmi vos investisseurs

[0.00s - 2.04s]  dans les derniers rounds, je crois, il y a NVIDIA.
[2.68s - 5.54s]  Est-ce que ça passe par des acteurs qui, eux,
[5.68s - 8.38s]  ont un peu le contrôle sur le hardware,
[8.70s - 10.98s]  ou l'infrastructure, ou les data centers ?
[10.98s - 12.62s]  Il y a Microsoft aussi, je crois, avec qui vous avez bossé.
[12.94s - 14.20s]  Est-ce que ça passe aussi par ça, justement,
[14.30s - 15.56s]  s'entourer des bonnes personnes ?

[0.00s - 0.68s]  Faut les bons partners

[0.00s - 6.48s]  Il faut les bons partenaires de distribution en particulier parce que le calcul passe souvent par le cloud.
[7.00s - 12.52s]  Et donc on a comme partenaire tous les fournisseurs de cloud américains parce que c'est quand même les plus gros.
[12.82s - 15.84s]  On a aussi des fournisseurs français, on a Outscale qui en travaille.
[16.32s - 18.32s]  Et puis Nvidia c'est un fournisseur quasiment de cloud aussi.
[18.58s - 19.76s]  Donc à ce titre on travaille avec eux.
[20.04s - 23.56s]  On a fait de la R&D avec un modèle qui s'appelle Mistral Nemo.

[0.00s - 1.12s]  Imaginez, il y a des gens qui nous

[0.00s - 3.30s]  écoutes qui n'ont pas su. Est-ce que tu peux nous expliquer c'est quoi aujourd'hui la

[0.00s - 1.50s]  des modèles qui sont à jour.
[1.50s - 3.50s]  Moi, j'ai vu que dans les derniers mises à jour récents,
[3.50s - 4.38s]  il y a le large 2.

[0.00s - 3.82s]  Oui, alors maintenant on les numérote comme Ubuntu, donc 24.11.
[4.94s - 11.24s]  Et donc celui-là, Mistral Large 24.11, il est très fort pour appeler des fonctions, orchestrer des choses.
[11.58s - 14.76s]  Parce qu'en fait les modèles ça génère du texte, c'est l'utilisation de base.
[15.32s - 19.60s]  Mais ce qui est intéressant c'est quand ils génèrent des appels à des outils et qu'on les utilise comme des orchestrateurs,
[20.00s - 22.70s]  comme un peu des operating systems.
[23.20s - 27.90s]  Et donc on travaille beaucoup sur le fait d'avoir des modèles qui puissent être connectés à plein d'outils différents

[0.00s - 2.24s]  auxquelles on peut poser des questions, auxquelles on peut donner des tâches
[2.24s - 4.10s]  et qui vont réfléchir d'eux-mêmes aux outils qu'ils vont appeler.
[4.46s - 6.18s]  Et donc on insiste beaucoup là-dessus.
[6.28s - 8.92s]  Et donc la nouvelle version de Mistral Large, elle est particulièrement forte là-dessus.

[0.00s - 0.14s]  Oui.

[0.00s - 0.82s]  Après, il y a...

[0.00s - 1.16s]  Il y a eu des mixtrales aussi.
[1.68s - 3.78s]  Ça, pour comprendre, c'est plutôt pour servir,
[4.00s - 4.78s]  pour une entreprise, par exemple,
[4.86s - 6.52s]  pour servir beaucoup d'utilisateurs en même temps ?

[0.00s - 4.96s]  C'est un autre type d'architecture qui est particulièrement pertinent quand on a une forte charge.
[5.52s - 8.46s]  Beaucoup d'utilisateurs, c'est des choses que nous on utilise par exemple.
[8.98s - 9.94s]  C'est des mixtures.
[10.58s - 11.28s]  C'est des mixtures.
[11.28s - 12.54s]  Une sorte de cerbère à 8 têtes.
[12.72s - 18.94s]  C'est ça, c'est plusieurs modèles en même temps et chaque mot passe sur le modèle le plus adapté.
[18.94s - 21.70s]  Pour plusieurs raisons, ça permet de mieux utiliser les GPUs.

[0.00s - 0.44s]  En ligne

[0.00s - 0.54s]  parce qu'en fait

[0.00s - 0.48s]  sur le cerveau.

[0.00s - 0.94s]  Et derrière, il y a les plus petits.
[1.54s - 5.34s]  Il y a les petits modèles qui passent sur les laptops, qui passent sur les smartphones.
[5.78s - 9.38s]  Et ceux-là, ils sont particulièrement adaptés à des usages de hobbyistes
[9.38s - 12.62s]  parce qu'il n'y a pas besoin d'aller dans le cloud, on peut les modifier facilement.
[13.10s - 16.64s]  Et puis, très vite, c'est aussi pas mal focalisé sur cet aspect petit et rapide
[16.64s - 18.90s]  parce que c'est vraiment l'ADN de l'entreprise.
[19.26s - 21.06s]  Nous, aujourd'hui, le produit, ce n'est pas le modèle.
[21.06s - 23.60s]  Le produit, c'est la plateforme pour les développeurs.

[0.00s - 0.06s]  ...

[0.00s - 1.56s]  Donc le produit, c'est

[0.00s - 2.22s]  la plateforme pour construire des applications en tant que développeur,
[2.22s - 3.20s]  et là-dedans il y a des modèles.

[0.00s - 2.34s]  et puis un ensemble d'applications

[0.00s - 1.66s]  qui permettent de gagner en productivité.
[1.92s - 2.90s]  C'est un milieu qui est super

[0.00s - 0.56s]  compétitif.

[0.00s - 2.86s]  Évidemment, que ce soit, on l'a dit, sur les modèles,
[2.94s - 4.58s]  mais aussi sur tout ce qu'il y a autour,
[4.78s - 7.86s]  sur comment améliorer l'expérience, les interfaces de chat, etc.
[8.60s - 11.16s]  On a vu les systèmes d'interface qui se bougent.
[11.24s - 13.72s]  Tout le monde est un peu en train d'essayer de trouver les meilleures solutions à ça,
[13.86s - 17.24s]  Anthropic, OpenAI, et vous, évidemment, en tant qu'outsider,
[17.24s - 22.06s]  c'est quoi votre cible précise à vous en termes de possibilités d'évolution,
[22.52s - 25.04s]  quand on a des aussi gros acteurs à côté ?
[25.04s - 27.86s]  C'est quoi, toi, tu penses, la direction où vous avez un edge ?

[0.00s - 4.60s]  On a un fort edge dans le fait de découpler la question de l'infrastructure, de la question de l'interface.
[5.24s - 7.10s]  Notre solution peut être déployée partout.
[7.48s - 11.16s]  Elle peut être déployée dans le cloud, mais elle peut être déployée chez les entreprises qui ne sont pas dans le cloud.
[11.92s - 14.38s]  Elle peut être déployée sur des laptops.

[0.00s - 25.00s]  C'est le edge qu'on a construit aussi au-dessus de l'aspect open source, que ça va assez bien avec. Que les poids des modèles soient accessibles, ça rend facile leur déploiement n'importe où. On a cet aspect portabilité qui est très important. C'est notre première différenciation qu'on a beaucoup utilisée cette année. La différenciation qu'on cherche tous, c'est d'avoir la meilleure interface utilisateur. Il y a plein de sujets qui ne sont pas résolus. Le fait d'utiliser plein d'outils en même temps,
[25.12s - 27.52s]  le fait d'avoir des agents qui tournent pendant longtemps
[27.52s - 30.80s]  et qui prennent le feedback des utilisateurs.
[31.06s - 33.76s]  C'est-à-dire qu'on peut les voir comme des stagiaires,
[33.84s - 35.78s]  des stagiaires auxquels il faut faire du feedback
[35.78s - 37.62s]  pour qu'ils deviennent de plus en plus performants.
[38.20s - 41.48s]  Et donc, on va aller vers ce genre de système de plus en plus autonome
[41.48s - 43.12s]  qui vont avoir besoin de plus en plus de feedback
[43.12s - 46.94s]  pour passer de 80% de performance à 100% de performance.

[0.00s - 0.86s]  des stagiaires
[0.86s - 2.04s]  des stagiaires
[2.04s - 2.30s]  en qualité

[0.00s - 1.82s]  Comment ça se dit ? Tu ne restes pas constamment derrière lui
[1.82s - 3.24s]  et attendre qu'il avance ?

[0.00s - 5.34s]  Tu lui donnes une tâche, tu regardes ce qu'il a fait, tu lui dis ce qu'il n'a pas bien fait,

[0.00s - 1.60s]  J'espère que la prochaine fois, ils le fassent mieux.
[1.96s - 4.50s]  Mais en fait, c'est plein de questions scientifiques qu'il faut résoudre pour que ça fonctionne.
[4.84s - 5.54s]  Et d'interface.
[5.70s - 6.34s]  Et d'interface, oui.
[6.74s - 7.68s]  C'est pas du mail, en fait.

[0.00s - 1.28s]  C'est pas du mail en fait
[1.28s - 2.20s]  Est-ce qu'on va pas
[2.20s - 3.02s]  Pour l'instant
[3.02s - 3.64s]  C'est du chat
[3.64s - 4.54s]  En mode temps réel
[4.54s - 6.00s]  Est-ce qu'à terme
[6.00s - 6.86s]  On va envoyer un mail
[6.86s - 8.22s]  A notre assistante

[0.00s - 1.64s]  Et juste, il nous ping quand il a fini.

[0.00s - 2.18s]  C'est une des formes, je pense que c'est plutôt
[2.18s - 4.02s]  l'assistant qui t'envoie un mail, en fait, à un moment donné.
[4.16s - 6.32s]  Tu y travailles, et puis toutes les deux heures,
[6.42s - 7.66s]  il te dit, voilà où j'en suis, etc.
[8.08s - 9.52s]  Donc oui, il y a un aspect passé
[9.52s - 11.28s]  du synchrone à la synchrone,
[11.72s - 14.02s]  qui est très important, et qui pose plein de questions
[14.02s - 15.40s]  d'interface, parce que

[0.00s - 1.48s]  Bon, le mail, c'est peut-être pas la meilleure interface.
[1.72s - 4.28s]  Il y en a certainement d'autres qui sont plus intelligentes.
[4.64s - 6.72s]  La question de quelle est l'interface pour donner le feedback,
[6.90s - 9.26s]  quelle est l'interface pour sélectionner ce qui est préférable pour l'humain ?

[0.00s - 0.70s]  il y en a
[0.70s - 1.40s]  certainement d'autres

[0.00s - 0.74s]  ça c'est
[0.74s - 1.84s]  on y travaille

[0.00s - 0.12s]  Ouais.

[0.00s - 1.90s]  J'allais me dire, je suis persuadé que le...
[1.90s - 2.92s]  Enfin, je sais pas, mais...

[0.00s - 0.18s]  Oui.

[0.00s - 2.46s]  quand on regarde le chat, la discussion,

[0.00s - 3.52s]  Ce n'est pas forcément l'interface ultime pour dialoguer avec un LLM.

[0.00s - 1.16s]  Ça a beaucoup évolué maintenant.
[1.94s - 4.82s]  Tu peux te chatter avec le chat et il peut décider de te mettre dans un document.
[5.02s - 7.02s]  Et là, tu travailles avec lui sur la construction d'un document ?

[0.00s - 7.40s]  Tu peux lui demander d'aller chercher des sources et tu vois les sources, tu peux retourner, tu peux aller regarder ce que des humains ont écrit et demander des résumés par exemple.
[7.96s - 13.98s]  Et donc en fait ce que ça crée, ce que ça permet, les AI génératifs, c'est une espèce de liquidité de ta manière d'accéder à la connaissance.

[0.00s - 0.90s]  Donc tu peux regarder

[0.00s - 3.20s]  tout un site web et tu peux dire condense moi ce site web en

[0.00s - 1.14s]  en deux phrases

[0.00s - 1.84s]  Et je pense qu'il reste encore beaucoup de choses

[0.00s - 2.30s]  à faire pour que le modèle te permette d'apprendre beaucoup plus vite
[2.30s - 3.80s]  et de charger de la connaissance beaucoup plus vite.

[0.00s - 3.96s]  Je crois que c'est Vercel qui avait fait des démos assez marrantes de composants

[0.00s - 0.46s]  Web ?

[0.00s - 1.02s]  qui se construisait

[0.00s - 1.78s]  En fonction de la nécessité ?

[0.00s - 0.78s]  tu te poses une question,
[1.10s - 1.84s]  et il te générait,
[1.92s - 2.80s]  sur la météo par exemple,
[2.90s - 4.04s]  il te générait un composant
[4.04s - 6.04s]  d'interface graphique

[0.00s - 0.52s]  À la volée quoi !

[0.00s - 0.60s]  Ils voient le budget.
[0.74s - 1.14s]  Ouais, c'est ça.
[1.28s - 3.42s]  En fait, la question, c'est une question en back-end et en fontaine.
[3.58s - 6.72s]  En back-end, c'est quel outil appeler pour aller chercher de l'information
[6.72s - 7.92s]  ou pour actionner des choses ?

[0.00s - 2.78s]  Et en front-end, c'est quelle interface il faut montrer à l'utilisateur,
[2.88s - 4.00s]  étant donné son intention actuelle.

[0.00s - 3.24s]  Et ce que ça veut dire, c'est que les gros logiciels avec 50 000 boutons,
[3.24s - 5.88s]  je pense notamment au montage, ça va progressivement disparaître
[5.88s - 8.52s]  parce que tu peux identifier son état d'esprit

[0.00s - 2.14s]  au moment où il est en train de créer, et adapter les boutons,
[2.54s - 4.16s]  lui donner exactement ce dont il a besoin.

[0.00s - 3.98s]  Et donc ça change vraiment complètement la manière dont les interfaces vont se comporter dans les années qui viennent.

[0.00s - 3.04s]  Justement, on parlait de cette interface, de communication,

[0.00s - 3.68s]  comment on y accède. Tu parlais du fait que vous êtes déployable un peu de n'importe où.
[3.68s - 7.96s]  Il y a un truc que je constate en parlant avec des gens autour de moi, c'est qu'on a un peu une génération

[0.00s - 1.90s]  d'employés d'entreprise frustrés
[1.90s - 3.82s]  actuellement parce que chez eux, ils peuvent
[3.82s - 5.70s]  utiliser des trucs incroyables
[5.70s - 7.80s]  genre les meilleurs modèles
[7.80s - 9.64s]  disponibles, ils vont sur OpenAI, etc.
[10.16s - 11.82s]  Une fois au travail, on leur interdit
[11.82s - 13.94s]  souvent l'utilisation des meilleurs
[13.94s - 15.94s]  outils et parfois ils se retrouvent
[15.94s - 17.90s]  avec des versions un peu bridées ou des
[17.90s - 19.26s]  copilotes. Ou avec rien du tout.
[19.34s - 21.06s]  Ou avec, pour la plupart du temps, rien du tout.
[21.64s - 22.34s]  Ça vient d'où ça ?

[0.00s - 0.78s]  Ou avec rien du tout.

[0.00s - 1.20s]  Ça vient du fait que
[1.20s - 3.64s]  les systèmes d'AI génératifs,
[3.78s - 4.56s]  ça touche beaucoup aux données.
[4.88s - 5.74s]  Et les données dans les entreprises,
[5.88s - 6.60s]  c'est quand même assez important.
[6.98s - 8.90s]  Et donc, c'est là-dessus
[8.90s - 9.98s]  que nous, on a cherché
[9.98s - 10.62s]  à trouver des solutions.
[11.02s - 11.48s]  Alors, faire en sorte
[11.48s - 12.38s]  que les données, elles restent
[12.38s - 13.18s]  dans l'entreprise,
[13.58s - 15.64s]  que nous, en tant que fournisseurs d'AI,
[15.78s - 17.00s]  on n'est pas à voir ces données-là.
[17.30s - 18.38s]  Ça permet...

[0.00s - 3.96s]  justement d'avoir le niveau de sécurité, le niveau de gouvernance dont tu as besoin sur les données.
[4.32s - 5.78s]  Et donc progressivement...

[0.00s - 1.46s]  on va résoudre ce problème.

[0.00s - 2.50s]  Et nous, je dirais que c'est un des problèmes essentiels qu'on cherche à résoudre,
[2.56s - 3.26s]  de faire en sorte que...

[0.00s - 2.04s]  l'IT dans les entreprises soit confortable
[2.04s - 3.90s]  pour amener le char à tous leurs employés
[3.90s - 5.02s]  et qu'ils arrêtent d'être frustrés.

[0.00s - 1.96s]  les exemples d'outils que tu donnais,
[2.32s - 3.94s]  il y a un truc qui revient,
[4.10s - 6.10s]  qu'on n'a pas explicité, mais qui est
[6.10s - 7.54s]  super important, c'est le

[0.00s - 1.12s]  la notion d'objectif,
[1.34s - 3.04s]  d'avoir des modèles qui sont capables

[0.00s - 3.06s]  d'effectuer des tâches et sur la route, d'arriver à créer

[0.00s - 0.84s]  des étapes
[0.84s - 1.60s]  et à appeler
[1.60s - 2.40s]  les bons outils

[0.00s - 1.70s]  comme le ferait un bon stagiaire.
[2.02s - 4.82s]  Tu n'as pas nécessairement à lui expliquer l'ensemble des étapes qu'il doit faire.
[5.54s - 9.20s]  Tu lui dis « Regarde les prochains vols pour New York et prends-en un ».

[0.00s - 2.12s]  Tu n'as pas besoin de lui expliquer étape par étape,
[2.66s - 4.34s]  seconde par seconde, ce qu'il doit effectuer.

[0.00s - 2.54s]  On a des modèles qui peuvent commencer à appeler des outils,

[0.00s - 4.38s]  qu'on sent un peu limité dans leur capacité à en utiliser plusieurs d'affilés notamment.
[4.88s - 6.38s]  Des trucs vraiment utiles, vraiment stylés.
[6.54s - 8.32s]  Comment tu penses que ça va évoluer ?
[8.32s - 9.86s]  Est-ce que c'est une...

[0.00s - 2.32s]  une frontière qui peut être bientôt franchie ?
[2.32s - 4.00s]  Est-ce que l'année prochaine, on aura résolu ce problème
[4.00s - 6.36s]  et on pourra faire 20 étapes avec beaucoup de fiabilité ?
[6.36s - 7.86s]  Ou est-ce qu'on est encore loin d'y arriver ?

[0.00s - 1.08s]  Je pense que c'est la frontière.
[2.08s - 3.72s]  Tout le monde essaie de la pousser,
[3.92s - 5.16s]  ça ne va pas se débloquer d'un coup.
[5.40s - 7.92s]  Parce qu'en fait, maîtriser un outil,
[8.20s - 9.28s]  ça prend du temps à un humain,
[9.38s - 10.28s]  ça prend aussi du temps à un modèle.
[10.28s - 12.04s]  Il faut des démonstrations,
[12.74s - 13.36s]  il faut du feedback,
[13.48s - 15.16s]  parce que la première fois, il va se tromper.
[15.82s - 17.82s]  Et une notion d'expertise qu'il faut distiller
[17.82s - 19.24s]  de l'entreprise vers les systèmes d'AI.
[19.62s - 22.12s]  Et ça, ça ne va pas se faire de manière magique.
[22.54s - 24.32s]  Il faut tous les systèmes en place.
[24.70s - 25.50s]  Il faut les métasystèmes,
[25.62s - 27.60s]  c'est-à-dire qu'il faut que les employés
[27.60s - 29.16s]  dans les entreprises soient capables de...

[0.00s - 3.24s]  de fournir du signal supplémentaire au système D.I. pour qu'il s'améliore.
[3.50s - 6.88s]  Donc ça va progresser, on va avoir de plus en plus d'outils utilisables en même temps
[6.88s - 8.62s]  et des modèles qui peuvent résonner de plus en plus.

[0.00s - 1.92s]  ça va être progressif.
[2.18s - 3.74s]  Et pour que ça marche vraiment très bien,
[3.92s - 5.50s]  il faut y mettre du sien, il faut investir dès maintenant.

[0.00s - 4.50s]  Pour illustrer ça, on voit que OpenAI, dans leur dernier modèle, dans les O1 et compagnie,

[0.00s - 3.00s]  Ce ne sont plus des améliorations significatives sur le modèle en lui-même,
[3.00s - 5.84s]  mais il tente des trucs de le faire boucler sur lui-même,
[5.84s - 7.96s]  faire des chaînes de pensée, je ne sais pas comment on dit en français.
[7.96s - 9.10s]  Chaîne de pensée, oui.

[0.00s - 0.30s]  ...

[0.00s - 1.00s]  c'est pas bien non ?
[1.00s - 1.64s]  ah c'est gros ça

[0.00s - 2.46s]  Est-ce que, selon toi, c'est un peu un signe que...

[0.00s - 1.82s]  on a atteint une sorte de plafond.
[2.08s - 5.10s]  C'est-à-dire que, justement, sur cette évolution exponentielle,
[5.28s - 8.38s]  on a bien optimisé par rapport à leur taille,
[8.84s - 9.76s]  la manière dont marchent les modèles.
[9.88s - 11.32s]  Maintenant, justement, il faut trouver autre chose.

[0.00s - 2.80s]  C'est un paradigme qui est de plus en plus saturé,
[2.84s - 3.88s]  je pense qu'il n'est pas encore saturé,
[3.96s - 5.44s]  qui est ce qu'on appelle le préentraînement,
[5.54s - 6.88s]  donc la compression de la connaissance humaine.

[0.00s - 2.72s]  D'une certaine manière, tu as une connaissance disponible

[0.00s - 1.68s]  humaine qui a une certaine taille,
[1.74s - 2.00s]  et à un moment,
[2.04s - 2.86s]  tu as fini de la compresser.
[3.16s - 4.56s]  Et c'est là où il faut aller chercher...

[0.00s - 7.74s]  du signal supplémentaire. Du coup, chaîne de pensée, utilisation de plusieurs outils, utilisation de signal experts dans les entreprises.
[8.30s - 11.92s]  Donc, il n'y a pas de saturation du système. On sait comment aller à l'étape suivante.
[12.14s - 16.70s]  Mais sur l'aspect pré-entraînement, oui, on commence à savoir bien le faire collectivement.
[16.86s - 20.98s]  Tout le monde sait à peu près faire la même chose. Et donc, ce n'est plus tellement là où est la compétition.

[0.00s - 5.48s]  La compétition, elle est sur les interfaces et la compétition, elle est sur avoir des modèles qui tournent pendant plus longtemps.

[0.00s - 0.22s]  Ok.

[0.00s - 2.64s]  Je trouve que c'est un peu dur de se faire un avis dessus, justement, quand on est...

[0.00s - 2.88s]  On ne maîtrise pas la stack scientifique
[2.88s - 4.78s]  derrière les transformeurs et compagnie.

[0.00s - 3.00s]  J'ai l'impression qu'il y a un peu ce débat entre est-ce que c'est juste une question
[3.00s - 8.26s]  de compute, de données qui va repousser cette barrière d'autonomie ?

[0.00s - 4.40s]  Est-ce que c'est vraiment un problème intrinsèque à la manière dont le modèle est designé ?
[4.40s - 11.20s]  Et que juste le fait que ce soit de la prédiction du prochain token qui peut avoir un petit pourcentage de partir en cacahuètes à chaque fois,
[11.56s - 15.80s]  ça rend nécessairement trop compliqué, trop difficile la planification long terme.
[16.08s - 20.18s]  Je sais que par exemple, il y a des gens comme Yann Lecay, on en parle souvent, qui sont un peu défenseurs de cette vision-là,
[20.22s - 23.14s]  que l'AGI, ou je ne sais pas comment on l'appelle, elle est cachée encore derrière...

[0.00s - 1.16s]  des découvertes scientifiques.

[0.00s - 8.90s]  Oui, c'est une bonne question. Ce qui est vrai, c'est que travailler sur des architectures qui induisent des biais de réflexion humaine, c'est souvent utile.
[9.34s - 19.78s]  Ça a été utile pendant les 12 dernières années de se dire comment nous on réfléchit, essayons d'écrire ça en mathématiques et de faire en sorte que les modèles copient un peu ce qu'on sait faire.

[0.00s - 4.60s]  Ce qu'on observe aussi, c'est que toute l'intelligence qu'on peut mettre dans les architectures...

[0.00s - 2.06s]  Il suffit de mettre deux fois plus de compute et ça disparaît.
[2.50s - 5.60s]  Donc, en fait, le paradigme qu'on a suivi dans les cinq dernières années,
[5.70s - 7.98s]  c'est plutôt de se dire, prenez une architecture extrêmement simple
[7.98s - 9.50s]  qui prédit des séquences...

[0.00s - 3.16s]  et passons là à l'échelle, allons chercher le plus de données possibles,
[3.44s - 6.34s]  allons chercher les données multimodales, allons chercher de l'audio, ce genre de choses.

[0.00s - 0.70s]  Et...

[0.00s - 1.68s]  Et passons-la à l'échelle et voyons ce que ça donne.
[2.02s - 5.12s]  Et en fait, ce que ça donne, c'est que c'était en tout cas plus intelligent,
[5.42s - 8.24s]  en termes d'allocation de ressources, de travailler sur un passage à l'échelle
[8.24s - 10.32s]  que de travailler sur des architectures subtiles.

[0.00s - 2.00s]  Alors est-ce que c'est toujours le cas maintenant,
[2.00s - 4.84s]  comment ça va avoir saturé la quantité de données qu'on s'est compressé ?

[0.00s - 0.96s]  Je pense que la question est ouverte.

[0.00s - 1.76s]  le sujet, ce n'est plus tellement une question d'architecture,
[1.96s - 3.14s]  c'est plutôt une question d'orchestration.
[3.32s - 3.70s]  C'est-à-dire...

[0.00s - 2.28s]  Comment on fait pour que les modèles se rappellent eux-mêmes ?

[0.00s - 1.42s]  qu'ils interagissent avec des outils,
[1.54s - 5.86s]  qu'ils durent longtemps, qu'ils fassent du raisonnement en plusieurs étapes.

[0.00s - 2.66s]  Et ça, ça reste les mêmes modèles au fond.

[0.00s - 3.50s]  C'est la brique de base, mais le système complet, ce n'est pas juste le modèle.
[3.62s - 6.50s]  C'est le modèle qui sait se rappeler lui-même, qui sait réfléchir,
[6.58s - 9.36s]  qui sait interagir avec tout son environnement, qui sait interagir avec les humains.
[10.00s - 12.78s]  Donc la complexité des systèmes, elle devient beaucoup plus grande
[12.78s - 15.06s]  que juste un simple modèle de génération de séquences.

[0.00s - 2.58s]  Ça reste le moteur, mais c'est pas du tout toute la voiture.

[0.00s - 2.66s]  Mais donc, tu es plutôt optimiste sur le fait que ce soit le bon moteur.
[2.94s - 3.76s]  C'est le bon moteur.
[4.06s - 6.94s]  Après, il y a une règle en machine learning qui dit
[6.94s - 9.22s]  essentiellement, augmenter la capacité de calcul,
[9.38s - 12.38s]  ça augmente la qualité des systèmes.
[12.80s - 13.90s]  Et tu as deux solutions pour le faire.
[14.00s - 16.32s]  Soit tu compresses de la donnée, soit tu fais de la recherche.
[16.50s - 17.64s]  C'est-à-dire que tu vas échantillonner,
[18.06s - 19.84s]  tu vas demander au modèle de tester mille trucs
[19.84s - 22.26s]  et de sélectionner l'échantillon qui marche le mieux
[22.26s - 24.10s]  et tu vas le renforcer là-dessus.

[0.00s - 2.42s]  et donc là on commence de plus en plus à basculer
[2.42s - 4.24s]  dans le mode recherche plutôt que dans le mode compression
[4.24s - 5.88s]  la personne qui a dit ça c'est
[5.88s - 7.88s]  Richard Sutton dans un

[0.00s - 2.32s]  blog post que je vous invite à lire qui s'appelle The Bitter Lesson.

[0.00s - 1.82s]  Est-ce que toi, il y a une démo

[0.00s - 0.92s]  un peu de bout en bout,

[0.00s - 1.14s]  d'un truc qui a

[0.00s - 1.90s]  même si parfois ça ne marche pas,
[1.90s - 3.76s]  mais d'un truc où tu as été impressionné,
[3.76s - 5.48s]  où ça a vraiment très bien marché,

[0.00s - 4.86s]  d'une suite d'étapes, un truc qui t'a fait un peu sentir comme Iron Man avec Jarvis ?

[0.00s - 3.12s]  Oui, avec le chat, on a connecté les API ouvertes de Spotify.
[3.50s - 6.90s]  Et donc, tu peux lui parler, lui demander des playlists et décrire ta playlist.
[7.08s - 8.90s]  Puis, ça te crée ta playlist et ça la joue pour toi.
[9.12s - 10.46s]  Donc, ça fait des choses intéressantes.
[10.54s - 11.52s]  Alors, c'est juste un seul outil, ça.
[12.04s - 14.06s]  Non, là, on a vu des choses très intéressantes.
[14.24s - 18.00s]  C'est une fois qu'on a connecté le web, ça te permet d'avoir toutes les informations en live.

[0.00s - 4.60s]  Très vite, tu peux te créer tes mémos pour savoir ce qu'il faut aller dire à tel client en fonction des informations qu'il a eues.

[0.00s - 5.48s]  Et donc la combinaison des outils, ensemble, ça fait émerger des cas d'usage que tu n'avais pas forcément prévus.
[5.70s - 9.28s]  Si tu as connecté le web, si tu connectes ton mail, tu peux faire plein de choses en même temps.
[9.70s - 14.40s]  Et si tu connectes ta connaissance interne et le web, tu peux combiner ces informations.

[0.00s - 1.74s]  de manière un peu imprévisible ?

[0.00s - 2.32s]  Et donc la quantité de cas d'usage que tu couvres

[0.00s - 3.30s]  et assez exponentielle au nombre d'outils.

[0.00s - 1.26s]  Et donc ça, c'est assez magique, je veux dire.

[0.00s - 2.66s]  Moi effectivement je trouve qu'il y a un côté un peu vertigineux
[2.66s - 4.56s]  On va pouvoir construire
[4.56s - 5.08s]  Des trucs de fou

[0.00s - 3.92s]  Mais du coup, ça fait que c'est un peu dur de s'imaginer, de se dire ça va ressembler
[3.92s - 6.26s]  à quoi concrètement, genre le métier de développeur de

[0.00s - 3.50s]  de quelqu'un qui doit faire des scénarios de LLM, ça ressemble à quoi ?

[0.00s - 4.76s]  Je dirais que c'est un outil qui augmente le niveau d'abstraction requis par les humains.
[5.64s - 11.04s]  En tant que développeur, tu vas continuer à réfléchir aux problèmes que tu cherches à résoudre pour tes utilisateurs.
[11.48s - 18.32s]  Tu vas continuer à réfléchir aux architectures au niveau qui remplissent tes contraintes, ton cahier des charges.
[18.84s - 22.44s]  Après, est-ce que tu vas continuer à coder tes applications en JavaScript ?
[22.44s - 25.38s]  Vrasemblablement, non, parce que les modèles arrivent bien à générer...

[0.00s - 2.90s]  des applications simples et des applications de plus en plus compliquées.
[3.40s - 7.52s]  Donc tous les sujets très abstraits qui vont nécessiter de la communication avec des humains.
[7.84s - 9.60s]  Le métier d'ingénieur, c'est aussi un métier de communication.
[9.78s - 11.54s]  Il faut aussi comprendre quelles sont les contraintes de chacun.
[12.14s - 14.10s]  Ça, ça ne va pas être facilement remplaçable.

[0.00s - 4.50s]  Mais en revanche, tout l'aspect « je t'aide à faire tes tests unitaires »,
[4.50s - 8.16s]  « je te fais ton application Pixel Perfect », à partir d'un design,

[0.00s - 6.98s]  Je pense que ça devait devenir de plus en plus automatisable pour juste coller au développeur.

[0.00s - 0.88s]  pour tous les métiers.

[0.00s - 3.44s]  Est-ce qu'on a une intuition de comment ça se fait que les modèles sont aussi sensibles

[0.00s - 0.36s]  au code,
[0.72s - 1.30s]  c'est pour se dire ?

[0.00s - 2.44s]  Par exemple, je veux un modèle qui est super fort en français et en anglais.

[0.00s - 3.04s]  Bon, qu'ils sachent, le Python et le JavaScript, a priori, ça n'est pas utile.
[3.44s - 5.46s]  Or, ce n'est pas du tout ce qu'on observe, de ce que j'ai compris.

[0.00s - 1.18s]  C'est une très bonne question.
[27.64s - 28.38s]  C'est vrai qu'on observe un genre de transfert. Entraîner son modèle sur beaucoup de codes, ça lui permet de raisonner mieux. Je ne suis pas le mieux placé pour en parler, il faudrait que ça soit plutôt Guillaume, mais la réalité, c'est que le code, ça a plus d'informations que le langage. Il y a plus de réflexion qui est passée dans le langage, c'est plus structuré. S'entraîner à générer du code, ça force le modèle à raisononner à plus haut niveau que l'entraîner à générer du texte.
[29.64s - 31.66s]  Et donc, il sait résonner sur du code, et donc quand il voit du texte,
[31.72s - 32.90s]  il sait aussi résonner sur du texte.
[33.32s - 35.54s]  Et c'est vrai qu'il y a ce transfert un peu magique,
[36.00s - 37.66s]  qui est, je pense, une des raisons pour lesquelles les modèles sont devenus
[37.66s - 38.96s]  largement meilleurs dans les deux dernières années.
[39.24s - 40.62s]  Ça sert aussi parce que

[0.00s - 0.48s]  En fait, le...

[0.00s - 2.26s]  Tu as beaucoup plus de codebase
[2.26s - 4.22s]  qui sont plus longs qu'un livre.
[4.34s - 5.84s]  Comprendre une codebase, c'est plus long que lire un livre.

[0.00s - 2.96s]  et donc un peu le maximum sur lequel tu peux t'entraîner

[0.00s - 3.48s]  pour faire un modèle qui comprend le contexte long

[0.00s - 2.22s]  c'est des livres du 19e siècle

[0.00s - 8.34s]  Et le maximum sur lequel tu peux t'entraîner pour faire du code, c'est des millions de lignes de projets open source.
[8.80s - 11.54s]  Et donc, c'est plus long et ton modèle peut résonner plus longtemps.

[0.00s - 1.04s]  C'est des millions de lignes.
[1.94s - 2.40s]  Oui, c'est ça.

[0.00s - 1.50s]  je pense que c'est une des intuitions

[0.00s - 1.90s]  Je te propose de parler maintenant un petit peu de

[0.00s - 0.36s]  T'allons.

[0.00s - 3.20s]  et des gens qui font que, justement, vous faites ce que vous faites.
[3.40s - 6.92s]  Déjà, pourquoi est-ce que vous avez décidé, à la base, de mettre Mistral à Paris ?

[0.00s - 0.78s]  Aujourd'hui, ça peut pas...

[0.00s - 1.40s]  paraître un peu plus évident,
[1.48s - 3.90s]  on sait que l'écosystème est super vivant,
[3.96s - 4.80s]  on va en parler,
[5.50s - 6.78s]  mais est-ce que c'était une décision,
[7.10s - 9.62s]  à cette époque-là, une décision évidente
[9.62s - 11.40s]  entre ce maître-là ou à San Francisco,
[11.78s - 14.54s]  même avec une boîte française ?

[0.00s - 2.12s]  On ne s'est même pas posé la question en réalité.
[2.40s - 4.58s]  Moi, je n'avais aucune envie de partir de Paris.
[4.86s - 7.36s]  Ma compagne est fonctionnaire, donc elle a quelques contraintes.
[7.84s - 11.50s]  Timothée, il n'avait aucune envie de partir de France aussi, et Guillaume non plus.
[12.00s - 16.56s]  Donc je pense qu'en réalité, si j'ai réfléchi, je crois qu'on ne s'est jamais posé la question.
[16.98s - 19.78s]  On savait aussi que les gens dans notre réseau, en fait, c'était des Parisiens.
[20.18s - 21.42s]  Des Parisiens, des gens à Londres aussi.
[21.72s - 24.10s]  Ces personnes qu'on pouvait recruter, c'était des gens locaux.
[24.34s - 26.70s]  Donc c'était une évidence de démarrer à Paris.
[27.30s - 27.50s]  Comment ça se fait ?

[0.00s - 0.24s]  que...

[0.00s - 0.26s]  Bye.

[0.00s - 4.66s]  Marie, particulièrement, fourmille autant de bons ingés et scientifiques ?

[0.00s - 1.94s]  Je pense parce qu'il y a un écosystème en machine learning.
[2.14s - 5.40s]  Il y a un écosystème avec l'INRIA, avec...

[0.00s - 2.74s]  l'académique d'un côté, et puis les laboratoires privés
[2.74s - 4.38s]  que Yann Lequin a contribué à créer.
[4.72s - 8.94s]  Un laboratoire FAIR, historiquement, qui a été créé en 2015, je pense.

[0.00s - 8.18s]  T'as DeepMind qui en réaction s'était installé là-bas et qui avait un énorme centre de compétences à Londres grâce à DeepMind.
[8.66s - 15.12s]  Et donc en fait ce qui fait les talents dans la tech c'est le fait que t'es une entreprise qui soit déjà passée par là avant, qui a grossi.

[0.00s - 3.88s]  et des gens qui ont appris dans cette entreprise là et qui ensuite essaiment.

[0.00s - 6.28s]  Et donc on a bénéficié de ça, on a bénéficié des entrepreneurs qui voulaient bien nous rejoindre et qui se sont formés là-bas.

[0.00s - 3.56s]  Très bon écosystème privé et très bon écosystème public aussi,
[4.02s - 8.78s]  parce qu'il y a beaucoup de nos chercheurs qui ont fait des thèses dans l'académique en France aussi.

[0.00s - 3.32s]  le résultat est assez dingue quand même parce que dans les conventions d'IA,
[3.52s - 7.52s]  j'ai vu Oliyama par exemple qui a organisé un meetup ou un truc comme ça et tu vois les images,

[0.00s - 1.86s]  On se dit pas que ça apparaît, quoi.
[1.98s - 3.70s]  Et en fait, ça bouge de fou, quoi.

[0.00s - 2.34s]  Il y a plein de gens dans ce domaine en particulier qui sont des Français.
[2.88s - 4.48s]  Yann Lequin est un Français, mais...

[0.00s - 26.62s]  En dessous des gens plus jeunes que lui, il y a beaucoup de gens qui ont fait... On a fait tout le mal, Sialon par exemple. Oui, par exemple. À Meta, Paris, il y a aussi des gens très forts. À DeepMind, Paris, il y a des... Je ne vais pas encore aussi débaucher pour l'instant. J'ai plein d'amis... Je pense qu'on a débauché les gens qu'on pouvait. Ah, c'est terminé, assez ! J'ai plein d'amis qui sont français et qui sont encore là-bas. Ils finiront peut-être par créer leur boîte. De manière générale, l'Europe et la France en particulier a les bonnes compétences.
[26.62s - 30.62s]  Il y a les bonnes écoles, il faut être fort en maths et fort en informatique pour faire

[0.00s - 0.28s]  C'est un truc de russes.

[0.00s - 0.44s]  Je suis plein d'eux.

[0.00s - 0.70s]  C'est plus cool.

[0.00s - 1.74s]  Ah c'est un idée assez

[0.00s - 2.28s]  scientifiques en intelligence artificielle
[2.28s - 4.40s]  et de fait on a les bons tuyaux
[4.40s - 4.80s]  de formation

[0.00s - 1.38s]  Et c'est quoi les arguments aujourd'hui ?
[1.38s - 3.60s]  Moi, je viens de sortir de ma thèse en ML.
[4.34s - 5.84s]  Qu'est-ce qui fait, d'après toi,
[6.28s - 8.00s]  que je vais plutôt décider d'avenir chez Mistral
[8.00s - 11.36s]  versus chez Meta ou chez DeepMind ?

[0.00s - 0.12s]  ...

[0.00s - 3.68s]  En sortant de master, à Paris, je pense qu'on est de loin le meilleur centre de formation.

[0.00s - 3.40s]  pour faire le cœur de la science en intelligence artificielle.

[0.00s - 2.24s]  Il n'y a pas de structure équivalente, même en Europe.

[0.00s - 6.00s]  sur ce qu'on fait. Il y a 50-60 scientifiques chez nous qui sont tous extrêmement bien formés.
[6.52s - 11.62s]  Et donc, en sortant de Master, je trouve que c'est le meilleur endroit parce qu'en six mois, ils sont formés.
[11.92s - 14.92s]  C'est aussi un excellent endroit pour nous parce qu'on va récupérer des gens juniors.
[15.24s - 18.62s]  On a une bonne marque avec eux, on leur propose une excellente formation.

[0.00s - 2.02s]  Ils nous rejoignent et en fait, ils sont très performants au bout de 6 mois.
[2.30s - 6.24s]  Et ils ont plein d'idées, ils sont extrêmement créatifs, ils ont 23 ans, ils ont toute l'énergie qu'il faut.
[6.72s - 11.92s]  Et donc, on les utilise, on doit beaucoup de ce qu'on a proposé à des gens qui sont très jeunes.

[0.00s - 2.18s]  et donc on encourage tous les gens en master
[2.18s - 4.34s]  à venir nous rejoindre, on a plein de places
[4.34s - 5.18s]  et on adore les former

[0.00s - 4.92s]  J'ai appris tout à l'heure d'ailleurs qu'après notre émission qu'on avait faite sur Mistral, il y a eu une recrue.
[5.22s - 7.06s]  Tu te souviens ? Tu ne sais pas quel genre de profil ?
[7.06s - 8.06s]  Si, si, si, c'est un ingénieur.

[0.00s - 0.66s]  Ce n'est pas un certain...

[0.00s - 1.14s]  tu fixer un ingénieur.
[1.16s - 1.60s]  Si tout va bien,
[1.72s - 2.02s]  on n'a pas un peu.
[2.02s - 2.96s]  Oui, très content.
[3.18s - 3.70s]  Ok, parfait.
[3.96s - 4.82s]  Tu conseillerais quoi, toi,
[4.94s - 6.16s]  à des gens qui sont peut-être
[6.16s - 7.22s]  moins avancés
[7.22s - 8.78s]  ou qui réfléchissent
[8.78s - 9.78s]  à ce qu'ils veulent faire,
[9.86s - 10.40s]  qui voient bien
[10.40s - 10.94s]  qu'il y a quelque chose
[10.94s - 12.32s]  qui se passe dans l'IA ?
[12.32s - 13.30s]  Est-ce que, d'après toi,
[13.42s - 14.62s]  c'est trop tard, entre guillemets,
[14.68s - 15.42s]  dans le sens où...

[0.00s - 0.52s]  Ça va ?

[0.00s - 0.98s]  C'est quoi ?

[0.00s - 2.70s]  ça va être un écosystème très saturé ?
[2.70s - 4.14s]  Ou est-ce que tu penses que c'est encore possible ?
[4.14s - 7.26s]  Et qu'est-ce que tu conseillerais comme choix d'études,
[7.36s - 8.46s]  de trucs à explorer ?
[8.46s - 10.40s]  Est-ce qu'il faut faire des modèles de langage comme tout le monde ?
[10.40s - 11.30s]  Est-ce que justement...

[0.00s - 2.34s]  il faut essayer de sortir un petit peu de la bulle.
[2.72s - 3.26s]  C'est quand on...

[0.00s - 0.10s]  de

[0.00s - 2.52s]  Je pense qu'il faut réfléchir aux systèmes qu'on crée avec.
[25.96s - 28.46s]  Nous, on crée des systèmes de gestion de la connaissance, mais il y a plein de systèmes verticalisés dans les sciences de la vie, dans l'architecture, dans la conception par ordinateur, qui ne demandent qu'à être pris avec des nouveaux systèmes. Le montage vidéo aussi. Et donc, se dire que la technologie va continuer à avancer, les technologies horizontales vont être de plus en plus performantes. Ça ouvre de nouvelles opportunités d'automatisation, de nouvelles opportunités
[28.46s - 29.80s]  de création de logiciels
[29.80s - 32.02s]  intéressants, de création de services
[32.02s - 34.14s]  complètement différents pour les utilisateurs.
[34.48s - 35.94s]  Et donc partir des besoins d'utilisateurs,
[36.06s - 36.58s]  partir de...

[0.00s - 2.38s]  un peu du rêve du logiciel de demain,
[2.98s - 4.72s]  la machine auquel on va parler
[4.72s - 6.92s]  et qui va faire tout ce qu'on veut à sa place.
[7.44s - 8.54s]  Je pense que c'est un bon point de départ ?

[0.00s - 4.16s]  on choisit sa verticale et puis on se dit est-ce que les modèles aujourd'hui peuvent résoudre ce problème
[4.44s - 8.48s]  s'ils peuvent pas résoudre ce problème peut-être qu'il faut les personnaliser peut-être qu'il faut aller un petit peu plus profondément
[8.76s - 15.14s]  et là dessus on peut aider on peut aider à le faire on a tous les outils qui permettent de faire les personnalisations verticalisées. Nous on n'a pas

[0.00s - 3.24s]  On a parlé avec Microsoft des conséquences que ça avait dans la recherche de matériaux,
[3.24s - 4.24s]  par exemple.
[4.24s - 7.80s]  Et tu te dis, mais c'est quoi la base ? On parle de modèles qui génèrent des images,
[7.80s - 9.36s]  du texte, etc.

[0.00s - 1.72s]  Grâce à ça, entre autres, on découvre des matériaux.
[1.82s - 4.30s]  Est-ce qu'il y a des trucs comme ça, soit dans vos partenaires actuels ?

[0.00s - 2.86s]  ou dans les trucs que toi t'as vu qui t'ont aussi frappé ?

[0.00s - 2.08s]  La découverte de matériaux, c'est les mêmes techniques,
[2.20s - 2.88s]  ce n'est pas les mêmes modèles.
[3.18s - 4.78s]  Parce qu'en fait, ce qu'on fait, c'est qu'on séquence,
[4.90s - 6.22s]  de manière générale, il y a plein de problèmes
[6.22s - 9.58s]  qui se résolvent en séquençant, en sérialisme,
[9.58s - 11.72s]  de manière générale, le problème.
[12.04s - 14.10s]  Et la découverte de matériaux, c'est de la chimie,
[14.20s - 16.16s]  donc on peut sérialiser les molécules chimiques
[16.16s - 18.04s]  et demander au modèle de les prédire.
[18.34s - 21.62s]  Après, le paradigme, je prédis des séquences
[21.62s - 22.76s]  et j'ai plein de séquences sous la main
[22.76s - 24.66s]  de données particulières,
[25.10s - 27.66s]  ils marchent en chimie, ils marchent en médecine,

[0.00s - 1.70s]  Il marche certainement en sciences de la vie aussi.
[2.20s - 2.92s]  Non, on ne travaille pas là-dessus,
[3.02s - 5.48s]  mais on est ravis d'être partenaires
[5.48s - 7.24s]  avec des startups qui font ça, oui.

[0.00s - 4.68s]  Et là où, peut-être, comme tu disais, on arrive à une fin d'exponentiel éventuellement sur le texte,
[4.88s - 7.34s]  il y a peut-être plein d'autres domaines où, en fait...

[0.00s - 2.46s]  Le transfert, il fait que commencer quoi ?

[0.00s - 2.34s]  Sur le texte, on n'est pas du tout à saturation,
[2.48s - 6.44s]  puisque la prochaine étape, c'est d'avoir des modèles qui appellent plein d'outils.
[6.70s - 10.08s]  Du coup, ils deviennent beaucoup plus intelligents que juste des générateurs de texte.
[10.40s - 12.36s]  Et un des outils qu'ils peuvent appeler, c'est des simulateurs, par exemple.
[12.36s - 16.70s]  Donc la boucle de travail du concepteur de turbines,
[17.60s - 19.80s]  de la personne qui travaille sur un nouveau médicament,
[19.86s - 21.36s]  elle va complètement changer dans les années qui viennent.
[21.78s - 23.80s]  Et ça vient à la fois de modèles spécialisés,

[0.00s - 1.74s]  de prédiction de molécules par exemple,

[0.00s - 3.48s]  Mais ça vient aussi de l'interface avec la connaissance qu'on est en train de recréer.

[0.00s - 2.30s]  Tu vas pouvoir parler avec ton ordinateur et dire
[2.30s - 4.54s]  « Est-ce que tu peux me simuler une turbine qui ressemble à celle-là,
[4.60s - 6.24s]  mais un peu plus grande ? »

[0.00s - 1.48s]  et fais-moi plusieurs essais
[1.48s - 2.92s]  et dis-moi celle qui marcherait la mieux

[0.00s - 1.34s]  Et donc, la manière d'itérer,
[1.88s - 3.68s]  l'ingénierie, finalement, ça va aussi
[3.68s - 5.20s]  beaucoup changer de nature dans les années qui viennent, oui.

[0.00s - 2.94s]  C'est hyper intéressant parce qu'effectivement, les trucs auxquels on pense rapidement,
[3.16s - 5.32s]  c'est, tu parlais de simulateur, il y a le simulateur de code tout simplement.
[5.54s - 6.48s]  Ça, c'est des trucs qui se font un peu déjà.
[6.54s - 9.22s]  C'est le plus facile à simuler puisque ça reste au sein de la machine.
[9.66s - 10.54s]  Il n'y a pas de monde extérieur.
[11.00s - 12.56s]  Et donc, on voit déjà un peu le...

[0.00s - 2.20s]  une sandbox, par exemple, qui peut exécuter un bout de code
[2.20s - 3.92s]  et on voit si ça marche, si ça marche pas.
[4.20s - 4.84s]  Ça, c'est des trucs qui existent.

[0.00s - 3.62s]  Le raisonnement et le fait d'avoir un modèle qui génère du code qui est ensuite exécuté,
[3.86s - 5.12s]  on y travaille pour très prochainement.
[5.76s - 9.52s]  Et donc, c'est une grosse porte d'entrée vers des cas d'usage complexes.
[9.88s - 11.58s]  Tu as un espace ouvert d'outils à appeler maintenant,
[11.92s - 13.48s]  parce que tes outils, c'est tes librairies.
[13.86s - 15.78s]  Le niveau de contrôle aussi que tu dois mettre sur les modèles,
[16.22s - 18.68s]  ça devient plus compliqué à évaluer,
[18.80s - 20.24s]  parce que c'est plus ouvert, plus le monde est ouvert,
[20.38s - 21.36s]  plus c'est difficile à évaluer.
[21.68s - 22.58s]  Mais clairement, on va vers ça.
[22.88s - 24.98s]  Un modèle avec un exécuteur de code,
[25.20s - 27.74s]  c'est beaucoup plus performant qu'un modèle sans exécuteur de code.

[0.00s - 0.38s]  Dès qu'on crée,

[0.00s - 2.74s]  on commence à parler d'outils, d'exécuter des trucs du code.

[0.00s - 3.94s]  Moi, j'entends tout de suite d'autres profils dans l'écosystème de l'IA
[3.94s - 6.08s]  qui sont dans la frange très...

[0.00s - 1.90s]  très un peu alarmiste de

[0.00s - 1.66s]  de comment pourrait évoluer l'IA,
[2.14s - 5.28s]  et des possibilités que l'IA devienne rogue,
[5.78s - 6.86s]  et prenne son indépendance.
[7.00s - 8.34s]  Tu parles d'exécuter du code ?

[0.00s - 2.06s]  elle sortirait, on l'imagine sortir de la boîte.
[2.34s - 3.50s]  C'est quoi, toi, ton avis là-dessus ?

[0.00s - 1.92s]  Pour être caricatural, c'est un peu...

[0.00s - 2.94s]  un moyen de mettre en place des régulations pour...

[0.00s - 0.78s]  pour canasser tout ça,
[0.98s - 2.52s]  ou est-ce qu'il y a vraiment une des...

[0.00s - 0.88s]  des questions à se poser

[0.00s - 2.62s]  Il y a des questions à se poser sur comment on fait du nouveau logiciel
[2.62s - 5.26s]  avec des modèles qui sont fondamentalement...

[0.00s - 2.20s]  imprévisible, c'est-à-dire que par essence
[2.20s - 4.22s]  ce qu'il génère c'est aléatoire ?

[0.00s - 1.36s]  ça dépend de l'entrée

[0.00s - 2.42s]  Tu peux pas, en tant qu'humain, prédire ce que le modèle va sortir.

[0.00s - 1.24s]  Tu veux quand même faire du logiciel avec.
[1.90s - 3.20s]  Un logiciel, par essence,
[3.56s - 5.02s]  avant de le mettre à disposition du marché,
[5.12s - 7.02s]  tu veux vérifier qu'il couvre tous tes cas
[7.02s - 8.40s]  et qu'il n'y a pas de bug.
[8.92s - 11.62s]  Et donc la question de faire en sorte que ton système
[11.62s - 13.66s]  qui repose sur des LLM n'ait pas de bug,
[13.72s - 14.32s]  c'est une question difficile.

[0.00s - 2.00s]  C'est une question de contrôle, c'est une question d'évaluation.

[0.00s - 2.06s]  et finalement pour nous la question de sûreté

[0.00s - 1.20s]  en anglais c'est safety,
[1.72s - 3.00s]  c'est d'abord une question d'évaluation,
[3.52s - 5.08s]  c'est d'abord une question d'avoir les bons outils pour
[5.08s - 7.34s]  vérifier que ça fonctionne, et si ça
[7.34s - 9.00s]  fonctionne pas, avoir les bons outils pour
[9.00s - 11.04s]  corriger ça. Un des outils qu'on met à dispo,

[0.00s - 5.98s]  et qui est utile pour ça, c'est si tu veux contraindre l'espace des possibles de ton modèle et de ton système,
[6.38s - 8.08s]  tu contrains l'espace des entrées.

[0.00s - 3.84s]  Donc tu mets une modération sur le type de questions que l'utilisateur peut poser.
[4.10s - 5.48s]  Et comme ça, soudain tu passes de

[0.00s - 34.34s]  Un système qui peut répondre à toutes les questions, un système qui peut répondre que aux questions qui sont intéressantes pour le logiciel que tu crées. Pour nous, on voit vraiment l'aspect sûreté au niveau du système lui-même, et comme un problème d'intégration continue, comme un problème de test, il faut répondre à la question comment on construit des logiciels déterministes qui tournent sur des systèmes fondamentalement stochastiques. Ça, c'est la première chose. Ensuite, oui, il y a de la science-fiction, et puis, tu as quelques entreprises aux Etats-Unis qui ont un intérêt à dire aux régulateurs « Écoutez, cette technologie, elle est quand même un peu trop compliquée, un peu trop difficile à comprendre, un petit peu trop dangereuse.
[34.44s - 41.12s]  Imaginez que le truc devienne indépendant. » Bon, tu vas dire ça à des gens qui ne comprennent pas forcément exactement ce qui se passe. Ils peuvent se dire « Non, c'est pas mal. »

[0.00s - 3.36s]  Ah oui, peut-être que si on donnait ça à trois personnes...

[0.00s - 1.16s]  ou à deux personnes
[1.16s - 2.30s]  aux Etats-Unis
[2.30s - 3.14s]  on contrôlerait
[3.14s - 3.88s]  tout ce qui se passe
[3.88s - 4.76s]  et puis il n'y a pas de problème quoi

[0.00s - 1.72s]  Mais nous, on pense que c'est faux, c'est-à-dire que...

[0.00s - 4.34s]  avoir deux entités ou encore pire une entité qui contrôle tous
[5.04s - 9.80s]  tous les systèmes et qui ouvre sa porte à des auditeurs auxquels ils montrent ce qu'ils veulent
[9.96s - 12.48s]  on pense que c'est pas la bonne solution la bonne solution

[0.00s - 1.36s]  en sûreté logicielle.

[0.00s - 25.98s]  C'est l'open source de manière générale. On l'a montré en cyber, on l'a montré sur les systèmes les plus fiables aujourd'hui, les operating systems les plus fiables, c'est Linux. Le fait d'avoir le plus d'yeux possible sur une technologie, le fait de la distribuer le plus possible, c'est une manière de faire en sorte que le contrôle de cette technologie soit sous un contrôle démocratique. Et donc, nous c'est ce qu'on dit. Et puis quand on entend les doomers raconter autre chose, il y a des gens qui sont de bonne foi, il faut le reconnaître, qu'ils ont vraiment peur
[25.98s - 27.44s]  que ce sont des choses qui vont se passer.
[27.72s - 29.60s]  Et puis il y a surtout beaucoup de gens qui ne sont pas du tout de bonne foi.
[29.96s - 31.66s]  Je pense qu'il est important
[31.66s - 32.72s]  de vérifier

[0.00s - 15.82s]  Ça ne doit pas être simple parce qu'en face, l'argument, il est super facile à comprendre justement quand tu disais pour quelqu'un qui n'est pas forcément adepte du sujet, on te dit voilà un outil dangereux, est-ce qu'il ne faudrait pas éviter qu'on le mette dans trop de mains ? Tu pars déjà avec un truc un peu dur à défendre quoi.

[0.00s - 1.74s]  L'atout qu'on a, on a un atout historique.
[1.88s - 4.02s]  C'est-à-dire que ce n'est pas la première fois qu'on a ce débat.
[4.20s - 5.30s]  On a eu ce débat pour l'Internet.
[5.42s - 8.14s]  Internet, ça aurait pu être un truc contrôlé par trois entreprises
[8.14s - 9.86s]  qui auraient fait leur propre nette.

[0.00s - 0.72s]  Moi je trouve qu'on meurt.

[0.00s - 1.80s]  qui auraient refusé de standardiser les choses.
[2.28s - 4.60s]  Et en fait, finalement, il y a eu suffisamment de pression.
[5.08s - 6.72s]  À un moment donné, le régulateur s'est dit
[6.72s - 9.20s]  « Ouais, on va faire en sorte qu'elle soit standardisée. »
[9.20s - 10.72s]  Et donc, Internet, ça appartient à tout le monde maintenant.

[0.00s - 3.54s]  Il aurait suffi que des personnes différentes fassent des choix différents, quelques personnes,

[0.00s - 5.98s]  Et on serait dans une situation où, en fait, il y a trois wall gardens...

[0.00s - 0.94s]  non interopérables.

[0.00s - 2.60s]  Ça aurait pu être la même chose pour le end-to-end encryption.
[2.94s - 3.98s]  Ça, c'est un autre exemple.
[4.30s - 6.34s]  À une époque, c'était considéré comme une arme
[6.34s - 9.30s]  et il y avait un contrôle des exports des États-Unis.
[9.78s - 10.50s]  En fait, on est ce qu'il fallait.

[0.00s - 1.16s]  En fait, on se dit pas fou maintenant.

[0.00s - 2.08s]  Et en fait, on se pose la question maintenant sur les poids.
[2.68s - 5.52s]  Il y a parfois certains régulateurs qui se posent cette question-là.
[6.06s - 8.82s]  Mais ça paraît fou pour le end-to-end encryption.
[8.98s - 11.40s]  Nous, ce qu'on pense, c'est que dans dix ans, ça paraîtrait complètement fou pour des poids d'un modèle.
[11.76s - 16.20s]  Parce que c'est tellement infrastructurel, c'est tellement une ressource qui doit être partagée par tout le monde,
[16.56s - 18.68s]  cette compression de la connaissance et cette intelligence,
[19.18s - 22.98s]  que pour nous, c'est criminel de la laisser entre les mains de deux entités
[22.98s - 24.96s]  qui ne sont pas du tout sous contrôle démocratique.

[0.00s - 3.88s]  Et pour défendre cette vision-là que le contrôle doit avoir lieu un peu plus tard
[3.88s - 6.00s]  dans la chaîne, au moment de l'interface par exemple,
[6.00s - 8.50s]  ou par l'entreprise vis-à-vis de son client,

[0.00s - 0.98s]  Tu vas notamment au Sénat.

[0.00s - 3.48s]  On t'a vu sur YouTube parler au Sénat.
[3.98s - 4.94s]  Ça fait quoi de parler ?
[4.94s - 9.70s]  D'essayer d'expliquer ce que c'est un modèle, un dataset, un LLM à des sénateurs ?

[0.00s - 25.52s]  C'est intéressant. C'est un exercice. Il y avait des bonnes questions, peut-être posées par des gens qui comprenaient un peu moins la technologie, on va dire. Mais je pense que c'est important. De manière générale, c'est des représentants des citoyens et il faut qu'ils comprennent que c'est une technologie qui va affecter les citoyens. Donc nous, on est prêts à investir du temps parce que mieux c'est compris, plus on comprend que c'est aussi un enjeu de souveraineté, c'est aussi un enjeu culturel. C'est un enjeu d'avoir des acteurs comme nous,
[25.68s - 27.88s]  et pas que nous, mais des acteurs comme nous sur le sol européen.
[27.98s - 28.66s]  Parce que si ce n'est pas le cas,
[29.10s - 33.52s]  le sujet, c'est qu'on a une dépendance économique énorme aux États-Unis
[33.52s - 36.34s]  et ça, elle est très, très dommageable à long terme.

[0.00s - 10.50s]  Le fait d'aller parler aux gens qui font les lois, aux gens qui vont aussi parler avec leurs citoyens, comprennent leurs angoisses, c'est une manière de dédramatiser cette technologie.
[10.78s - 16.32s]  C'est une technologie qui va apporter beaucoup de bénéfices dans l'éducation, dans la santé, dans la manière dont on travaille.

[0.00s - 8.84s]  Et il faut que les représentants de la démocratie française, de la démocratie européenne, de la démocratie américaine, aient conscience, savent de quoi il s'agit.
[9.12s - 14.14s]  Moi, je dois dire que personnellement, je n'avais pas tellement prévu de faire ça en démarrant la boîte, mais il faut faire entendre savoir,
[14.30s - 20.66s]  parce que sinon, le vide est comblé par des gens qui n'ont pas forcément des intérêts alignés avec...

[0.00s - 3.26s]  la démocratie est certainement pas alignée avec ce que nous on essaie de faire.

[0.00s - 0.88s]  Si vous n'avez pas su,

[0.00s - 5.40s]  l'histoire du Vesuvius Challenge ou comment un papyrus a été décrypté par un étudiant en IA.
[5.40s - 7.60s]  Allez voir cette vidéo, c'était vraiment passionnant !

[0.00s - 87.12s]  Aujourd'hui, on reçoit une légende de la tech française, Arthur Mench, cofondateur de Mistral AI, la seule entreprise d'Europe capable de tenir tête à OpenAI et aux GAFAM dans leur course à l'intelligence artificielle. En à peine un an, lui et ses deux associés ont réussi l'impossible, lever plus d'un milliard d'euros, développer des modèles d'IA qui rivalisent avec Chagipity et transformer leur start-up parisienne en une entreprise valorisée 6 milliards d'euros. Dans cet épisode exceptionnel, Arthur va nous dévoiler les secrets de cette success story, comment trois Français ont quitté leur job en or chez Google et Meta pour se lancer dans cette aventure folle, comment ils rivalisent avec des géants qui ont 100 fois plus de puissance de calcul qu'eux et surtout la guerre des talents qui fait rage en coulisses entre Mistral et les GAFAM pour attirer les meilleurs ingénieurs. On va aussi lui demander si d'après lui les modèles d'IA ont atteint un plafond et qu'est-ce qu'ils nous réservent pour la suite. Je suis très excité et honoré de pouvoir vous partager cette conversation avec Arthur Mech. Mais juste avant, j'ai un message pour tous ceux qui hésitent à prendre un abonnement chat GPT. Notre partenaire du jour, Mammouth Ayaï eu une idée assez brillante. Rassembler tous les meilleurs modèles d'IA dans une seule interface et derrière un unique abonnement. Pour 10 euros par mois, vous avez accès aux modèles de langages les plus récents, O1, Grock, DeepSync, et même des modèles de génération d'images comme Midjournay ou Flux. Quand on sait que cumuler tous ces abonnements, ça coûterait dans les 80-100 euros, c'est assez imbattable. Si vous avez besoin de générer beaucoup d'images par mois, ils ont aussi des plans
[87.12s - 88.12s]  un peu plus chers.
[88.12s - 90.12s]  Le truc cool, c'est qu'ils sont toujours à l'affût des nouvelles sorties.
[90.12s - 92.62s]  Par exemple, ils ont déjà flux pour la génération d'images.
[92.62s - 95.76s]  Et globalement, c'est juste agréable de ne pas avoir à changer d'interface tout
[95.76s - 96.76s]  le temps.
[96.76s - 98.76s]  Je vous mets le lien vers leurs différentes formules en description et on reprend.
[98.76s - 102.14s]  C'est quoi l'élément déclencheur déjà pour se dire « on va créer notre propre
[102.14s - 105.34s]  société face à ces géants » quand on est déjà bien installé.

[0.00s - 0.50s]  confortable.

[0.00s - 5.94s]  Je pense qu'il y a deux conversations, une en septembre 2022 avec Timothée et une en novembre 2022 à NeurIPS,
[5.94s - 9.48s]  qui est la grosse conférence de machine learning avec Guillaume,
[9.84s - 13.94s]  où on s'est rendu compte qu'on avait des aspirations similaires de lancer une entreprise...

[0.00s - 3.08s]  en France et qu'on connaissait pas mal de gens que ça intéresserait.

[0.00s - 3.12s]  Et donc à partir de là, c'est un peu le début de l'engrenage.
[3.24s - 5.64s]  C'est-à-dire qu'au début, tu te dis « Ah, c'est peut-être une bonne idée ».
[5.64s - 7.74s]  Et puis au fur et à mesure, chaque jour qui passe,
[8.00s - 10.30s]  tu t'impliques de plus en plus émotionnellement dans cette idée.
[10.74s - 12.38s]  Puis à un moment donné, tu as un peu un point de non-retour
[12.38s - 15.92s]  parce qu'en fait, tu es plus dans l'idée que dans le travail dans ton entreprise actuelle.

[0.00s - 6.04s]  Mais à partir de février, c'est vrai qu'on s'est dit, là on peut avoir 15 personnes, on peut aller vite, on sait le faire,
[6.34s - 11.20s]  on peut démontrer que l'Europe peut faire des choses intéressantes dans le domaine et peut reprendre une position de leadership.
[11.66s - 15.26s]  Et donc c'est comme ça que ça s'est fait, et à partir d'avril, on s'est lancé, quoi.

[0.00s - 5.22s]  Donc, Tau, il y a déjà cette idée que le projet, c'est de faire de l'IA très performante européenne

[0.00s - 0.96s]  plus ça que

[0.00s - 7.40s]  Juste, on se sent un petit peu peut-être ralenti par une grosse structure au-dessus de nous, donc Meta ou Google, et on pense aller plus vite tout seul. C'était quoi ?

[0.00s - 6.44s]  Non, tu avais les deux. En fait, Guillaume, Timothée et moi, on travaillait sur ce sujet depuis à peu près 2020.
[6.74s - 10.38s]  Et on a vu ce qu'on pouvait faire avec des petites équipes très concentrées.
[10.90s - 16.48s]  C'est vrai qu'en 2022, ces équipes sont devenues moins concentrées parce que c'est le moment où le monde a réalisé
[16.48s - 19.54s]  qu'il y avait une opportunité économique autour des modèles de langue.
[19.54s - 26.30s]  Et donc, on s'est dit qu'on pouvait bénéficier aussi de cet aspect de désorganisation pour nous être mieux organisés
[26.30s - 27.66s]  et fournir des choses plus rapidement.

[0.00s - 1.14s]  Ça se passe comment le tout début ?

[0.00s - 3.34s]  Vous avez chacun une spécialité ?
[3.34s - 5.28s]  Comment vous vous organisez au tout début de la boîte ?

[0.00s - 1.32s]  On vient tous les trois du même...

[0.00s - 0.42s]  Même formation.

[0.00s - 3.50s]  De la même formation, on a fait la même chose, on a tous les trois des thèses en machine learning.
[4.24s - 9.56s]  C'est vrai qu'on s'est rapidement spécialisé avec Guillaume, qui est le scientifique le plus fort d'entre nous,
[9.62s - 11.08s]  qui a pris la partie scientifique.
[11.72s - 15.16s]  Timothée, qui est plus un ingénieur et qui s'est occupé de faire toute l'infrastructure
[15.16s - 18.76s]  et de monter l'équipe d'ingénieurs produits aussi.
[19.38s - 25.44s]  Et moi, j'ai assez vite fait l'aspect lever de fond, l'aspect parler avec des clients...

[0.00s - 4.44s]  C'est des choses que j'aimais bien faire, on s'est répartis comme ça assez vite.
[5.00s - 7.86s]  Et pour revenir à comment ça démarre, ça démarre par une levée de fonds,
[8.02s - 13.00s]  parce qu'il faut la capacité de calcul et il faut la capacité humaine pour avancer assez vite.
[13.44s - 16.28s]  Et donc on a fait une levée de fonds en quelques semaines,
[16.78s - 20.14s]  et à partir de là on était partis pour faire le premier modèle en septembre.

[0.00s - 2.36s]  C'est-à-dire qu'il n'y a pas une seule ligne de code, en fait ?

[0.00s - 2.76s]  Avant même de savoir que c'est bon, il y a une levée de fonds qui va se faire.
[3.04s - 5.58s]  C'est un domaine où il faut forcément attendre la levée de fonds.
[5.64s - 7.16s]  Si on veut commencer la première...

[0.00s - 2.10s]  On peut un peu paralléliser, commencer à faire du code.

[0.00s - 0.92s]  un peu paralléliser ?

[0.00s - 1.10s]  À trois, tu n'as pas beaucoup de levier.
[1.32s - 2.80s]  Il vaut mieux avoir une petite équipe
[2.80s - 4.94s]  d'une dizaine de personnes pour aller plus vite.
[5.40s - 7.28s]  On a commencé par la data, parce qu'il faut la donner
[7.28s - 8.40s]  pour entraîner les modèles.
[8.62s - 10.34s]  Il y a beaucoup de travail manuel là-dessus.

[0.00s - 3.16s]  à faire et Guillaume, Timothée essentiellement avaient commencé

[0.00s - 1.08s]  pendant qu'on finissait la levée.

[0.00s - 0.16s]  OK.

[0.00s - 1.40s]  on parle des levées de fonds.
[1.48s - 4.40s]  Toi, tu as fait Polytechnique, Centra de Paris, l'UNS et un doctorat.
[4.84s - 7.10s]  Est-ce que ça aide quand même à lever des fonds
[7.10s - 8.72s]  alors que vous n'êtes que trois ?
[8.72s - 12.06s]  Ou alors, est-ce que c'est encore plus le non-méta Google ?
[12.06s - 13.40s]  Tu dirais que c'est quoi qui aide le plus ?

[0.00s - 6.36s]  Je pense que ce qui a aidé au démarrage, c'est qu'on était crédibles sur le domaine le plus chaud du moment en 2023
[6.36s - 11.50s]  et qu'on avait plutôt des papiers qui étaient liés à ce domaine-là.
[11.66s - 14.10s]  C'est-à-dire que moi, j'étais dans l'équipe qui travaillait à DeepMind là-dessus.
[14.64s - 17.18s]  Guillaume et Timothée, ils étaient à Meta, c'est eux qui ont fait de la mâle le premier.
[18.06s - 22.30s]  Et donc cette crédibilité-là, ce n'est pas ce qu'on a fait dans notre jeunesse à l'école.
[22.30s - 26.56s]  C'est plutôt une crédibilité scientifique qu'on a construite dans notre première partie de carrière, on va dire.

[0.00s - 2.98s]  Un alignement de planète avec ce qui intéresse le plus
[2.98s - 5.08s]  et les meilleures personnes pour le développer, en fait.

[0.00s - 5.78s]  Oui, effectivement, notre crédibilité venait aussi du fait qu'on avait une excellente équipe de démarrage
[5.78s - 9.68s]  et qu'on pouvait démontrer qu'on saurait la recruter.

[0.00s - 3.20s]  Et il y a ce jour qui arrive, c'est le 27 septembre 2023,
[3.42s - 7.46s]  où vous postez un lien sur votre compte Twitter parfaitement inactif,
[7.58s - 8.84s]  et c'est votre premier modèle en fait.
[9.18s - 10.78s]  Donc Mistral 7B.

[0.00s - 1.98s]  Le tweet est vu plus d'un million de fois.
[2.12s - 3.84s]  Vous êtes repris par tous les médias américains,
[3.94s - 4.68s]  tout le monde de l'IA.

[0.00s - 2.00s]  et en effervescence et s'amuse avec le modèle.
[2.00s - 4.00s]  Il est téléchargé un million de fois, mais super vite.

[0.00s - 1.14s]  Nous, on a vu ça de l'extérieur.

[0.00s - 1.54s]  on a vu cet engouement. Vous,

[0.00s - 1.28s]  De l'intérieur, c'était comment ?

[0.00s - 45.48s]  Le tweet, c'était une idée de Guillaume, le chief scientist, pour rendre à César ce qui appartient à César. Parce que vous ne le publiez pas comme les autres. On ne le publie pas comme les autres. Effectivement, on a mis à disposition un magnet link qui permet de le télécharger en BitTorrent. C'est comme ça qu'on a parlé la première fois et c'était une excellente idée. C'était une journée où on avait aussi prévu de faire de la communication plus habituelle. Moi, j'étais allé parler aux journalistes Figaro etc et donc il fallait mettre le torrent le matin et puis l'embargo était vers 16h donc il y avait cette période où en fait on avait rompu l'embargo mais bon les journalistes a priori allaient pas comprendre ce qui se passait donc ça se passait bien c'est moi qui postais, je crois c'était à 5h du mat donc j'avais mis un petit réveil parce que j'étais pas sûr du schedule send de Twitter
[45.48s - 46.94s]  ça s'appelait encore Twitter à l'époque
[46.94s - 48.20s]  et je l'ai mis
[48.20s - 49.30s]  et après je suis allé me recoucher
[49.30s - 50.12s]  et ensuite on a vu
[50.12s - 51.32s]  que ça avait bien démarré au démarrage
[51.32s - 52.08s]  c'est un truc où
[52.08s - 53.76s]  vous vous y attendiez un petit peu
[53.76s - 54.22s]  ou quand

[0.00s - 1.74s]  On savait que le modèle était bon
[1.74s - 4.68s]  On savait qu'on était largement au-dessus
[4.68s - 6.58s]  Des meilleurs modèles open source
[6.58s - 10.18s]  Qu'on avait visé explicitement cette taille-là
[10.18s - 11.76s]  Parce qu'on savait que ça tournait sur des laptops aussi
[11.76s - 13.88s]  Donc ça voulait dire que
[13.88s - 15.84s]  Tous les hobbyistes allaient pouvoir jouer avec
[15.84s - 17.30s]  Et ça n'a pas manqué, ça a fonctionné
[17.30s - 20.24s]  Donc on se doutait qu'on allait être remarqué
[20.24s - 21.56s]  Ce qu'on ne se doutait pas
[21.56s - 23.46s]  C'est que les gens allaient le mettre dans des perroquies en peluche
[23.46s - 25.64s]  Et ce genre de choses en un mois
[25.64s - 28.00s]  La réception était plus grande que ce qu'on espérait
[28.00s - 28.74s]  On était très contents

[0.06s - 0.82s]  Il y a un autre truc
[0.82s - 1.96s]  qui s'est passé nécessairement
[1.96s - 3.12s]  en publiant des modèles
[3.12s - 4.24s]  avec des poids ouverts comme ça.
[4.36s - 5.76s]  C'est que ça laisse la porte
[5.76s - 6.98s]  à tout ce qui est
[6.98s - 8.46s]  entraînement, fine tuning.
[8.84s - 9.68s]  Et tout le monde
[9.68s - 10.42s]  s'en est donné à cœur joie.
[10.50s - 11.90s]  Je pense que c'était déjà
[11.90s - 12.54s]  un peu le cas
[12.54s - 13.40s]  sur les modèles de l'Yama.
[13.60s - 14.42s]  Mais je me souviens
[14.42s - 14.82s]  que c'est un modèle
[14.82s - 15.60s]  qui a été beaucoup,
[15.72s - 16.50s]  beaucoup réentraîné.
[16.72s - 17.28s]  C'est quoi les...

[0.00s - 0.42s]  Les...

[0.00s - 0.48s]  fine-tuning
[0.48s - 1.66s]  un peu étonnant
[1.66s - 2.60s]  ou curieux
[2.60s - 3.14s]  dont tu te souviens
[3.14s - 3.70s]  de ce modèle-là
[3.70s - 4.08s]  ou d'autres ?
[4.08s - 4.78s]  Il y a quelqu'un
[4.78s - 4.88s]  qui s'est

[0.00s - 2.94s]  qui nous avait entraîné ce modèle pour parler aux morts.

[0.00s - 5.24s]  Je ne sais plus comment ça s'appelait mais il avait fait un fine tuning un petit peu
[5.24s - 10.40s]  ésotérique et le truc marchait relativement bien donc c'était assez marrant.
[10.40s - 14.64s]  C'est vrai que cette taille là c'est aussi une taille où tu peux fin tuner même sur
[14.64s - 18.60s]  des gros PC de gaming éventuellement et puis ça ne coûte pas très cher et ça permet
[18.60s - 22.96s]  de rentrer du style, ça permet de faire du roleplay et donc les gens s'en sont donné
[22.96s - 24.26s]  à coeur de joie effectivement.

[0.00s - 0.82s]  pour expliquer.

[0.00s - 3.66s]  Il y a le modèle de fondation qui est lui le plus coûteux et le plus compliqué.
[3.66s - 5.82s]  Et j'imagine qu'il contient en gros l'information.
[5.82s - 10.34s]  Et après, le fine tuning, c'est conversationnel, c'est en faire un bon agent de discussion.

[0.00s - 3.06s]  Oui, il faut voir la première phase comme une compression de la connaissance humaine,
[3.22s - 8.12s]  et la deuxième phase comme une manière d'instruire le modèle à suivre ce qu'on lui demande de faire.
[8.24s - 11.08s]  Donc on le rend contrôlable, et une manière de le contrôler, c'est de le rendre...

[0.00s - 0.94s]  de le rendre conversationnel.

[0.00s - 1.64s]  Donc ces deux phases-là sont assez distinctes, effectivement.

[0.00s - 0.98s]  Et est-ce que sur cette deuxième

[0.00s - 4.74s]  phase, il y a des trucs d'indépendants tout seuls qui ont testé des choses sur du fine
[4.74s - 7.06s]  tuning et ont découvert des bonnes personnes.

[0.00s - 4.48s]  technique ? Ouais, on a appris des trucs, je ne vais pas rentrer dans les détails, mais il y avait
[4.48s - 9.64s]  de direct preference optimization, c'est un peu du jargon, mais qu'on n'avait pas fait sur le
[9.64s - 13.30s]  premier modèle et on a vu des gens le faire, on s'est dit ça devrait bien marcher sur le deuxième
[13.30s - 16.22s]  modèle et ça a bien marché sur le deuxième modèle. Maintenant on fait d'autres choses,
[16.92s - 21.04s]  mais effectivement une des raisons pour lesquelles on a lancé la boîte au-delà de l'Europe, c'est
[21.04s - 25.24s]  aussi l'aspect ouvert et l'aspect contribution de la communauté. C'est

[0.00s - 7.82s]  En fait, l'EI entre 2012 et 2022, ça s'est construit les uns par-dessus les autres pendant les conférences, les grosses boîtes par-dessus les grosses boîtes.
[8.10s - 9.58s]  Puis soudain, quand c'est devenu...

[0.00s - 1.98s]  un modèle économique intéressant
[1.98s - 2.90s]  les gens ont arrêté
[2.90s - 4.08s]  les grosses entreprises
[4.08s - 4.30s]  ont arrêté
[4.30s - 5.52s]  et donc
[5.52s - 7.14s]  on a essayé de prolonger ça
[7.14s - 7.74s]  un peu
[7.74s - 8.76s]  avec ce qu'on a fait

[0.00s - 0.22s]  Réalisé.

[0.00s - 2.72s]  Oui, aujourd'hui, tu as vraiment deux camps distincts.
[2.72s - 9.02s]  C'est assez particulier sur, d'un côté, les Anthropik, OpenAI et compagnie qui ne publient plus grand-chose.
[9.14s - 11.82s]  Google aussi, j'ai l'impression, a beaucoup ralenti les publications.
[12.34s - 14.12s]  Et de l'autre côté, les Chinois, bizarrement.

[0.00s - 0.16s]  Donc...

[0.00s - 3.18s]  Pourquoi les Chinois, ils sont autant à fond dans les modèles open source ?
[3.18s - 3.86s]  C'est curieux, non ?

[0.00s - 1.40s]  Je pense qu'ils sont en position de challenger.
[2.06s - 4.86s]  Est-ce que l'open source, c'est une bonne stratégie de challenger ?
[4.86s - 7.70s]  On en est, je pense, la bonne illustration.
[8.10s - 9.66s]  Je pense qu'ils ont des bonnes techniques,
[9.78s - 11.78s]  ils ont des bons renseignements aussi, c'est vrai.

[0.00s - 3.52s]  Mais ils ont beaucoup fait avancer la science, les nouvelles techniques.
[3.72s - 4.98s]  C'est clairement ceux qui publient le plus, effectivement.

[0.00s - 3.36s]  Et tu parlais de la position de challenger, est-ce que Meta, quand il publie Yama pour la première fois,
[3.50s - 5.08s]  ils sont en position de challenger à ce moment-là ?

[0.00s - 1.22s]  C'est les Timothée et Guillaume.
[1.40s - 3.64s]  Je pense qu'ils sont dans la position de challenge parce qu'ils n'ont pas encore parlé.

[0.00s - 7.96s]  Et je pense qu'avec le mouvement qu'on a perpétué avec nos modèles en septembre et en décembre en particulier,
[8.26s - 12.74s]  donc Mistral 7B, Mistral 8X 7B, je pense qu'on a lancé cette route l'open source.
[12.90s - 17.00s]  Et donc il y a aussi un peu une concurrence sur qui fait les meilleurs modèles open source.
[17.08s - 18.20s]  Je pense que ça bénéficie à tout le monde.
[18.40s - 20.44s]  Et donc on est ravi d'avoir bien participé à ça.

[0.00s - 0.44s]  On va nous avoir

[0.00s - 1.22s]  Oui, c'est vrai.

[0.00s - 0.22s]  C'est...

[0.00s - 0.16s]  ...

[0.00s - 0.18s]  ...

[0.00s - 0.48s]  La régalade.
[0.68s - 2.32s]  Qu'est-ce qui fait, tu penses qu'à ce moment-là,
[2.58s - 3.94s]  vous avez autant d'avance, vous ?
[3.94s - 6.26s]  Après, il y a un yo-yo avec tout le monde qui se produit,
[6.74s - 9.10s]  mais là, il y a une vraie avance sociale.

[0.00s - 0.72s]  Indiscutable quoi.

[0.00s - 4.92s]  Je pense qu'on connaissait l'importance de la donnée et on a beaucoup travaillé là-dessus.
[5.48s - 12.00s]  On savait aussi comment entraîner le modèle de manière efficace parce qu'on avait trois
[12.00s - 13.62s]  ronds d'expérience chacun dans ce domaine.

[0.00s - 1.06s]  des bonnes connaissances
[1.06s - 1.90s]  et on a insisté
[1.90s - 3.04s]  sur les aspects
[3.04s - 3.50s]  de l'entraînement
[3.50s - 4.30s]  qui ont le plus de leviers
[4.30s - 4.90s]  c'est-à-dire la qualité
[4.90s - 5.30s]  de la donnée.

[0.00s - 1.90s]  Effectivement, c'est derrière un peu tout...

[0.00s - 2.50s]  l'évolution de la recherche, j'ai l'impression de temps vert.
[2.50s - 4.86s]  En fait, il n'y a que la donnée qui compte.

[0.00s - 0.92s]  Grosse partie.

[0.00s - 1.52s]  la donnée et la quantité de calcul.

[0.00s - 3.58s]  Il y a aussi le compute, et c'est lié à un autre sujet très important, c'est

[0.00s - 0.96s]  les fonds, tout simplement.
[1.52s - 3.10s]  En un an, vous avez en tout levé quand même
[3.10s - 5.76s]  un milliard d'euros, ce qui est vertigineux.
[5.92s - 7.54s]  Vous avez aussi sorti plein de nouveaux
[7.54s - 9.64s]  modèles, des Pixral, par exemple,
[9.76s - 11.38s]  des modèles un peu différents, multimodaux, etc.
[11.76s - 13.10s]  Comment vous approchez le fait que, justement,
[13.16s - 15.36s]  en termes de quantité de compute, par rapport
[15.36s - 16.54s]  à un méta, par exemple, qui a

[0.00s - 2.68s]  qui aura à la fin de l'année 350 000 H...

[0.00s - 0.82s]  100, c'est ça ?
[0.82s - 0.92s]  Oui.
[1.16s - 2.02s]  Si je ne dis pas de bêtises.
[2.06s - 2.32s]  En GPU.
[2.88s - 6.02s]  Est-ce que justement, il n'y a pas le choix que de passer par des très grosses levées de fonds ?

[0.00s - 3.30s]  Mais après, comme on pérennise le truc, c'est quoi un peu ta vision du compute ?

[0.00s - 3.98s]  Nous, notre vision, c'est qu'on a besoin de compute, mais on n'a pas besoin de 350 000 à champs.

[0.00s - 10.86s]  Et donc ça a été toujours notre test qu'on pouvait être plus efficace, qu'on pouvait en étant focalisé sur le fait de faire des excellents produits et ne pas faire plein d'autres choses à côté.
[11.02s - 14.74s]  Parce que nos concurrents américains ont tendance à faire beaucoup de choses à côté.
[15.06s - 17.98s]  L'allocation de ressources, c'est une question importante.

[0.00s - 0.84s]  constante chez nous.
[1.02s - 1.30s]  C'est un peu le nerf
[1.30s - 1.82s]  de l'aïr quoi.
[2.08s - 3.82s]  C'est arriver à tenir
[3.82s - 4.80s]  l'amélioration des modèles
[4.80s - 5.28s]  dans le temps
[5.28s - 7.04s]  versus le cramage
[7.04s - 8.08s]  du compute.

[0.00s - 4.16s]  Il faut gérer le budget, il faut être intelligent pour ne pas dépenser trop.
[5.98s - 9.74s]  Tout est une question de mettre le curseur au bon endroit et d'avoir les bons compromis.
[10.04s - 12.58s]  Ce n'est pas facile, mais je pense que pour le moment, on a bien réussi.
[12.90s - 14.90s]  On a réussi à avoir des modèles qui sont très performants,
[15.42s - 18.80s]  avec un niveau de dépense de capital qui est quand même très contrôlé.

[0.00s - 0.10s]  Ouais.

[0.00s - 4.26s]  Est-ce que justement, j'ai vu que parmi vos investisseurs

[0.00s - 2.04s]  dans les derniers rounds, je crois, il y a NVIDIA.
[2.68s - 5.54s]  Est-ce que ça passe par des acteurs qui, eux,
[5.68s - 8.38s]  ont un peu le contrôle sur le hardware,
[8.70s - 10.98s]  ou l'infrastructure, ou les data centers ?
[10.98s - 12.62s]  Il y a Microsoft aussi, je crois, avec qui vous avez bossé.
[12.94s - 14.20s]  Est-ce que ça passe aussi par ça, justement,
[14.30s - 15.56s]  s'entourer des bonnes personnes ?

[0.00s - 0.68s]  Faut les bons partners

[0.00s - 6.48s]  Il faut les bons partenaires de distribution en particulier parce que le calcul passe souvent par le cloud.
[7.00s - 12.52s]  Et donc on a comme partenaire tous les fournisseurs de cloud américains parce que c'est quand même les plus gros.
[12.82s - 15.84s]  On a aussi des fournisseurs français, on a Outscale qui en travaille.
[16.32s - 18.32s]  Et puis Nvidia c'est un fournisseur quasiment de cloud aussi.
[18.58s - 19.76s]  Donc à ce titre on travaille avec eux.
[20.04s - 23.56s]  On a fait de la R&D avec un modèle qui s'appelle Mistral Nemo.

[0.00s - 1.12s]  Imaginez, il y a des gens qui nous

[0.00s - 3.30s]  écoutes qui n'ont pas su. Est-ce que tu peux nous expliquer c'est quoi aujourd'hui la

[0.00s - 1.50s]  des modèles qui sont à jour.
[1.50s - 3.50s]  Moi, j'ai vu que dans les derniers mises à jour récents,
[3.50s - 4.38s]  il y a le large 2.

[0.00s - 3.82s]  Oui, alors maintenant on les numérote comme Ubuntu, donc 24.11.
[4.94s - 11.24s]  Et donc celui-là, Mistral Large 24.11, il est très fort pour appeler des fonctions, orchestrer des choses.
[11.58s - 14.76s]  Parce qu'en fait les modèles ça génère du texte, c'est l'utilisation de base.
[15.32s - 19.60s]  Mais ce qui est intéressant c'est quand ils génèrent des appels à des outils et qu'on les utilise comme des orchestrateurs,
[20.00s - 22.70s]  comme un peu des operating systems.
[23.20s - 27.90s]  Et donc on travaille beaucoup sur le fait d'avoir des modèles qui puissent être connectés à plein d'outils différents

[0.00s - 2.24s]  auxquelles on peut poser des questions, auxquelles on peut donner des tâches
[2.24s - 4.10s]  et qui vont réfléchir d'eux-mêmes aux outils qu'ils vont appeler.
[4.46s - 6.18s]  Et donc on insiste beaucoup là-dessus.
[6.28s - 8.92s]  Et donc la nouvelle version de Mistral Large, elle est particulièrement forte là-dessus.

[0.00s - 0.14s]  Oui.

[0.00s - 0.82s]  Après, il y a...

[0.00s - 1.16s]  Il y a eu des mixtrales aussi.
[1.68s - 3.78s]  Ça, pour comprendre, c'est plutôt pour servir,
[4.00s - 4.78s]  pour une entreprise, par exemple,
[4.86s - 6.52s]  pour servir beaucoup d'utilisateurs en même temps ?

[0.00s - 4.96s]  C'est un autre type d'architecture qui est particulièrement pertinent quand on a une forte charge.
[5.52s - 8.46s]  Beaucoup d'utilisateurs, c'est des choses que nous on utilise par exemple.
[8.98s - 9.94s]  C'est des mixtures.
[10.58s - 11.28s]  C'est des mixtures.
[11.28s - 12.54s]  Une sorte de cerbère à 8 têtes.
[12.72s - 18.94s]  C'est ça, c'est plusieurs modèles en même temps et chaque mot passe sur le modèle le plus adapté.
[18.94s - 21.70s]  Pour plusieurs raisons, ça permet de mieux utiliser les GPUs.

[0.00s - 0.44s]  En ligne

[0.00s - 0.54s]  parce qu'en fait

[0.00s - 0.48s]  sur le cerveau.

[0.00s - 0.94s]  Et derrière, il y a les plus petits.
[1.54s - 5.34s]  Il y a les petits modèles qui passent sur les laptops, qui passent sur les smartphones.
[5.78s - 9.38s]  Et ceux-là, ils sont particulièrement adaptés à des usages de hobbyistes
[9.38s - 12.62s]  parce qu'il n'y a pas besoin d'aller dans le cloud, on peut les modifier facilement.
[13.10s - 16.64s]  Et puis, très vite, c'est aussi pas mal focalisé sur cet aspect petit et rapide
[16.64s - 18.90s]  parce que c'est vraiment l'ADN de l'entreprise.
[19.26s - 21.06s]  Nous, aujourd'hui, le produit, ce n'est pas le modèle.
[21.06s - 23.60s]  Le produit, c'est la plateforme pour les développeurs.

[0.00s - 0.06s]  ...

[0.00s - 1.56s]  Donc le produit, c'est

[0.00s - 2.22s]  la plateforme pour construire des applications en tant que développeur,
[2.22s - 3.20s]  et là-dedans il y a des modèles.

[0.00s - 2.34s]  et puis un ensemble d'applications

[0.00s - 1.66s]  qui permettent de gagner en productivité.
[1.92s - 2.90s]  C'est un milieu qui est super

[0.00s - 0.56s]  compétitif.

[0.00s - 2.86s]  Évidemment, que ce soit, on l'a dit, sur les modèles,
[2.94s - 4.58s]  mais aussi sur tout ce qu'il y a autour,
[4.78s - 7.86s]  sur comment améliorer l'expérience, les interfaces de chat, etc.
[8.60s - 11.16s]  On a vu les systèmes d'interface qui se bougent.
[11.24s - 13.72s]  Tout le monde est un peu en train d'essayer de trouver les meilleures solutions à ça,
[13.86s - 17.24s]  Anthropic, OpenAI, et vous, évidemment, en tant qu'outsider,
[17.24s - 22.06s]  c'est quoi votre cible précise à vous en termes de possibilités d'évolution,
[22.52s - 25.04s]  quand on a des aussi gros acteurs à côté ?
[25.04s - 27.86s]  C'est quoi, toi, tu penses, la direction où vous avez un edge ?

[0.00s - 4.60s]  On a un fort edge dans le fait de découpler la question de l'infrastructure, de la question de l'interface.
[5.24s - 7.10s]  Notre solution peut être déployée partout.
[7.48s - 11.16s]  Elle peut être déployée dans le cloud, mais elle peut être déployée chez les entreprises qui ne sont pas dans le cloud.
[11.92s - 14.38s]  Elle peut être déployée sur des laptops.

[0.00s - 25.00s]  C'est le edge qu'on a construit aussi au-dessus de l'aspect open source, que ça va assez bien avec. Que les poids des modèles soient accessibles, ça rend facile leur déploiement n'importe où. On a cet aspect portabilité qui est très important. C'est notre première différenciation qu'on a beaucoup utilisée cette année. La différenciation qu'on cherche tous, c'est d'avoir la meilleure interface utilisateur. Il y a plein de sujets qui ne sont pas résolus. Le fait d'utiliser plein d'outils en même temps,
[25.12s - 27.52s]  le fait d'avoir des agents qui tournent pendant longtemps
[27.52s - 30.80s]  et qui prennent le feedback des utilisateurs.
[31.06s - 33.76s]  C'est-à-dire qu'on peut les voir comme des stagiaires,
[33.84s - 35.78s]  des stagiaires auxquels il faut faire du feedback
[35.78s - 37.62s]  pour qu'ils deviennent de plus en plus performants.
[38.20s - 41.48s]  Et donc, on va aller vers ce genre de système de plus en plus autonome
[41.48s - 43.12s]  qui vont avoir besoin de plus en plus de feedback
[43.12s - 46.94s]  pour passer de 80% de performance à 100% de performance.

[0.00s - 0.86s]  des stagiaires
[0.86s - 2.04s]  des stagiaires
[2.04s - 2.30s]  en qualité

[0.00s - 1.82s]  Comment ça se dit ? Tu ne restes pas constamment derrière lui
[1.82s - 3.24s]  et attendre qu'il avance ?

[0.00s - 5.34s]  Tu lui donnes une tâche, tu regardes ce qu'il a fait, tu lui dis ce qu'il n'a pas bien fait,

[0.00s - 1.60s]  J'espère que la prochaine fois, ils le fassent mieux.
[1.96s - 4.50s]  Mais en fait, c'est plein de questions scientifiques qu'il faut résoudre pour que ça fonctionne.
[4.84s - 5.54s]  Et d'interface.
[5.70s - 6.34s]  Et d'interface, oui.
[6.74s - 7.68s]  C'est pas du mail, en fait.

[0.00s - 1.28s]  C'est pas du mail en fait
[1.28s - 2.20s]  Est-ce qu'on va pas
[2.20s - 3.02s]  Pour l'instant
[3.02s - 3.64s]  C'est du chat
[3.64s - 4.54s]  En mode temps réel
[4.54s - 6.00s]  Est-ce qu'à terme
[6.00s - 6.86s]  On va envoyer un mail
[6.86s - 8.22s]  A notre assistante

[0.00s - 1.64s]  Et juste, il nous ping quand il a fini.

[0.00s - 2.18s]  C'est une des formes, je pense que c'est plutôt
[2.18s - 4.02s]  l'assistant qui t'envoie un mail, en fait, à un moment donné.
[4.16s - 6.32s]  Tu y travailles, et puis toutes les deux heures,
[6.42s - 7.66s]  il te dit, voilà où j'en suis, etc.
[8.08s - 9.52s]  Donc oui, il y a un aspect passé
[9.52s - 11.28s]  du synchrone à la synchrone,
[11.72s - 14.02s]  qui est très important, et qui pose plein de questions
[14.02s - 15.40s]  d'interface, parce que

[0.00s - 1.48s]  Bon, le mail, c'est peut-être pas la meilleure interface.
[1.72s - 4.28s]  Il y en a certainement d'autres qui sont plus intelligentes.
[4.64s - 6.72s]  La question de quelle est l'interface pour donner le feedback,
[6.90s - 9.26s]  quelle est l'interface pour sélectionner ce qui est préférable pour l'humain ?

[0.00s - 0.70s]  il y en a
[0.70s - 1.40s]  certainement d'autres

[0.00s - 0.74s]  ça c'est
[0.74s - 1.84s]  on y travaille

[0.00s - 0.12s]  Ouais.

[0.00s - 1.90s]  J'allais me dire, je suis persuadé que le...
[1.90s - 2.92s]  Enfin, je sais pas, mais...

[0.00s - 0.18s]  Oui.

[0.00s - 2.46s]  quand on regarde le chat, la discussion,

[0.00s - 3.52s]  Ce n'est pas forcément l'interface ultime pour dialoguer avec un LLM.

[0.00s - 1.16s]  Ça a beaucoup évolué maintenant.
[1.94s - 4.82s]  Tu peux te chatter avec le chat et il peut décider de te mettre dans un document.
[5.02s - 7.02s]  Et là, tu travailles avec lui sur la construction d'un document ?

[0.00s - 7.40s]  Tu peux lui demander d'aller chercher des sources et tu vois les sources, tu peux retourner, tu peux aller regarder ce que des humains ont écrit et demander des résumés par exemple.
[7.96s - 13.98s]  Et donc en fait ce que ça crée, ce que ça permet, les AI génératifs, c'est une espèce de liquidité de ta manière d'accéder à la connaissance.

[0.00s - 0.90s]  Donc tu peux regarder

[0.00s - 3.20s]  tout un site web et tu peux dire condense moi ce site web en

[0.00s - 1.14s]  en deux phrases

[0.00s - 1.84s]  Et je pense qu'il reste encore beaucoup de choses

[0.00s - 2.30s]  à faire pour que le modèle te permette d'apprendre beaucoup plus vite
[2.30s - 3.80s]  et de charger de la connaissance beaucoup plus vite.

[0.00s - 3.96s]  Je crois que c'est Vercel qui avait fait des démos assez marrantes de composants

[0.00s - 0.46s]  Web ?

[0.00s - 1.02s]  qui se construisait

[0.00s - 1.78s]  En fonction de la nécessité ?

[0.00s - 0.78s]  tu te poses une question,
[1.10s - 1.84s]  et il te générait,
[1.92s - 2.80s]  sur la météo par exemple,
[2.90s - 4.04s]  il te générait un composant
[4.04s - 6.04s]  d'interface graphique

[0.00s - 0.52s]  À la volée quoi !

[0.00s - 0.60s]  Ils voient le budget.
[0.74s - 1.14s]  Ouais, c'est ça.
[1.28s - 3.42s]  En fait, la question, c'est une question en back-end et en fontaine.
[3.58s - 6.72s]  En back-end, c'est quel outil appeler pour aller chercher de l'information
[6.72s - 7.92s]  ou pour actionner des choses ?

[0.00s - 2.78s]  Et en front-end, c'est quelle interface il faut montrer à l'utilisateur,
[2.88s - 4.00s]  étant donné son intention actuelle.

[0.00s - 3.24s]  Et ce que ça veut dire, c'est que les gros logiciels avec 50 000 boutons,
[3.24s - 5.88s]  je pense notamment au montage, ça va progressivement disparaître
[5.88s - 8.52s]  parce que tu peux identifier son état d'esprit

[0.00s - 2.14s]  au moment où il est en train de créer, et adapter les boutons,
[2.54s - 4.16s]  lui donner exactement ce dont il a besoin.

[0.00s - 3.98s]  Et donc ça change vraiment complètement la manière dont les interfaces vont se comporter dans les années qui viennent.

[0.00s - 3.04s]  Justement, on parlait de cette interface, de communication,

[0.00s - 3.68s]  comment on y accède. Tu parlais du fait que vous êtes déployable un peu de n'importe où.
[3.68s - 7.96s]  Il y a un truc que je constate en parlant avec des gens autour de moi, c'est qu'on a un peu une génération

[0.00s - 1.90s]  d'employés d'entreprise frustrés
[1.90s - 3.82s]  actuellement parce que chez eux, ils peuvent
[3.82s - 5.70s]  utiliser des trucs incroyables
[5.70s - 7.80s]  genre les meilleurs modèles
[7.80s - 9.64s]  disponibles, ils vont sur OpenAI, etc.
[10.16s - 11.82s]  Une fois au travail, on leur interdit
[11.82s - 13.94s]  souvent l'utilisation des meilleurs
[13.94s - 15.94s]  outils et parfois ils se retrouvent
[15.94s - 17.90s]  avec des versions un peu bridées ou des
[17.90s - 19.26s]  copilotes. Ou avec rien du tout.
[19.34s - 21.06s]  Ou avec, pour la plupart du temps, rien du tout.
[21.64s - 22.34s]  Ça vient d'où ça ?

[0.00s - 0.78s]  Ou avec rien du tout.

[0.00s - 1.20s]  Ça vient du fait que
[1.20s - 3.64s]  les systèmes d'AI génératifs,
[3.78s - 4.56s]  ça touche beaucoup aux données.
[4.88s - 5.74s]  Et les données dans les entreprises,
[5.88s - 6.60s]  c'est quand même assez important.
[6.98s - 8.90s]  Et donc, c'est là-dessus
[8.90s - 9.98s]  que nous, on a cherché
[9.98s - 10.62s]  à trouver des solutions.
[11.02s - 11.48s]  Alors, faire en sorte
[11.48s - 12.38s]  que les données, elles restent
[12.38s - 13.18s]  dans l'entreprise,
[13.58s - 15.64s]  que nous, en tant que fournisseurs d'AI,
[15.78s - 17.00s]  on n'est pas à voir ces données-là.
[17.30s - 18.38s]  Ça permet...

[0.00s - 3.96s]  justement d'avoir le niveau de sécurité, le niveau de gouvernance dont tu as besoin sur les données.
[4.32s - 5.78s]  Et donc progressivement...

[0.00s - 1.46s]  on va résoudre ce problème.

[0.00s - 2.50s]  Et nous, je dirais que c'est un des problèmes essentiels qu'on cherche à résoudre,
[2.56s - 3.26s]  de faire en sorte que...

[0.00s - 2.04s]  l'IT dans les entreprises soit confortable
[2.04s - 3.90s]  pour amener le char à tous leurs employés
[3.90s - 5.02s]  et qu'ils arrêtent d'être frustrés.

[0.00s - 1.96s]  les exemples d'outils que tu donnais,
[2.32s - 3.94s]  il y a un truc qui revient,
[4.10s - 6.10s]  qu'on n'a pas explicité, mais qui est
[6.10s - 7.54s]  super important, c'est le

[0.00s - 1.12s]  la notion d'objectif,
[1.34s - 3.04s]  d'avoir des modèles qui sont capables

[0.00s - 3.06s]  d'effectuer des tâches et sur la route, d'arriver à créer

[0.00s - 0.84s]  des étapes
[0.84s - 1.60s]  et à appeler
[1.60s - 2.40s]  les bons outils

[0.00s - 1.70s]  comme le ferait un bon stagiaire.
[2.02s - 4.82s]  Tu n'as pas nécessairement à lui expliquer l'ensemble des étapes qu'il doit faire.
[5.54s - 9.20s]  Tu lui dis « Regarde les prochains vols pour New York et prends-en un ».

[0.00s - 2.12s]  Tu n'as pas besoin de lui expliquer étape par étape,
[2.66s - 4.34s]  seconde par seconde, ce qu'il doit effectuer.

[0.00s - 2.54s]  On a des modèles qui peuvent commencer à appeler des outils,

[0.00s - 4.38s]  qu'on sent un peu limité dans leur capacité à en utiliser plusieurs d'affilés notamment.
[4.88s - 6.38s]  Des trucs vraiment utiles, vraiment stylés.
[6.54s - 8.32s]  Comment tu penses que ça va évoluer ?
[8.32s - 9.86s]  Est-ce que c'est une...

[0.00s - 2.32s]  une frontière qui peut être bientôt franchie ?
[2.32s - 4.00s]  Est-ce que l'année prochaine, on aura résolu ce problème
[4.00s - 6.36s]  et on pourra faire 20 étapes avec beaucoup de fiabilité ?
[6.36s - 7.86s]  Ou est-ce qu'on est encore loin d'y arriver ?

[0.00s - 1.08s]  Je pense que c'est la frontière.
[2.08s - 3.72s]  Tout le monde essaie de la pousser,
[3.92s - 5.16s]  ça ne va pas se débloquer d'un coup.
[5.40s - 7.92s]  Parce qu'en fait, maîtriser un outil,
[8.20s - 9.28s]  ça prend du temps à un humain,
[9.38s - 10.28s]  ça prend aussi du temps à un modèle.
[10.28s - 12.04s]  Il faut des démonstrations,
[12.74s - 13.36s]  il faut du feedback,
[13.48s - 15.16s]  parce que la première fois, il va se tromper.
[15.82s - 17.82s]  Et une notion d'expertise qu'il faut distiller
[17.82s - 19.24s]  de l'entreprise vers les systèmes d'AI.
[19.62s - 22.12s]  Et ça, ça ne va pas se faire de manière magique.
[22.54s - 24.32s]  Il faut tous les systèmes en place.
[24.70s - 25.50s]  Il faut les métasystèmes,
[25.62s - 27.60s]  c'est-à-dire qu'il faut que les employés
[27.60s - 29.16s]  dans les entreprises soient capables de...

[0.00s - 3.24s]  de fournir du signal supplémentaire au système D.I. pour qu'il s'améliore.
[3.50s - 6.88s]  Donc ça va progresser, on va avoir de plus en plus d'outils utilisables en même temps
[6.88s - 8.62s]  et des modèles qui peuvent résonner de plus en plus.

[0.00s - 1.92s]  ça va être progressif.
[2.18s - 3.74s]  Et pour que ça marche vraiment très bien,
[3.92s - 5.50s]  il faut y mettre du sien, il faut investir dès maintenant.

[0.00s - 4.50s]  Pour illustrer ça, on voit que OpenAI, dans leur dernier modèle, dans les O1 et compagnie,

[0.00s - 3.00s]  Ce ne sont plus des améliorations significatives sur le modèle en lui-même,
[3.00s - 5.84s]  mais il tente des trucs de le faire boucler sur lui-même,
[5.84s - 7.96s]  faire des chaînes de pensée, je ne sais pas comment on dit en français.
[7.96s - 9.10s]  Chaîne de pensée, oui.

[0.00s - 0.30s]  ...

[0.00s - 1.00s]  c'est pas bien non ?
[1.00s - 1.64s]  ah c'est gros ça

[0.00s - 2.46s]  Est-ce que, selon toi, c'est un peu un signe que...

[0.00s - 1.82s]  on a atteint une sorte de plafond.
[2.08s - 5.10s]  C'est-à-dire que, justement, sur cette évolution exponentielle,
[5.28s - 8.38s]  on a bien optimisé par rapport à leur taille,
[8.84s - 9.76s]  la manière dont marchent les modèles.
[9.88s - 11.32s]  Maintenant, justement, il faut trouver autre chose.

[0.00s - 2.80s]  C'est un paradigme qui est de plus en plus saturé,
[2.84s - 3.88s]  je pense qu'il n'est pas encore saturé,
[3.96s - 5.44s]  qui est ce qu'on appelle le préentraînement,
[5.54s - 6.88s]  donc la compression de la connaissance humaine.

[0.00s - 2.72s]  D'une certaine manière, tu as une connaissance disponible

[0.00s - 1.68s]  humaine qui a une certaine taille,
[1.74s - 2.00s]  et à un moment,
[2.04s - 2.86s]  tu as fini de la compresser.
[3.16s - 4.56s]  Et c'est là où il faut aller chercher...

[0.00s - 7.74s]  du signal supplémentaire. Du coup, chaîne de pensée, utilisation de plusieurs outils, utilisation de signal experts dans les entreprises.
[8.30s - 11.92s]  Donc, il n'y a pas de saturation du système. On sait comment aller à l'étape suivante.
[12.14s - 16.70s]  Mais sur l'aspect pré-entraînement, oui, on commence à savoir bien le faire collectivement.
[16.86s - 20.98s]  Tout le monde sait à peu près faire la même chose. Et donc, ce n'est plus tellement là où est la compétition.

[0.00s - 5.48s]  La compétition, elle est sur les interfaces et la compétition, elle est sur avoir des modèles qui tournent pendant plus longtemps.

[0.00s - 0.22s]  Ok.

[0.00s - 2.64s]  Je trouve que c'est un peu dur de se faire un avis dessus, justement, quand on est...

[0.00s - 2.88s]  On ne maîtrise pas la stack scientifique
[2.88s - 4.78s]  derrière les transformeurs et compagnie.

[0.00s - 3.00s]  J'ai l'impression qu'il y a un peu ce débat entre est-ce que c'est juste une question
[3.00s - 8.26s]  de compute, de données qui va repousser cette barrière d'autonomie ?

[0.00s - 4.40s]  Est-ce que c'est vraiment un problème intrinsèque à la manière dont le modèle est designé ?
[4.40s - 11.20s]  Et que juste le fait que ce soit de la prédiction du prochain token qui peut avoir un petit pourcentage de partir en cacahuètes à chaque fois,
[11.56s - 15.80s]  ça rend nécessairement trop compliqué, trop difficile la planification long terme.
[16.08s - 20.18s]  Je sais que par exemple, il y a des gens comme Yann Lecay, on en parle souvent, qui sont un peu défenseurs de cette vision-là,
[20.22s - 23.14s]  que l'AGI, ou je ne sais pas comment on l'appelle, elle est cachée encore derrière...

[0.00s - 1.16s]  des découvertes scientifiques.

[0.00s - 8.90s]  Oui, c'est une bonne question. Ce qui est vrai, c'est que travailler sur des architectures qui induisent des biais de réflexion humaine, c'est souvent utile.
[9.34s - 19.78s]  Ça a été utile pendant les 12 dernières années de se dire comment nous on réfléchit, essayons d'écrire ça en mathématiques et de faire en sorte que les modèles copient un peu ce qu'on sait faire.

[0.00s - 4.60s]  Ce qu'on observe aussi, c'est que toute l'intelligence qu'on peut mettre dans les architectures...

[0.00s - 2.06s]  Il suffit de mettre deux fois plus de compute et ça disparaît.
[2.50s - 5.60s]  Donc, en fait, le paradigme qu'on a suivi dans les cinq dernières années,
[5.70s - 7.98s]  c'est plutôt de se dire, prenez une architecture extrêmement simple
[7.98s - 9.50s]  qui prédit des séquences...

[0.00s - 3.16s]  et passons là à l'échelle, allons chercher le plus de données possibles,
[3.44s - 6.34s]  allons chercher les données multimodales, allons chercher de l'audio, ce genre de choses.

[0.00s - 0.70s]  Et...

[0.00s - 1.68s]  Et passons-la à l'échelle et voyons ce que ça donne.
[2.02s - 5.12s]  Et en fait, ce que ça donne, c'est que c'était en tout cas plus intelligent,
[5.42s - 8.24s]  en termes d'allocation de ressources, de travailler sur un passage à l'échelle
[8.24s - 10.32s]  que de travailler sur des architectures subtiles.

[0.00s - 2.00s]  Alors est-ce que c'est toujours le cas maintenant,
[2.00s - 4.84s]  comment ça va avoir saturé la quantité de données qu'on s'est compressé ?

[0.00s - 0.96s]  Je pense que la question est ouverte.

[0.00s - 1.76s]  le sujet, ce n'est plus tellement une question d'architecture,
[1.96s - 3.14s]  c'est plutôt une question d'orchestration.
[3.32s - 3.70s]  C'est-à-dire...

[0.00s - 2.28s]  Comment on fait pour que les modèles se rappellent eux-mêmes ?

[0.00s - 1.42s]  qu'ils interagissent avec des outils,
[1.54s - 5.86s]  qu'ils durent longtemps, qu'ils fassent du raisonnement en plusieurs étapes.

[0.00s - 2.66s]  Et ça, ça reste les mêmes modèles au fond.

[0.00s - 3.50s]  C'est la brique de base, mais le système complet, ce n'est pas juste le modèle.
[3.62s - 6.50s]  C'est le modèle qui sait se rappeler lui-même, qui sait réfléchir,
[6.58s - 9.36s]  qui sait interagir avec tout son environnement, qui sait interagir avec les humains.
[10.00s - 12.78s]  Donc la complexité des systèmes, elle devient beaucoup plus grande
[12.78s - 15.06s]  que juste un simple modèle de génération de séquences.

[0.00s - 2.58s]  Ça reste le moteur, mais c'est pas du tout toute la voiture.

[0.00s - 2.66s]  Mais donc, tu es plutôt optimiste sur le fait que ce soit le bon moteur.
[2.94s - 3.76s]  C'est le bon moteur.
[4.06s - 6.94s]  Après, il y a une règle en machine learning qui dit
[6.94s - 9.22s]  essentiellement, augmenter la capacité de calcul,
[9.38s - 12.38s]  ça augmente la qualité des systèmes.
[12.80s - 13.90s]  Et tu as deux solutions pour le faire.
[14.00s - 16.32s]  Soit tu compresses de la donnée, soit tu fais de la recherche.
[16.50s - 17.64s]  C'est-à-dire que tu vas échantillonner,
[18.06s - 19.84s]  tu vas demander au modèle de tester mille trucs
[19.84s - 22.26s]  et de sélectionner l'échantillon qui marche le mieux
[22.26s - 24.10s]  et tu vas le renforcer là-dessus.

[0.00s - 2.42s]  et donc là on commence de plus en plus à basculer
[2.42s - 4.24s]  dans le mode recherche plutôt que dans le mode compression
[4.24s - 5.88s]  la personne qui a dit ça c'est
[5.88s - 7.88s]  Richard Sutton dans un

[0.00s - 2.32s]  blog post que je vous invite à lire qui s'appelle The Bitter Lesson.

[0.00s - 1.82s]  Est-ce que toi, il y a une démo

[0.00s - 0.92s]  un peu de bout en bout,

[0.00s - 1.14s]  d'un truc qui a

[0.00s - 1.90s]  même si parfois ça ne marche pas,
[1.90s - 3.76s]  mais d'un truc où tu as été impressionné,
[3.76s - 5.48s]  où ça a vraiment très bien marché,

[0.00s - 4.86s]  d'une suite d'étapes, un truc qui t'a fait un peu sentir comme Iron Man avec Jarvis ?

[0.00s - 3.12s]  Oui, avec le chat, on a connecté les API ouvertes de Spotify.
[3.50s - 6.90s]  Et donc, tu peux lui parler, lui demander des playlists et décrire ta playlist.
[7.08s - 8.90s]  Puis, ça te crée ta playlist et ça la joue pour toi.
[9.12s - 10.46s]  Donc, ça fait des choses intéressantes.
[10.54s - 11.52s]  Alors, c'est juste un seul outil, ça.
[12.04s - 14.06s]  Non, là, on a vu des choses très intéressantes.
[14.24s - 18.00s]  C'est une fois qu'on a connecté le web, ça te permet d'avoir toutes les informations en live.

[0.00s - 4.60s]  Très vite, tu peux te créer tes mémos pour savoir ce qu'il faut aller dire à tel client en fonction des informations qu'il a eues.

[0.00s - 5.48s]  Et donc la combinaison des outils, ensemble, ça fait émerger des cas d'usage que tu n'avais pas forcément prévus.
[5.70s - 9.28s]  Si tu as connecté le web, si tu connectes ton mail, tu peux faire plein de choses en même temps.
[9.70s - 14.40s]  Et si tu connectes ta connaissance interne et le web, tu peux combiner ces informations.

[0.00s - 1.74s]  de manière un peu imprévisible ?

[0.00s - 2.32s]  Et donc la quantité de cas d'usage que tu couvres

[0.00s - 3.30s]  et assez exponentielle au nombre d'outils.

[0.00s - 1.26s]  Et donc ça, c'est assez magique, je veux dire.

[0.00s - 2.66s]  Moi effectivement je trouve qu'il y a un côté un peu vertigineux
[2.66s - 4.56s]  On va pouvoir construire
[4.56s - 5.08s]  Des trucs de fou

[0.00s - 3.92s]  Mais du coup, ça fait que c'est un peu dur de s'imaginer, de se dire ça va ressembler
[3.92s - 6.26s]  à quoi concrètement, genre le métier de développeur de

[0.00s - 3.50s]  de quelqu'un qui doit faire des scénarios de LLM, ça ressemble à quoi ?

[0.00s - 4.76s]  Je dirais que c'est un outil qui augmente le niveau d'abstraction requis par les humains.
[5.64s - 11.04s]  En tant que développeur, tu vas continuer à réfléchir aux problèmes que tu cherches à résoudre pour tes utilisateurs.
[11.48s - 18.32s]  Tu vas continuer à réfléchir aux architectures au niveau qui remplissent tes contraintes, ton cahier des charges.
[18.84s - 22.44s]  Après, est-ce que tu vas continuer à coder tes applications en JavaScript ?
[22.44s - 25.38s]  Vrasemblablement, non, parce que les modèles arrivent bien à générer...

[0.00s - 2.90s]  des applications simples et des applications de plus en plus compliquées.
[3.40s - 7.52s]  Donc tous les sujets très abstraits qui vont nécessiter de la communication avec des humains.
[7.84s - 9.60s]  Le métier d'ingénieur, c'est aussi un métier de communication.
[9.78s - 11.54s]  Il faut aussi comprendre quelles sont les contraintes de chacun.
[12.14s - 14.10s]  Ça, ça ne va pas être facilement remplaçable.

[0.00s - 4.50s]  Mais en revanche, tout l'aspect « je t'aide à faire tes tests unitaires »,
[4.50s - 8.16s]  « je te fais ton application Pixel Perfect », à partir d'un design,

[0.00s - 6.98s]  Je pense que ça devait devenir de plus en plus automatisable pour juste coller au développeur.

[0.00s - 0.88s]  pour tous les métiers.

[0.00s - 3.44s]  Est-ce qu'on a une intuition de comment ça se fait que les modèles sont aussi sensibles

[0.00s - 0.36s]  au code,
[0.72s - 1.30s]  c'est pour se dire ?

[0.00s - 2.44s]  Par exemple, je veux un modèle qui est super fort en français et en anglais.

[0.00s - 3.04s]  Bon, qu'ils sachent, le Python et le JavaScript, a priori, ça n'est pas utile.
[3.44s - 5.46s]  Or, ce n'est pas du tout ce qu'on observe, de ce que j'ai compris.

[0.00s - 1.18s]  C'est une très bonne question.
[27.64s - 28.38s]  C'est vrai qu'on observe un genre de transfert. Entraîner son modèle sur beaucoup de codes, ça lui permet de raisonner mieux. Je ne suis pas le mieux placé pour en parler, il faudrait que ça soit plutôt Guillaume, mais la réalité, c'est que le code, ça a plus d'informations que le langage. Il y a plus de réflexion qui est passée dans le langage, c'est plus structuré. S'entraîner à générer du code, ça force le modèle à raisononner à plus haut niveau que l'entraîner à générer du texte.
[29.64s - 31.66s]  Et donc, il sait résonner sur du code, et donc quand il voit du texte,
[31.72s - 32.90s]  il sait aussi résonner sur du texte.
[33.32s - 35.54s]  Et c'est vrai qu'il y a ce transfert un peu magique,
[36.00s - 37.66s]  qui est, je pense, une des raisons pour lesquelles les modèles sont devenus
[37.66s - 38.96s]  largement meilleurs dans les deux dernières années.
[39.24s - 40.62s]  Ça sert aussi parce que

[0.00s - 0.48s]  En fait, le...

[0.00s - 2.26s]  Tu as beaucoup plus de codebase
[2.26s - 4.22s]  qui sont plus longs qu'un livre.
[4.34s - 5.84s]  Comprendre une codebase, c'est plus long que lire un livre.

[0.00s - 2.96s]  et donc un peu le maximum sur lequel tu peux t'entraîner

[0.00s - 3.48s]  pour faire un modèle qui comprend le contexte long

[0.00s - 2.22s]  c'est des livres du 19e siècle

[0.00s - 8.34s]  Et le maximum sur lequel tu peux t'entraîner pour faire du code, c'est des millions de lignes de projets open source.
[8.80s - 11.54s]  Et donc, c'est plus long et ton modèle peut résonner plus longtemps.

[0.00s - 1.04s]  C'est des millions de lignes.
[1.94s - 2.40s]  Oui, c'est ça.

[0.00s - 1.50s]  je pense que c'est une des intuitions

[0.00s - 1.90s]  Je te propose de parler maintenant un petit peu de

[0.00s - 0.36s]  T'allons.

[0.00s - 3.20s]  et des gens qui font que, justement, vous faites ce que vous faites.
[3.40s - 6.92s]  Déjà, pourquoi est-ce que vous avez décidé, à la base, de mettre Mistral à Paris ?

[0.00s - 0.78s]  Aujourd'hui, ça peut pas...

[0.00s - 1.40s]  paraître un peu plus évident,
[1.48s - 3.90s]  on sait que l'écosystème est super vivant,
[3.96s - 4.80s]  on va en parler,
[5.50s - 6.78s]  mais est-ce que c'était une décision,
[7.10s - 9.62s]  à cette époque-là, une décision évidente
[9.62s - 11.40s]  entre ce maître-là ou à San Francisco,
[11.78s - 14.54s]  même avec une boîte française ?

[0.00s - 2.12s]  On ne s'est même pas posé la question en réalité.
[2.40s - 4.58s]  Moi, je n'avais aucune envie de partir de Paris.
[4.86s - 7.36s]  Ma compagne est fonctionnaire, donc elle a quelques contraintes.
[7.84s - 11.50s]  Timothée, il n'avait aucune envie de partir de France aussi, et Guillaume non plus.
[12.00s - 16.56s]  Donc je pense qu'en réalité, si j'ai réfléchi, je crois qu'on ne s'est jamais posé la question.
[16.98s - 19.78s]  On savait aussi que les gens dans notre réseau, en fait, c'était des Parisiens.
[20.18s - 21.42s]  Des Parisiens, des gens à Londres aussi.
[21.72s - 24.10s]  Ces personnes qu'on pouvait recruter, c'était des gens locaux.
[24.34s - 26.70s]  Donc c'était une évidence de démarrer à Paris.
[27.30s - 27.50s]  Comment ça se fait ?

[0.00s - 0.24s]  que...

[0.00s - 0.26s]  Bye.

[0.00s - 4.66s]  Marie, particulièrement, fourmille autant de bons ingés et scientifiques ?

[0.00s - 1.94s]  Je pense parce qu'il y a un écosystème en machine learning.
[2.14s - 5.40s]  Il y a un écosystème avec l'INRIA, avec...

[0.00s - 2.74s]  l'académique d'un côté, et puis les laboratoires privés
[2.74s - 4.38s]  que Yann Lequin a contribué à créer.
[4.72s - 8.94s]  Un laboratoire FAIR, historiquement, qui a été créé en 2015, je pense.

[0.00s - 8.18s]  T'as DeepMind qui en réaction s'était installé là-bas et qui avait un énorme centre de compétences à Londres grâce à DeepMind.
[8.66s - 15.12s]  Et donc en fait ce qui fait les talents dans la tech c'est le fait que t'es une entreprise qui soit déjà passée par là avant, qui a grossi.

[0.00s - 3.88s]  et des gens qui ont appris dans cette entreprise là et qui ensuite essaiment.

[0.00s - 6.28s]  Et donc on a bénéficié de ça, on a bénéficié des entrepreneurs qui voulaient bien nous rejoindre et qui se sont formés là-bas.

[0.00s - 3.56s]  Très bon écosystème privé et très bon écosystème public aussi,
[4.02s - 8.78s]  parce qu'il y a beaucoup de nos chercheurs qui ont fait des thèses dans l'académique en France aussi.

[0.00s - 3.32s]  le résultat est assez dingue quand même parce que dans les conventions d'IA,
[3.52s - 7.52s]  j'ai vu Oliyama par exemple qui a organisé un meetup ou un truc comme ça et tu vois les images,

[0.00s - 1.86s]  On se dit pas que ça apparaît, quoi.
[1.98s - 3.70s]  Et en fait, ça bouge de fou, quoi.

[0.00s - 2.34s]  Il y a plein de gens dans ce domaine en particulier qui sont des Français.
[2.88s - 4.48s]  Yann Lequin est un Français, mais...

[0.00s - 26.62s]  En dessous des gens plus jeunes que lui, il y a beaucoup de gens qui ont fait... On a fait tout le mal, Sialon par exemple. Oui, par exemple. À Meta, Paris, il y a aussi des gens très forts. À DeepMind, Paris, il y a des... Je ne vais pas encore aussi débaucher pour l'instant. J'ai plein d'amis... Je pense qu'on a débauché les gens qu'on pouvait. Ah, c'est terminé, assez ! J'ai plein d'amis qui sont français et qui sont encore là-bas. Ils finiront peut-être par créer leur boîte. De manière générale, l'Europe et la France en particulier a les bonnes compétences.
[26.62s - 30.62s]  Il y a les bonnes écoles, il faut être fort en maths et fort en informatique pour faire

[0.00s - 0.28s]  C'est un truc de russes.

[0.00s - 0.44s]  Je suis plein d'eux.

[0.00s - 0.70s]  C'est plus cool.

[0.00s - 1.74s]  Ah c'est un idée assez

[0.00s - 2.28s]  scientifiques en intelligence artificielle
[2.28s - 4.40s]  et de fait on a les bons tuyaux
[4.40s - 4.80s]  de formation

[0.00s - 1.38s]  Et c'est quoi les arguments aujourd'hui ?
[1.38s - 3.60s]  Moi, je viens de sortir de ma thèse en ML.
[4.34s - 5.84s]  Qu'est-ce qui fait, d'après toi,
[6.28s - 8.00s]  que je vais plutôt décider d'avenir chez Mistral
[8.00s - 11.36s]  versus chez Meta ou chez DeepMind ?

[0.00s - 0.12s]  ...

[0.00s - 3.68s]  En sortant de master, à Paris, je pense qu'on est de loin le meilleur centre de formation.

[0.00s - 3.40s]  pour faire le cœur de la science en intelligence artificielle.

[0.00s - 2.24s]  Il n'y a pas de structure équivalente, même en Europe.

[0.00s - 6.00s]  sur ce qu'on fait. Il y a 50-60 scientifiques chez nous qui sont tous extrêmement bien formés.
[6.52s - 11.62s]  Et donc, en sortant de Master, je trouve que c'est le meilleur endroit parce qu'en six mois, ils sont formés.
[11.92s - 14.92s]  C'est aussi un excellent endroit pour nous parce qu'on va récupérer des gens juniors.
[15.24s - 18.62s]  On a une bonne marque avec eux, on leur propose une excellente formation.

[0.00s - 2.02s]  Ils nous rejoignent et en fait, ils sont très performants au bout de 6 mois.
[2.30s - 6.24s]  Et ils ont plein d'idées, ils sont extrêmement créatifs, ils ont 23 ans, ils ont toute l'énergie qu'il faut.
[6.72s - 11.92s]  Et donc, on les utilise, on doit beaucoup de ce qu'on a proposé à des gens qui sont très jeunes.

[0.00s - 2.18s]  et donc on encourage tous les gens en master
[2.18s - 4.34s]  à venir nous rejoindre, on a plein de places
[4.34s - 5.18s]  et on adore les former

[0.00s - 4.92s]  J'ai appris tout à l'heure d'ailleurs qu'après notre émission qu'on avait faite sur Mistral, il y a eu une recrue.
[5.22s - 7.06s]  Tu te souviens ? Tu ne sais pas quel genre de profil ?
[7.06s - 8.06s]  Si, si, si, c'est un ingénieur.

[0.00s - 0.66s]  Ce n'est pas un certain...

[0.00s - 1.14s]  tu fixer un ingénieur.
[1.16s - 1.60s]  Si tout va bien,
[1.72s - 2.02s]  on n'a pas un peu.
[2.02s - 2.96s]  Oui, très content.
[3.18s - 3.70s]  Ok, parfait.
[3.96s - 4.82s]  Tu conseillerais quoi, toi,
[4.94s - 6.16s]  à des gens qui sont peut-être
[6.16s - 7.22s]  moins avancés
[7.22s - 8.78s]  ou qui réfléchissent
[8.78s - 9.78s]  à ce qu'ils veulent faire,
[9.86s - 10.40s]  qui voient bien
[10.40s - 10.94s]  qu'il y a quelque chose
[10.94s - 12.32s]  qui se passe dans l'IA ?
[12.32s - 13.30s]  Est-ce que, d'après toi,
[13.42s - 14.62s]  c'est trop tard, entre guillemets,
[14.68s - 15.42s]  dans le sens où...

[0.00s - 0.52s]  Ça va ?

[0.00s - 0.98s]  C'est quoi ?

[0.00s - 2.70s]  ça va être un écosystème très saturé ?
[2.70s - 4.14s]  Ou est-ce que tu penses que c'est encore possible ?
[4.14s - 7.26s]  Et qu'est-ce que tu conseillerais comme choix d'études,
[7.36s - 8.46s]  de trucs à explorer ?
[8.46s - 10.40s]  Est-ce qu'il faut faire des modèles de langage comme tout le monde ?
[10.40s - 11.30s]  Est-ce que justement...

[0.00s - 2.34s]  il faut essayer de sortir un petit peu de la bulle.
[2.72s - 3.26s]  C'est quand on...

[0.00s - 0.10s]  de

[0.00s - 2.52s]  Je pense qu'il faut réfléchir aux systèmes qu'on crée avec.
[25.96s - 28.46s]  Nous, on crée des systèmes de gestion de la connaissance, mais il y a plein de systèmes verticalisés dans les sciences de la vie, dans l'architecture, dans la conception par ordinateur, qui ne demandent qu'à être pris avec des nouveaux systèmes. Le montage vidéo aussi. Et donc, se dire que la technologie va continuer à avancer, les technologies horizontales vont être de plus en plus performantes. Ça ouvre de nouvelles opportunités d'automatisation, de nouvelles opportunités
[28.46s - 29.80s]  de création de logiciels
[29.80s - 32.02s]  intéressants, de création de services
[32.02s - 34.14s]  complètement différents pour les utilisateurs.
[34.48s - 35.94s]  Et donc partir des besoins d'utilisateurs,
[36.06s - 36.58s]  partir de...

[0.00s - 2.38s]  un peu du rêve du logiciel de demain,
[2.98s - 4.72s]  la machine auquel on va parler
[4.72s - 6.92s]  et qui va faire tout ce qu'on veut à sa place.
[7.44s - 8.54s]  Je pense que c'est un bon point de départ ?

[0.00s - 4.16s]  on choisit sa verticale et puis on se dit est-ce que les modèles aujourd'hui peuvent résoudre ce problème
[4.44s - 8.48s]  s'ils peuvent pas résoudre ce problème peut-être qu'il faut les personnaliser peut-être qu'il faut aller un petit peu plus profondément
[8.76s - 15.14s]  et là dessus on peut aider on peut aider à le faire on a tous les outils qui permettent de faire les personnalisations verticalisées. Nous on n'a pas

[0.00s - 3.24s]  On a parlé avec Microsoft des conséquences que ça avait dans la recherche de matériaux,
[3.24s - 4.24s]  par exemple.
[4.24s - 7.80s]  Et tu te dis, mais c'est quoi la base ? On parle de modèles qui génèrent des images,
[7.80s - 9.36s]  du texte, etc.

[0.00s - 1.72s]  Grâce à ça, entre autres, on découvre des matériaux.
[1.82s - 4.30s]  Est-ce qu'il y a des trucs comme ça, soit dans vos partenaires actuels ?

[0.00s - 2.86s]  ou dans les trucs que toi t'as vu qui t'ont aussi frappé ?

[0.00s - 2.08s]  La découverte de matériaux, c'est les mêmes techniques,
[2.20s - 2.88s]  ce n'est pas les mêmes modèles.
[3.18s - 4.78s]  Parce qu'en fait, ce qu'on fait, c'est qu'on séquence,
[4.90s - 6.22s]  de manière générale, il y a plein de problèmes
[6.22s - 9.58s]  qui se résolvent en séquençant, en sérialisme,
[9.58s - 11.72s]  de manière générale, le problème.
[12.04s - 14.10s]  Et la découverte de matériaux, c'est de la chimie,
[14.20s - 16.16s]  donc on peut sérialiser les molécules chimiques
[16.16s - 18.04s]  et demander au modèle de les prédire.
[18.34s - 21.62s]  Après, le paradigme, je prédis des séquences
[21.62s - 22.76s]  et j'ai plein de séquences sous la main
[22.76s - 24.66s]  de données particulières,
[25.10s - 27.66s]  ils marchent en chimie, ils marchent en médecine,

[0.00s - 1.70s]  Il marche certainement en sciences de la vie aussi.
[2.20s - 2.92s]  Non, on ne travaille pas là-dessus,
[3.02s - 5.48s]  mais on est ravis d'être partenaires
[5.48s - 7.24s]  avec des startups qui font ça, oui.

[0.00s - 4.68s]  Et là où, peut-être, comme tu disais, on arrive à une fin d'exponentiel éventuellement sur le texte,
[4.88s - 7.34s]  il y a peut-être plein d'autres domaines où, en fait...

[0.00s - 2.46s]  Le transfert, il fait que commencer quoi ?

[0.00s - 2.34s]  Sur le texte, on n'est pas du tout à saturation,
[2.48s - 6.44s]  puisque la prochaine étape, c'est d'avoir des modèles qui appellent plein d'outils.
[6.70s - 10.08s]  Du coup, ils deviennent beaucoup plus intelligents que juste des générateurs de texte.
[10.40s - 12.36s]  Et un des outils qu'ils peuvent appeler, c'est des simulateurs, par exemple.
[12.36s - 16.70s]  Donc la boucle de travail du concepteur de turbines,
[17.60s - 19.80s]  de la personne qui travaille sur un nouveau médicament,
[19.86s - 21.36s]  elle va complètement changer dans les années qui viennent.
[21.78s - 23.80s]  Et ça vient à la fois de modèles spécialisés,

[0.00s - 1.74s]  de prédiction de molécules par exemple,

[0.00s - 3.48s]  Mais ça vient aussi de l'interface avec la connaissance qu'on est en train de recréer.

[0.00s - 2.30s]  Tu vas pouvoir parler avec ton ordinateur et dire
[2.30s - 4.54s]  « Est-ce que tu peux me simuler une turbine qui ressemble à celle-là,
[4.60s - 6.24s]  mais un peu plus grande ? »

[0.00s - 1.48s]  et fais-moi plusieurs essais
[1.48s - 2.92s]  et dis-moi celle qui marcherait la mieux

[0.00s - 1.34s]  Et donc, la manière d'itérer,
[1.88s - 3.68s]  l'ingénierie, finalement, ça va aussi
[3.68s - 5.20s]  beaucoup changer de nature dans les années qui viennent, oui.

[0.00s - 2.94s]  C'est hyper intéressant parce qu'effectivement, les trucs auxquels on pense rapidement,
[3.16s - 5.32s]  c'est, tu parlais de simulateur, il y a le simulateur de code tout simplement.
[5.54s - 6.48s]  Ça, c'est des trucs qui se font un peu déjà.
[6.54s - 9.22s]  C'est le plus facile à simuler puisque ça reste au sein de la machine.
[9.66s - 10.54s]  Il n'y a pas de monde extérieur.
[11.00s - 12.56s]  Et donc, on voit déjà un peu le...

[0.00s - 2.20s]  une sandbox, par exemple, qui peut exécuter un bout de code
[2.20s - 3.92s]  et on voit si ça marche, si ça marche pas.
[4.20s - 4.84s]  Ça, c'est des trucs qui existent.

[0.00s - 3.62s]  Le raisonnement et le fait d'avoir un modèle qui génère du code qui est ensuite exécuté,
[3.86s - 5.12s]  on y travaille pour très prochainement.
[5.76s - 9.52s]  Et donc, c'est une grosse porte d'entrée vers des cas d'usage complexes.
[9.88s - 11.58s]  Tu as un espace ouvert d'outils à appeler maintenant,
[11.92s - 13.48s]  parce que tes outils, c'est tes librairies.
[13.86s - 15.78s]  Le niveau de contrôle aussi que tu dois mettre sur les modèles,
[16.22s - 18.68s]  ça devient plus compliqué à évaluer,
[18.80s - 20.24s]  parce que c'est plus ouvert, plus le monde est ouvert,
[20.38s - 21.36s]  plus c'est difficile à évaluer.
[21.68s - 22.58s]  Mais clairement, on va vers ça.
[22.88s - 24.98s]  Un modèle avec un exécuteur de code,
[25.20s - 27.74s]  c'est beaucoup plus performant qu'un modèle sans exécuteur de code.

[0.00s - 0.38s]  Dès qu'on crée,

[0.00s - 2.74s]  on commence à parler d'outils, d'exécuter des trucs du code.

[0.00s - 3.94s]  Moi, j'entends tout de suite d'autres profils dans l'écosystème de l'IA
[3.94s - 6.08s]  qui sont dans la frange très...

[0.00s - 1.90s]  très un peu alarmiste de

[0.00s - 1.66s]  de comment pourrait évoluer l'IA,
[2.14s - 5.28s]  et des possibilités que l'IA devienne rogue,
[5.78s - 6.86s]  et prenne son indépendance.
[7.00s - 8.34s]  Tu parles d'exécuter du code ?

[0.00s - 2.06s]  elle sortirait, on l'imagine sortir de la boîte.
[2.34s - 3.50s]  C'est quoi, toi, ton avis là-dessus ?

[0.00s - 1.92s]  Pour être caricatural, c'est un peu...

[0.00s - 2.94s]  un moyen de mettre en place des régulations pour...

[0.00s - 0.78s]  pour canasser tout ça,
[0.98s - 2.52s]  ou est-ce qu'il y a vraiment une des...

[0.00s - 0.88s]  des questions à se poser

[0.00s - 2.62s]  Il y a des questions à se poser sur comment on fait du nouveau logiciel
[2.62s - 5.26s]  avec des modèles qui sont fondamentalement...

[0.00s - 2.20s]  imprévisible, c'est-à-dire que par essence
[2.20s - 4.22s]  ce qu'il génère c'est aléatoire ?

[0.00s - 1.36s]  ça dépend de l'entrée

[0.00s - 2.42s]  Tu peux pas, en tant qu'humain, prédire ce que le modèle va sortir.

[0.00s - 1.24s]  Tu veux quand même faire du logiciel avec.
[1.90s - 3.20s]  Un logiciel, par essence,
[3.56s - 5.02s]  avant de le mettre à disposition du marché,
[5.12s - 7.02s]  tu veux vérifier qu'il couvre tous tes cas
[7.02s - 8.40s]  et qu'il n'y a pas de bug.
[8.92s - 11.62s]  Et donc la question de faire en sorte que ton système
[11.62s - 13.66s]  qui repose sur des LLM n'ait pas de bug,
[13.72s - 14.32s]  c'est une question difficile.

[0.00s - 2.00s]  C'est une question de contrôle, c'est une question d'évaluation.

[0.00s - 2.06s]  et finalement pour nous la question de sûreté

[0.00s - 1.20s]  en anglais c'est safety,
[1.72s - 3.00s]  c'est d'abord une question d'évaluation,
[3.52s - 5.08s]  c'est d'abord une question d'avoir les bons outils pour
[5.08s - 7.34s]  vérifier que ça fonctionne, et si ça
[7.34s - 9.00s]  fonctionne pas, avoir les bons outils pour
[9.00s - 11.04s]  corriger ça. Un des outils qu'on met à dispo,

[0.00s - 5.98s]  et qui est utile pour ça, c'est si tu veux contraindre l'espace des possibles de ton modèle et de ton système,
[6.38s - 8.08s]  tu contrains l'espace des entrées.

[0.00s - 3.84s]  Donc tu mets une modération sur le type de questions que l'utilisateur peut poser.
[4.10s - 5.48s]  Et comme ça, soudain tu passes de

[0.00s - 34.34s]  Un système qui peut répondre à toutes les questions, un système qui peut répondre que aux questions qui sont intéressantes pour le logiciel que tu crées. Pour nous, on voit vraiment l'aspect sûreté au niveau du système lui-même, et comme un problème d'intégration continue, comme un problème de test, il faut répondre à la question comment on construit des logiciels déterministes qui tournent sur des systèmes fondamentalement stochastiques. Ça, c'est la première chose. Ensuite, oui, il y a de la science-fiction, et puis, tu as quelques entreprises aux Etats-Unis qui ont un intérêt à dire aux régulateurs « Écoutez, cette technologie, elle est quand même un peu trop compliquée, un peu trop difficile à comprendre, un petit peu trop dangereuse.
[34.44s - 41.12s]  Imaginez que le truc devienne indépendant. » Bon, tu vas dire ça à des gens qui ne comprennent pas forcément exactement ce qui se passe. Ils peuvent se dire « Non, c'est pas mal. »

[0.00s - 3.36s]  Ah oui, peut-être que si on donnait ça à trois personnes...

[0.00s - 1.16s]  ou à deux personnes
[1.16s - 2.30s]  aux Etats-Unis
[2.30s - 3.14s]  on contrôlerait
[3.14s - 3.88s]  tout ce qui se passe
[3.88s - 4.76s]  et puis il n'y a pas de problème quoi

[0.00s - 1.72s]  Mais nous, on pense que c'est faux, c'est-à-dire que...

[0.00s - 4.34s]  avoir deux entités ou encore pire une entité qui contrôle tous
[5.04s - 9.80s]  tous les systèmes et qui ouvre sa porte à des auditeurs auxquels ils montrent ce qu'ils veulent
[9.96s - 12.48s]  on pense que c'est pas la bonne solution la bonne solution

[0.00s - 1.36s]  en sûreté logicielle.

[0.00s - 25.98s]  C'est l'open source de manière générale. On l'a montré en cyber, on l'a montré sur les systèmes les plus fiables aujourd'hui, les operating systems les plus fiables, c'est Linux. Le fait d'avoir le plus d'yeux possible sur une technologie, le fait de la distribuer le plus possible, c'est une manière de faire en sorte que le contrôle de cette technologie soit sous un contrôle démocratique. Et donc, nous c'est ce qu'on dit. Et puis quand on entend les doomers raconter autre chose, il y a des gens qui sont de bonne foi, il faut le reconnaître, qu'ils ont vraiment peur
[25.98s - 27.44s]  que ce sont des choses qui vont se passer.
[27.72s - 29.60s]  Et puis il y a surtout beaucoup de gens qui ne sont pas du tout de bonne foi.
[29.96s - 31.66s]  Je pense qu'il est important
[31.66s - 32.72s]  de vérifier

[0.00s - 15.82s]  Ça ne doit pas être simple parce qu'en face, l'argument, il est super facile à comprendre justement quand tu disais pour quelqu'un qui n'est pas forcément adepte du sujet, on te dit voilà un outil dangereux, est-ce qu'il ne faudrait pas éviter qu'on le mette dans trop de mains ? Tu pars déjà avec un truc un peu dur à défendre quoi.

[0.00s - 1.74s]  L'atout qu'on a, on a un atout historique.
[1.88s - 4.02s]  C'est-à-dire que ce n'est pas la première fois qu'on a ce débat.
[4.20s - 5.30s]  On a eu ce débat pour l'Internet.
[5.42s - 8.14s]  Internet, ça aurait pu être un truc contrôlé par trois entreprises
[8.14s - 9.86s]  qui auraient fait leur propre nette.

[0.00s - 0.72s]  Moi je trouve qu'on meurt.

[0.00s - 1.80s]  qui auraient refusé de standardiser les choses.
[2.28s - 4.60s]  Et en fait, finalement, il y a eu suffisamment de pression.
[5.08s - 6.72s]  À un moment donné, le régulateur s'est dit
[6.72s - 9.20s]  « Ouais, on va faire en sorte qu'elle soit standardisée. »
[9.20s - 10.72s]  Et donc, Internet, ça appartient à tout le monde maintenant.

[0.00s - 3.54s]  Il aurait suffi que des personnes différentes fassent des choix différents, quelques personnes,

[0.00s - 5.98s]  Et on serait dans une situation où, en fait, il y a trois wall gardens...

[0.00s - 0.94s]  non interopérables.

[0.00s - 2.60s]  Ça aurait pu être la même chose pour le end-to-end encryption.
[2.94s - 3.98s]  Ça, c'est un autre exemple.
[4.30s - 6.34s]  À une époque, c'était considéré comme une arme
[6.34s - 9.30s]  et il y avait un contrôle des exports des États-Unis.
[9.78s - 10.50s]  En fait, on est ce qu'il fallait.

[0.00s - 1.16s]  En fait, on se dit pas fou maintenant.

[0.00s - 2.08s]  Et en fait, on se pose la question maintenant sur les poids.
[2.68s - 5.52s]  Il y a parfois certains régulateurs qui se posent cette question-là.
[6.06s - 8.82s]  Mais ça paraît fou pour le end-to-end encryption.
[8.98s - 11.40s]  Nous, ce qu'on pense, c'est que dans dix ans, ça paraîtrait complètement fou pour des poids d'un modèle.
[11.76s - 16.20s]  Parce que c'est tellement infrastructurel, c'est tellement une ressource qui doit être partagée par tout le monde,
[16.56s - 18.68s]  cette compression de la connaissance et cette intelligence,
[19.18s - 22.98s]  que pour nous, c'est criminel de la laisser entre les mains de deux entités
[22.98s - 24.96s]  qui ne sont pas du tout sous contrôle démocratique.

[0.00s - 3.88s]  Et pour défendre cette vision-là que le contrôle doit avoir lieu un peu plus tard
[3.88s - 6.00s]  dans la chaîne, au moment de l'interface par exemple,
[6.00s - 8.50s]  ou par l'entreprise vis-à-vis de son client,

[0.00s - 0.98s]  Tu vas notamment au Sénat.

[0.00s - 3.48s]  On t'a vu sur YouTube parler au Sénat.
[3.98s - 4.94s]  Ça fait quoi de parler ?
[4.94s - 9.70s]  D'essayer d'expliquer ce que c'est un modèle, un dataset, un LLM à des sénateurs ?

[0.00s - 25.52s]  C'est intéressant. C'est un exercice. Il y avait des bonnes questions, peut-être posées par des gens qui comprenaient un peu moins la technologie, on va dire. Mais je pense que c'est important. De manière générale, c'est des représentants des citoyens et il faut qu'ils comprennent que c'est une technologie qui va affecter les citoyens. Donc nous, on est prêts à investir du temps parce que mieux c'est compris, plus on comprend que c'est aussi un enjeu de souveraineté, c'est aussi un enjeu culturel. C'est un enjeu d'avoir des acteurs comme nous,
[25.68s - 27.88s]  et pas que nous, mais des acteurs comme nous sur le sol européen.
[27.98s - 28.66s]  Parce que si ce n'est pas le cas,
[29.10s - 33.52s]  le sujet, c'est qu'on a une dépendance économique énorme aux États-Unis
[33.52s - 36.34s]  et ça, elle est très, très dommageable à long terme.

[0.00s - 10.50s]  Le fait d'aller parler aux gens qui font les lois, aux gens qui vont aussi parler avec leurs citoyens, comprennent leurs angoisses, c'est une manière de dédramatiser cette technologie.
[10.78s - 16.32s]  C'est une technologie qui va apporter beaucoup de bénéfices dans l'éducation, dans la santé, dans la manière dont on travaille.

[0.00s - 8.84s]  Et il faut que les représentants de la démocratie française, de la démocratie européenne, de la démocratie américaine, aient conscience, savent de quoi il s'agit.
[9.12s - 14.14s]  Moi, je dois dire que personnellement, je n'avais pas tellement prévu de faire ça en démarrant la boîte, mais il faut faire entendre savoir,
[14.30s - 20.66s]  parce que sinon, le vide est comblé par des gens qui n'ont pas forcément des intérêts alignés avec...

[0.00s - 3.26s]  la démocratie est certainement pas alignée avec ce que nous on essaie de faire.

[0.00s - 0.88s]  Si vous n'avez pas su,

[0.00s - 5.40s]  l'histoire du Vesuvius Challenge ou comment un papyrus a été décrypté par un étudiant en IA.
[5.40s - 7.60s]  Allez voir cette vidéo, c'était vraiment passionnant !

[0.00s - 87.12s]  Aujourd'hui, on reçoit une légende de la tech française, Arthur Mench, cofondateur de Mistral AI, la seule entreprise d'Europe capable de tenir tête à OpenAI et aux GAFAM dans leur course à l'intelligence artificielle. En à peine un an, lui et ses deux associés ont réussi l'impossible, lever plus d'un milliard d'euros, développer des modèles d'IA qui rivalisent avec Chagipity et transformer leur start-up parisienne en une entreprise valorisée 6 milliards d'euros. Dans cet épisode exceptionnel, Arthur va nous dévoiler les secrets de cette success story, comment trois Français ont quitté leur job en or chez Google et Meta pour se lancer dans cette aventure folle, comment ils rivalisent avec des géants qui ont 100 fois plus de puissance de calcul qu'eux et surtout la guerre des talents qui fait rage en coulisses entre Mistral et les GAFAM pour attirer les meilleurs ingénieurs. On va aussi lui demander si d'après lui les modèles d'IA ont atteint un plafond et qu'est-ce qu'ils nous réservent pour la suite. Je suis très excité et honoré de pouvoir vous partager cette conversation avec Arthur Mech. Mais juste avant, j'ai un message pour tous ceux qui hésitent à prendre un abonnement chat GPT. Notre partenaire du jour, Mammouth Ayaï eu une idée assez brillante. Rassembler tous les meilleurs modèles d'IA dans une seule interface et derrière un unique abonnement. Pour 10 euros par mois, vous avez accès aux modèles de langages les plus récents, O1, Grock, DeepSync, et même des modèles de génération d'images comme Midjournay ou Flux. Quand on sait que cumuler tous ces abonnements, ça coûterait dans les 80-100 euros, c'est assez imbattable. Si vous avez besoin de générer beaucoup d'images par mois, ils ont aussi des plans
[87.12s - 88.12s]  un peu plus chers.
[88.12s - 90.12s]  Le truc cool, c'est qu'ils sont toujours à l'affût des nouvelles sorties.
[90.12s - 92.62s]  Par exemple, ils ont déjà flux pour la génération d'images.
[92.62s - 95.76s]  Et globalement, c'est juste agréable de ne pas avoir à changer d'interface tout
[95.76s - 96.76s]  le temps.
[96.76s - 98.76s]  Je vous mets le lien vers leurs différentes formules en description et on reprend.
[98.76s - 102.14s]  C'est quoi l'élément déclencheur déjà pour se dire « on va créer notre propre
[102.14s - 105.34s]  société face à ces géants » quand on est déjà bien installé.

[0.00s - 0.50s]  confortable.

[0.00s - 5.94s]  Je pense qu'il y a deux conversations, une en septembre 2022 avec Timothée et une en novembre 2022 à NeurIPS,
[5.94s - 9.48s]  qui est la grosse conférence de machine learning avec Guillaume,
[9.84s - 13.94s]  où on s'est rendu compte qu'on avait des aspirations similaires de lancer une entreprise...

[0.00s - 3.08s]  en France et qu'on connaissait pas mal de gens que ça intéresserait.

[0.00s - 3.12s]  Et donc à partir de là, c'est un peu le début de l'engrenage.
[3.24s - 5.64s]  C'est-à-dire qu'au début, tu te dis « Ah, c'est peut-être une bonne idée ».
[5.64s - 7.74s]  Et puis au fur et à mesure, chaque jour qui passe,
[8.00s - 10.30s]  tu t'impliques de plus en plus émotionnellement dans cette idée.
[10.74s - 12.38s]  Puis à un moment donné, tu as un peu un point de non-retour
[12.38s - 15.92s]  parce qu'en fait, tu es plus dans l'idée que dans le travail dans ton entreprise actuelle.

[0.00s - 6.04s]  Mais à partir de février, c'est vrai qu'on s'est dit, là on peut avoir 15 personnes, on peut aller vite, on sait le faire,
[6.34s - 11.20s]  on peut démontrer que l'Europe peut faire des choses intéressantes dans le domaine et peut reprendre une position de leadership.
[11.66s - 15.26s]  Et donc c'est comme ça que ça s'est fait, et à partir d'avril, on s'est lancé, quoi.

[0.00s - 5.22s]  Donc, Tau, il y a déjà cette idée que le projet, c'est de faire de l'IA très performante européenne

[0.00s - 0.96s]  plus ça que

[0.00s - 7.40s]  Juste, on se sent un petit peu peut-être ralenti par une grosse structure au-dessus de nous, donc Meta ou Google, et on pense aller plus vite tout seul. C'était quoi ?

[0.00s - 6.44s]  Non, tu avais les deux. En fait, Guillaume, Timothée et moi, on travaillait sur ce sujet depuis à peu près 2020.
[6.74s - 10.38s]  Et on a vu ce qu'on pouvait faire avec des petites équipes très concentrées.
[10.90s - 16.48s]  C'est vrai qu'en 2022, ces équipes sont devenues moins concentrées parce que c'est le moment où le monde a réalisé
[16.48s - 19.54s]  qu'il y avait une opportunité économique autour des modèles de langue.
[19.54s - 26.30s]  Et donc, on s'est dit qu'on pouvait bénéficier aussi de cet aspect de désorganisation pour nous être mieux organisés
[26.30s - 27.66s]  et fournir des choses plus rapidement.

[0.00s - 1.14s]  Ça se passe comment le tout début ?

[0.00s - 3.34s]  Vous avez chacun une spécialité ?
[3.34s - 5.28s]  Comment vous vous organisez au tout début de la boîte ?

[0.00s - 1.32s]  On vient tous les trois du même...

[0.00s - 0.42s]  Même formation.

[0.00s - 3.50s]  De la même formation, on a fait la même chose, on a tous les trois des thèses en machine learning.
[4.24s - 9.56s]  C'est vrai qu'on s'est rapidement spécialisé avec Guillaume, qui est le scientifique le plus fort d'entre nous,
[9.62s - 11.08s]  qui a pris la partie scientifique.
[11.72s - 15.16s]  Timothée, qui est plus un ingénieur et qui s'est occupé de faire toute l'infrastructure
[15.16s - 18.76s]  et de monter l'équipe d'ingénieurs produits aussi.
[19.38s - 25.44s]  Et moi, j'ai assez vite fait l'aspect lever de fond, l'aspect parler avec des clients...

[0.00s - 4.44s]  C'est des choses que j'aimais bien faire, on s'est répartis comme ça assez vite.
[5.00s - 7.86s]  Et pour revenir à comment ça démarre, ça démarre par une levée de fonds,
[8.02s - 13.00s]  parce qu'il faut la capacité de calcul et il faut la capacité humaine pour avancer assez vite.
[13.44s - 16.28s]  Et donc on a fait une levée de fonds en quelques semaines,
[16.78s - 20.14s]  et à partir de là on était partis pour faire le premier modèle en septembre.

[0.00s - 2.36s]  C'est-à-dire qu'il n'y a pas une seule ligne de code, en fait ?

[0.00s - 2.76s]  Avant même de savoir que c'est bon, il y a une levée de fonds qui va se faire.
[3.04s - 5.58s]  C'est un domaine où il faut forcément attendre la levée de fonds.
[5.64s - 7.16s]  Si on veut commencer la première...

[0.00s - 2.10s]  On peut un peu paralléliser, commencer à faire du code.

[0.00s - 0.92s]  un peu paralléliser ?

[0.00s - 1.10s]  À trois, tu n'as pas beaucoup de levier.
[1.32s - 2.80s]  Il vaut mieux avoir une petite équipe
[2.80s - 4.94s]  d'une dizaine de personnes pour aller plus vite.
[5.40s - 7.28s]  On a commencé par la data, parce qu'il faut la donner
[7.28s - 8.40s]  pour entraîner les modèles.
[8.62s - 10.34s]  Il y a beaucoup de travail manuel là-dessus.

[0.00s - 3.16s]  à faire et Guillaume, Timothée essentiellement avaient commencé

[0.00s - 1.08s]  pendant qu'on finissait la levée.

[0.00s - 0.16s]  OK.

[0.00s - 1.40s]  on parle des levées de fonds.
[1.48s - 4.40s]  Toi, tu as fait Polytechnique, Centra de Paris, l'UNS et un doctorat.
[4.84s - 7.10s]  Est-ce que ça aide quand même à lever des fonds
[7.10s - 8.72s]  alors que vous n'êtes que trois ?
[8.72s - 12.06s]  Ou alors, est-ce que c'est encore plus le non-méta Google ?
[12.06s - 13.40s]  Tu dirais que c'est quoi qui aide le plus ?

[0.00s - 6.36s]  Je pense que ce qui a aidé au démarrage, c'est qu'on était crédibles sur le domaine le plus chaud du moment en 2023
[6.36s - 11.50s]  et qu'on avait plutôt des papiers qui étaient liés à ce domaine-là.
[11.66s - 14.10s]  C'est-à-dire que moi, j'étais dans l'équipe qui travaillait à DeepMind là-dessus.
[14.64s - 17.18s]  Guillaume et Timothée, ils étaient à Meta, c'est eux qui ont fait de la mâle le premier.
[18.06s - 22.30s]  Et donc cette crédibilité-là, ce n'est pas ce qu'on a fait dans notre jeunesse à l'école.
[22.30s - 26.56s]  C'est plutôt une crédibilité scientifique qu'on a construite dans notre première partie de carrière, on va dire.

[0.00s - 2.98s]  Un alignement de planète avec ce qui intéresse le plus
[2.98s - 5.08s]  et les meilleures personnes pour le développer, en fait.

[0.00s - 5.78s]  Oui, effectivement, notre crédibilité venait aussi du fait qu'on avait une excellente équipe de démarrage
[5.78s - 9.68s]  et qu'on pouvait démontrer qu'on saurait la recruter.

[0.00s - 3.20s]  Et il y a ce jour qui arrive, c'est le 27 septembre 2023,
[3.42s - 7.46s]  où vous postez un lien sur votre compte Twitter parfaitement inactif,
[7.58s - 8.84s]  et c'est votre premier modèle en fait.
[9.18s - 10.78s]  Donc Mistral 7B.

[0.00s - 1.98s]  Le tweet est vu plus d'un million de fois.
[2.12s - 3.84s]  Vous êtes repris par tous les médias américains,
[3.94s - 4.68s]  tout le monde de l'IA.

[0.00s - 2.00s]  et en effervescence et s'amuse avec le modèle.
[2.00s - 4.00s]  Il est téléchargé un million de fois, mais super vite.

[0.00s - 1.14s]  Nous, on a vu ça de l'extérieur.

[0.00s - 1.54s]  on a vu cet engouement. Vous,

[0.00s - 1.28s]  De l'intérieur, c'était comment ?

[0.00s - 45.48s]  Le tweet, c'était une idée de Guillaume, le chief scientist, pour rendre à César ce qui appartient à César. Parce que vous ne le publiez pas comme les autres. On ne le publie pas comme les autres. Effectivement, on a mis à disposition un magnet link qui permet de le télécharger en BitTorrent. C'est comme ça qu'on a parlé la première fois et c'était une excellente idée. C'était une journée où on avait aussi prévu de faire de la communication plus habituelle. Moi, j'étais allé parler aux journalistes Figaro etc et donc il fallait mettre le torrent le matin et puis l'embargo était vers 16h donc il y avait cette période où en fait on avait rompu l'embargo mais bon les journalistes a priori allaient pas comprendre ce qui se passait donc ça se passait bien c'est moi qui postais, je crois c'était à 5h du mat donc j'avais mis un petit réveil parce que j'étais pas sûr du schedule send de Twitter
[45.48s - 46.94s]  ça s'appelait encore Twitter à l'époque
[46.94s - 48.20s]  et je l'ai mis
[48.20s - 49.30s]  et après je suis allé me recoucher
[49.30s - 50.12s]  et ensuite on a vu
[50.12s - 51.32s]  que ça avait bien démarré au démarrage
[51.32s - 52.08s]  c'est un truc où
[52.08s - 53.76s]  vous vous y attendiez un petit peu
[53.76s - 54.22s]  ou quand

[0.00s - 1.74s]  On savait que le modèle était bon
[1.74s - 4.68s]  On savait qu'on était largement au-dessus
[4.68s - 6.58s]  Des meilleurs modèles open source
[6.58s - 10.18s]  Qu'on avait visé explicitement cette taille-là
[10.18s - 11.76s]  Parce qu'on savait que ça tournait sur des laptops aussi
[11.76s - 13.88s]  Donc ça voulait dire que
[13.88s - 15.84s]  Tous les hobbyistes allaient pouvoir jouer avec
[15.84s - 17.30s]  Et ça n'a pas manqué, ça a fonctionné
[17.30s - 20.24s]  Donc on se doutait qu'on allait être remarqué
[20.24s - 21.56s]  Ce qu'on ne se doutait pas
[21.56s - 23.46s]  C'est que les gens allaient le mettre dans des perroquies en peluche
[23.46s - 25.64s]  Et ce genre de choses en un mois
[25.64s - 28.00s]  La réception était plus grande que ce qu'on espérait
[28.00s - 28.74s]  On était très contents

[0.06s - 0.82s]  Il y a un autre truc
[0.82s - 1.96s]  qui s'est passé nécessairement
[1.96s - 3.12s]  en publiant des modèles
[3.12s - 4.24s]  avec des poids ouverts comme ça.
[4.36s - 5.76s]  C'est que ça laisse la porte
[5.76s - 6.98s]  à tout ce qui est
[6.98s - 8.46s]  entraînement, fine tuning.
[8.84s - 9.68s]  Et tout le monde
[9.68s - 10.42s]  s'en est donné à cœur joie.
[10.50s - 11.90s]  Je pense que c'était déjà
[11.90s - 12.54s]  un peu le cas
[12.54s - 13.40s]  sur les modèles de l'Yama.
[13.60s - 14.42s]  Mais je me souviens
[14.42s - 14.82s]  que c'est un modèle
[14.82s - 15.60s]  qui a été beaucoup,
[15.72s - 16.50s]  beaucoup réentraîné.
[16.72s - 17.28s]  C'est quoi les...

[0.00s - 0.42s]  Les...

[0.00s - 0.48s]  fine-tuning
[0.48s - 1.66s]  un peu étonnant
[1.66s - 2.60s]  ou curieux
[2.60s - 3.14s]  dont tu te souviens
[3.14s - 3.70s]  de ce modèle-là
[3.70s - 4.08s]  ou d'autres ?
[4.08s - 4.78s]  Il y a quelqu'un
[4.78s - 4.88s]  qui s'est

[0.00s - 2.94s]  qui nous avait entraîné ce modèle pour parler aux morts.

[0.00s - 5.24s]  Je ne sais plus comment ça s'appelait mais il avait fait un fine tuning un petit peu
[5.24s - 10.40s]  ésotérique et le truc marchait relativement bien donc c'était assez marrant.
[10.40s - 14.64s]  C'est vrai que cette taille là c'est aussi une taille où tu peux fin tuner même sur
[14.64s - 18.60s]  des gros PC de gaming éventuellement et puis ça ne coûte pas très cher et ça permet
[18.60s - 22.96s]  de rentrer du style, ça permet de faire du roleplay et donc les gens s'en sont donné
[22.96s - 24.26s]  à coeur de joie effectivement.

[0.00s - 0.82s]  pour expliquer.

[0.00s - 3.66s]  Il y a le modèle de fondation qui est lui le plus coûteux et le plus compliqué.
[3.66s - 5.82s]  Et j'imagine qu'il contient en gros l'information.
[5.82s - 10.34s]  Et après, le fine tuning, c'est conversationnel, c'est en faire un bon agent de discussion.

[0.00s - 3.06s]  Oui, il faut voir la première phase comme une compression de la connaissance humaine,
[3.22s - 8.12s]  et la deuxième phase comme une manière d'instruire le modèle à suivre ce qu'on lui demande de faire.
[8.24s - 11.08s]  Donc on le rend contrôlable, et une manière de le contrôler, c'est de le rendre...

[0.00s - 0.94s]  de le rendre conversationnel.

[0.00s - 1.64s]  Donc ces deux phases-là sont assez distinctes, effectivement.

[0.00s - 0.98s]  Et est-ce que sur cette deuxième

[0.00s - 4.74s]  phase, il y a des trucs d'indépendants tout seuls qui ont testé des choses sur du fine
[4.74s - 7.06s]  tuning et ont découvert des bonnes personnes.

[0.00s - 4.48s]  technique ? Ouais, on a appris des trucs, je ne vais pas rentrer dans les détails, mais il y avait
[4.48s - 9.64s]  de direct preference optimization, c'est un peu du jargon, mais qu'on n'avait pas fait sur le
[9.64s - 13.30s]  premier modèle et on a vu des gens le faire, on s'est dit ça devrait bien marcher sur le deuxième
[13.30s - 16.22s]  modèle et ça a bien marché sur le deuxième modèle. Maintenant on fait d'autres choses,
[16.92s - 21.04s]  mais effectivement une des raisons pour lesquelles on a lancé la boîte au-delà de l'Europe, c'est
[21.04s - 25.24s]  aussi l'aspect ouvert et l'aspect contribution de la communauté. C'est

[0.00s - 7.82s]  En fait, l'EI entre 2012 et 2022, ça s'est construit les uns par-dessus les autres pendant les conférences, les grosses boîtes par-dessus les grosses boîtes.
[8.10s - 9.58s]  Puis soudain, quand c'est devenu...

[0.00s - 1.98s]  un modèle économique intéressant
[1.98s - 2.90s]  les gens ont arrêté
[2.90s - 4.08s]  les grosses entreprises
[4.08s - 4.30s]  ont arrêté
[4.30s - 5.52s]  et donc
[5.52s - 7.14s]  on a essayé de prolonger ça
[7.14s - 7.74s]  un peu
[7.74s - 8.76s]  avec ce qu'on a fait

[0.00s - 0.22s]  Réalisé.

[0.00s - 2.72s]  Oui, aujourd'hui, tu as vraiment deux camps distincts.
[2.72s - 9.02s]  C'est assez particulier sur, d'un côté, les Anthropik, OpenAI et compagnie qui ne publient plus grand-chose.
[9.14s - 11.82s]  Google aussi, j'ai l'impression, a beaucoup ralenti les publications.
[12.34s - 14.12s]  Et de l'autre côté, les Chinois, bizarrement.

[0.00s - 0.16s]  Donc...

[0.00s - 3.18s]  Pourquoi les Chinois, ils sont autant à fond dans les modèles open source ?
[3.18s - 3.86s]  C'est curieux, non ?

[0.00s - 1.40s]  Je pense qu'ils sont en position de challenger.
[2.06s - 4.86s]  Est-ce que l'open source, c'est une bonne stratégie de challenger ?
[4.86s - 7.70s]  On en est, je pense, la bonne illustration.
[8.10s - 9.66s]  Je pense qu'ils ont des bonnes techniques,
[9.78s - 11.78s]  ils ont des bons renseignements aussi, c'est vrai.

[0.00s - 3.52s]  Mais ils ont beaucoup fait avancer la science, les nouvelles techniques.
[3.72s - 4.98s]  C'est clairement ceux qui publient le plus, effectivement.

[0.00s - 3.36s]  Et tu parlais de la position de challenger, est-ce que Meta, quand il publie Yama pour la première fois,
[3.50s - 5.08s]  ils sont en position de challenger à ce moment-là ?

[0.00s - 1.22s]  C'est les Timothée et Guillaume.
[1.40s - 3.64s]  Je pense qu'ils sont dans la position de challenge parce qu'ils n'ont pas encore parlé.

[0.00s - 7.96s]  Et je pense qu'avec le mouvement qu'on a perpétué avec nos modèles en septembre et en décembre en particulier,
[8.26s - 12.74s]  donc Mistral 7B, Mistral 8X 7B, je pense qu'on a lancé cette route l'open source.
[12.90s - 17.00s]  Et donc il y a aussi un peu une concurrence sur qui fait les meilleurs modèles open source.
[17.08s - 18.20s]  Je pense que ça bénéficie à tout le monde.
[18.40s - 20.44s]  Et donc on est ravi d'avoir bien participé à ça.

[0.00s - 0.44s]  On va nous avoir

[0.00s - 1.22s]  Oui, c'est vrai.

[0.00s - 0.22s]  C'est...

[0.00s - 0.16s]  ...

[0.00s - 0.18s]  ...

[0.00s - 0.48s]  La régalade.
[0.68s - 2.32s]  Qu'est-ce qui fait, tu penses qu'à ce moment-là,
[2.58s - 3.94s]  vous avez autant d'avance, vous ?
[3.94s - 6.26s]  Après, il y a un yo-yo avec tout le monde qui se produit,
[6.74s - 9.10s]  mais là, il y a une vraie avance sociale.

[0.00s - 0.72s]  Indiscutable quoi.

[0.00s - 4.92s]  Je pense qu'on connaissait l'importance de la donnée et on a beaucoup travaillé là-dessus.
[5.48s - 12.00s]  On savait aussi comment entraîner le modèle de manière efficace parce qu'on avait trois
[12.00s - 13.62s]  ronds d'expérience chacun dans ce domaine.

[0.00s - 1.06s]  des bonnes connaissances
[1.06s - 1.90s]  et on a insisté
[1.90s - 3.04s]  sur les aspects
[3.04s - 3.50s]  de l'entraînement
[3.50s - 4.30s]  qui ont le plus de leviers
[4.30s - 4.90s]  c'est-à-dire la qualité
[4.90s - 5.30s]  de la donnée.

[0.00s - 1.90s]  Effectivement, c'est derrière un peu tout...

[0.00s - 2.50s]  l'évolution de la recherche, j'ai l'impression de temps vert.
[2.50s - 4.86s]  En fait, il n'y a que la donnée qui compte.

[0.00s - 0.92s]  Grosse partie.

[0.00s - 1.52s]  la donnée et la quantité de calcul.

[0.00s - 3.58s]  Il y a aussi le compute, et c'est lié à un autre sujet très important, c'est

[0.00s - 0.96s]  les fonds, tout simplement.
[1.52s - 3.10s]  En un an, vous avez en tout levé quand même
[3.10s - 5.76s]  un milliard d'euros, ce qui est vertigineux.
[5.92s - 7.54s]  Vous avez aussi sorti plein de nouveaux
[7.54s - 9.64s]  modèles, des Pixral, par exemple,
[9.76s - 11.38s]  des modèles un peu différents, multimodaux, etc.
[11.76s - 13.10s]  Comment vous approchez le fait que, justement,
[13.16s - 15.36s]  en termes de quantité de compute, par rapport
[15.36s - 16.54s]  à un méta, par exemple, qui a

[0.00s - 2.68s]  qui aura à la fin de l'année 350 000 H...

[0.00s - 0.82s]  100, c'est ça ?
[0.82s - 0.92s]  Oui.
[1.16s - 2.02s]  Si je ne dis pas de bêtises.
[2.06s - 2.32s]  En GPU.
[2.88s - 6.02s]  Est-ce que justement, il n'y a pas le choix que de passer par des très grosses levées de fonds ?

[0.00s - 3.30s]  Mais après, comme on pérennise le truc, c'est quoi un peu ta vision du compute ?

[0.00s - 3.98s]  Nous, notre vision, c'est qu'on a besoin de compute, mais on n'a pas besoin de 350 000 à champs.

[0.00s - 10.86s]  Et donc ça a été toujours notre test qu'on pouvait être plus efficace, qu'on pouvait en étant focalisé sur le fait de faire des excellents produits et ne pas faire plein d'autres choses à côté.
[11.02s - 14.74s]  Parce que nos concurrents américains ont tendance à faire beaucoup de choses à côté.
[15.06s - 17.98s]  L'allocation de ressources, c'est une question importante.

[0.00s - 0.84s]  constante chez nous.
[1.02s - 1.30s]  C'est un peu le nerf
[1.30s - 1.82s]  de l'aïr quoi.
[2.08s - 3.82s]  C'est arriver à tenir
[3.82s - 4.80s]  l'amélioration des modèles
[4.80s - 5.28s]  dans le temps
[5.28s - 7.04s]  versus le cramage
[7.04s - 8.08s]  du compute.

[0.00s - 4.16s]  Il faut gérer le budget, il faut être intelligent pour ne pas dépenser trop.
[5.98s - 9.74s]  Tout est une question de mettre le curseur au bon endroit et d'avoir les bons compromis.
[10.04s - 12.58s]  Ce n'est pas facile, mais je pense que pour le moment, on a bien réussi.
[12.90s - 14.90s]  On a réussi à avoir des modèles qui sont très performants,
[15.42s - 18.80s]  avec un niveau de dépense de capital qui est quand même très contrôlé.

[0.00s - 0.10s]  Ouais.

[0.00s - 4.26s]  Est-ce que justement, j'ai vu que parmi vos investisseurs

[0.00s - 2.04s]  dans les derniers rounds, je crois, il y a NVIDIA.
[2.68s - 5.54s]  Est-ce que ça passe par des acteurs qui, eux,
[5.68s - 8.38s]  ont un peu le contrôle sur le hardware,
[8.70s - 10.98s]  ou l'infrastructure, ou les data centers ?
[10.98s - 12.62s]  Il y a Microsoft aussi, je crois, avec qui vous avez bossé.
[12.94s - 14.20s]  Est-ce que ça passe aussi par ça, justement,
[14.30s - 15.56s]  s'entourer des bonnes personnes ?

[0.00s - 0.68s]  Faut les bons partners

[0.00s - 6.48s]  Il faut les bons partenaires de distribution en particulier parce que le calcul passe souvent par le cloud.
[7.00s - 12.52s]  Et donc on a comme partenaire tous les fournisseurs de cloud américains parce que c'est quand même les plus gros.
[12.82s - 15.84s]  On a aussi des fournisseurs français, on a Outscale qui en travaille.
[16.32s - 18.32s]  Et puis Nvidia c'est un fournisseur quasiment de cloud aussi.
[18.58s - 19.76s]  Donc à ce titre on travaille avec eux.
[20.04s - 23.56s]  On a fait de la R&D avec un modèle qui s'appelle Mistral Nemo.

[0.00s - 1.12s]  Imaginez, il y a des gens qui nous

[0.00s - 3.30s]  écoutes qui n'ont pas su. Est-ce que tu peux nous expliquer c'est quoi aujourd'hui la

[0.00s - 1.50s]  des modèles qui sont à jour.
[1.50s - 3.50s]  Moi, j'ai vu que dans les derniers mises à jour récents,
[3.50s - 4.38s]  il y a le large 2.

[0.00s - 3.82s]  Oui, alors maintenant on les numérote comme Ubuntu, donc 24.11.
[4.94s - 11.24s]  Et donc celui-là, Mistral Large 24.11, il est très fort pour appeler des fonctions, orchestrer des choses.
[11.58s - 14.76s]  Parce qu'en fait les modèles ça génère du texte, c'est l'utilisation de base.
[15.32s - 19.60s]  Mais ce qui est intéressant c'est quand ils génèrent des appels à des outils et qu'on les utilise comme des orchestrateurs,
[20.00s - 22.70s]  comme un peu des operating systems.
[23.20s - 27.90s]  Et donc on travaille beaucoup sur le fait d'avoir des modèles qui puissent être connectés à plein d'outils différents

[0.00s - 2.24s]  auxquelles on peut poser des questions, auxquelles on peut donner des tâches
[2.24s - 4.10s]  et qui vont réfléchir d'eux-mêmes aux outils qu'ils vont appeler.
[4.46s - 6.18s]  Et donc on insiste beaucoup là-dessus.
[6.28s - 8.92s]  Et donc la nouvelle version de Mistral Large, elle est particulièrement forte là-dessus.

[0.00s - 0.14s]  Oui.

[0.00s - 0.82s]  Après, il y a...

[0.00s - 1.16s]  Il y a eu des mixtrales aussi.
[1.68s - 3.78s]  Ça, pour comprendre, c'est plutôt pour servir,
[4.00s - 4.78s]  pour une entreprise, par exemple,
[4.86s - 6.52s]  pour servir beaucoup d'utilisateurs en même temps ?

[0.00s - 4.96s]  C'est un autre type d'architecture qui est particulièrement pertinent quand on a une forte charge.
[5.52s - 8.46s]  Beaucoup d'utilisateurs, c'est des choses que nous on utilise par exemple.
[8.98s - 9.94s]  C'est des mixtures.
[10.58s - 11.28s]  C'est des mixtures.
[11.28s - 12.54s]  Une sorte de cerbère à 8 têtes.
[12.72s - 18.94s]  C'est ça, c'est plusieurs modèles en même temps et chaque mot passe sur le modèle le plus adapté.
[18.94s - 21.70s]  Pour plusieurs raisons, ça permet de mieux utiliser les GPUs.

[0.00s - 0.44s]  En ligne

[0.00s - 0.54s]  parce qu'en fait

[0.00s - 0.48s]  sur le cerveau.

[0.00s - 0.94s]  Et derrière, il y a les plus petits.
[1.54s - 5.34s]  Il y a les petits modèles qui passent sur les laptops, qui passent sur les smartphones.
[5.78s - 9.38s]  Et ceux-là, ils sont particulièrement adaptés à des usages de hobbyistes
[9.38s - 12.62s]  parce qu'il n'y a pas besoin d'aller dans le cloud, on peut les modifier facilement.
[13.10s - 16.64s]  Et puis, très vite, c'est aussi pas mal focalisé sur cet aspect petit et rapide
[16.64s - 18.90s]  parce que c'est vraiment l'ADN de l'entreprise.
[19.26s - 21.06s]  Nous, aujourd'hui, le produit, ce n'est pas le modèle.
[21.06s - 23.60s]  Le produit, c'est la plateforme pour les développeurs.

[0.00s - 0.06s]  ...

[0.00s - 1.56s]  Donc le produit, c'est

[0.00s - 2.22s]  la plateforme pour construire des applications en tant que développeur,
[2.22s - 3.20s]  et là-dedans il y a des modèles.

[0.00s - 2.34s]  et puis un ensemble d'applications

[0.00s - 1.66s]  qui permettent de gagner en productivité.
[1.92s - 2.90s]  C'est un milieu qui est super

[0.00s - 0.56s]  compétitif.

[0.00s - 2.86s]  Évidemment, que ce soit, on l'a dit, sur les modèles,
[2.94s - 4.58s]  mais aussi sur tout ce qu'il y a autour,
[4.78s - 7.86s]  sur comment améliorer l'expérience, les interfaces de chat, etc.
[8.60s - 11.16s]  On a vu les systèmes d'interface qui se bougent.
[11.24s - 13.72s]  Tout le monde est un peu en train d'essayer de trouver les meilleures solutions à ça,
[13.86s - 17.24s]  Anthropic, OpenAI, et vous, évidemment, en tant qu'outsider,
[17.24s - 22.06s]  c'est quoi votre cible précise à vous en termes de possibilités d'évolution,
[22.52s - 25.04s]  quand on a des aussi gros acteurs à côté ?
[25.04s - 27.86s]  C'est quoi, toi, tu penses, la direction où vous avez un edge ?

[0.00s - 4.60s]  On a un fort edge dans le fait de découpler la question de l'infrastructure, de la question de l'interface.
[5.24s - 7.10s]  Notre solution peut être déployée partout.
[7.48s - 11.16s]  Elle peut être déployée dans le cloud, mais elle peut être déployée chez les entreprises qui ne sont pas dans le cloud.
[11.92s - 14.38s]  Elle peut être déployée sur des laptops.

[0.00s - 25.00s]  C'est le edge qu'on a construit aussi au-dessus de l'aspect open source, que ça va assez bien avec. Que les poids des modèles soient accessibles, ça rend facile leur déploiement n'importe où. On a cet aspect portabilité qui est très important. C'est notre première différenciation qu'on a beaucoup utilisée cette année. La différenciation qu'on cherche tous, c'est d'avoir la meilleure interface utilisateur. Il y a plein de sujets qui ne sont pas résolus. Le fait d'utiliser plein d'outils en même temps,
[25.12s - 27.52s]  le fait d'avoir des agents qui tournent pendant longtemps
[27.52s - 30.80s]  et qui prennent le feedback des utilisateurs.
[31.06s - 33.76s]  C'est-à-dire qu'on peut les voir comme des stagiaires,
[33.84s - 35.78s]  des stagiaires auxquels il faut faire du feedback
[35.78s - 37.62s]  pour qu'ils deviennent de plus en plus performants.
[38.20s - 41.48s]  Et donc, on va aller vers ce genre de système de plus en plus autonome
[41.48s - 43.12s]  qui vont avoir besoin de plus en plus de feedback
[43.12s - 46.94s]  pour passer de 80% de performance à 100% de performance.

[0.00s - 0.86s]  des stagiaires
[0.86s - 2.04s]  des stagiaires
[2.04s - 2.30s]  en qualité

[0.00s - 1.82s]  Comment ça se dit ? Tu ne restes pas constamment derrière lui
[1.82s - 3.24s]  et attendre qu'il avance ?

[0.00s - 5.34s]  Tu lui donnes une tâche, tu regardes ce qu'il a fait, tu lui dis ce qu'il n'a pas bien fait,

[0.00s - 1.60s]  J'espère que la prochaine fois, ils le fassent mieux.
[1.96s - 4.50s]  Mais en fait, c'est plein de questions scientifiques qu'il faut résoudre pour que ça fonctionne.
[4.84s - 5.54s]  Et d'interface.
[5.70s - 6.34s]  Et d'interface, oui.
[6.74s - 7.68s]  C'est pas du mail, en fait.

[0.00s - 1.28s]  C'est pas du mail en fait
[1.28s - 2.20s]  Est-ce qu'on va pas
[2.20s - 3.02s]  Pour l'instant
[3.02s - 3.64s]  C'est du chat
[3.64s - 4.54s]  En mode temps réel
[4.54s - 6.00s]  Est-ce qu'à terme
[6.00s - 6.86s]  On va envoyer un mail
[6.86s - 8.22s]  A notre assistante

[0.00s - 1.64s]  Et juste, il nous ping quand il a fini.

[0.00s - 2.18s]  C'est une des formes, je pense que c'est plutôt
[2.18s - 4.02s]  l'assistant qui t'envoie un mail, en fait, à un moment donné.
[4.16s - 6.32s]  Tu y travailles, et puis toutes les deux heures,
[6.42s - 7.66s]  il te dit, voilà où j'en suis, etc.
[8.08s - 9.52s]  Donc oui, il y a un aspect passé
[9.52s - 11.28s]  du synchrone à la synchrone,
[11.72s - 14.02s]  qui est très important, et qui pose plein de questions
[14.02s - 15.40s]  d'interface, parce que

[0.00s - 1.48s]  Bon, le mail, c'est peut-être pas la meilleure interface.
[1.72s - 4.28s]  Il y en a certainement d'autres qui sont plus intelligentes.
[4.64s - 6.72s]  La question de quelle est l'interface pour donner le feedback,
[6.90s - 9.26s]  quelle est l'interface pour sélectionner ce qui est préférable pour l'humain ?

[0.00s - 0.70s]  il y en a
[0.70s - 1.40s]  certainement d'autres

[0.00s - 0.74s]  ça c'est
[0.74s - 1.84s]  on y travaille

[0.00s - 0.12s]  Ouais.

[0.00s - 1.90s]  J'allais me dire, je suis persuadé que le...
[1.90s - 2.92s]  Enfin, je sais pas, mais...

[0.00s - 0.18s]  Oui.

[0.00s - 2.46s]  quand on regarde le chat, la discussion,

[0.00s - 3.52s]  Ce n'est pas forcément l'interface ultime pour dialoguer avec un LLM.

[0.00s - 1.16s]  Ça a beaucoup évolué maintenant.
[1.94s - 4.82s]  Tu peux te chatter avec le chat et il peut décider de te mettre dans un document.
[5.02s - 7.02s]  Et là, tu travailles avec lui sur la construction d'un document ?

[0.00s - 7.40s]  Tu peux lui demander d'aller chercher des sources et tu vois les sources, tu peux retourner, tu peux aller regarder ce que des humains ont écrit et demander des résumés par exemple.
[7.96s - 13.98s]  Et donc en fait ce que ça crée, ce que ça permet, les AI génératifs, c'est une espèce de liquidité de ta manière d'accéder à la connaissance.

[0.00s - 0.90s]  Donc tu peux regarder

[0.00s - 3.20s]  tout un site web et tu peux dire condense moi ce site web en

[0.00s - 1.14s]  en deux phrases

[0.00s - 1.84s]  Et je pense qu'il reste encore beaucoup de choses

[0.00s - 2.30s]  à faire pour que le modèle te permette d'apprendre beaucoup plus vite
[2.30s - 3.80s]  et de charger de la connaissance beaucoup plus vite.

[0.00s - 3.96s]  Je crois que c'est Vercel qui avait fait des démos assez marrantes de composants

[0.00s - 0.46s]  Web ?

[0.00s - 1.02s]  qui se construisait

[0.00s - 1.78s]  En fonction de la nécessité ?

[0.00s - 0.78s]  tu te poses une question,
[1.10s - 1.84s]  et il te générait,
[1.92s - 2.80s]  sur la météo par exemple,
[2.90s - 4.04s]  il te générait un composant
[4.04s - 6.04s]  d'interface graphique

[0.00s - 0.52s]  À la volée quoi !

[0.00s - 0.60s]  Ils voient le budget.
[0.74s - 1.14s]  Ouais, c'est ça.
[1.28s - 3.42s]  En fait, la question, c'est une question en back-end et en fontaine.
[3.58s - 6.72s]  En back-end, c'est quel outil appeler pour aller chercher de l'information
[6.72s - 7.92s]  ou pour actionner des choses ?

[0.00s - 2.78s]  Et en front-end, c'est quelle interface il faut montrer à l'utilisateur,
[2.88s - 4.00s]  étant donné son intention actuelle.

[0.00s - 3.24s]  Et ce que ça veut dire, c'est que les gros logiciels avec 50 000 boutons,
[3.24s - 5.88s]  je pense notamment au montage, ça va progressivement disparaître
[5.88s - 8.52s]  parce que tu peux identifier son état d'esprit

[0.00s - 2.14s]  au moment où il est en train de créer, et adapter les boutons,
[2.54s - 4.16s]  lui donner exactement ce dont il a besoin.

[0.00s - 3.98s]  Et donc ça change vraiment complètement la manière dont les interfaces vont se comporter dans les années qui viennent.

[0.00s - 3.04s]  Justement, on parlait de cette interface, de communication,

[0.00s - 3.68s]  comment on y accède. Tu parlais du fait que vous êtes déployable un peu de n'importe où.
[3.68s - 7.96s]  Il y a un truc que je constate en parlant avec des gens autour de moi, c'est qu'on a un peu une génération

[0.00s - 1.90s]  d'employés d'entreprise frustrés
[1.90s - 3.82s]  actuellement parce que chez eux, ils peuvent
[3.82s - 5.70s]  utiliser des trucs incroyables
[5.70s - 7.80s]  genre les meilleurs modèles
[7.80s - 9.64s]  disponibles, ils vont sur OpenAI, etc.
[10.16s - 11.82s]  Une fois au travail, on leur interdit
[11.82s - 13.94s]  souvent l'utilisation des meilleurs
[13.94s - 15.94s]  outils et parfois ils se retrouvent
[15.94s - 17.90s]  avec des versions un peu bridées ou des
[17.90s - 19.26s]  copilotes. Ou avec rien du tout.
[19.34s - 21.06s]  Ou avec, pour la plupart du temps, rien du tout.
[21.64s - 22.34s]  Ça vient d'où ça ?

[0.00s - 0.78s]  Ou avec rien du tout.

[0.00s - 1.20s]  Ça vient du fait que
[1.20s - 3.64s]  les systèmes d'AI génératifs,
[3.78s - 4.56s]  ça touche beaucoup aux données.
[4.88s - 5.74s]  Et les données dans les entreprises,
[5.88s - 6.60s]  c'est quand même assez important.
[6.98s - 8.90s]  Et donc, c'est là-dessus
[8.90s - 9.98s]  que nous, on a cherché
[9.98s - 10.62s]  à trouver des solutions.
[11.02s - 11.48s]  Alors, faire en sorte
[11.48s - 12.38s]  que les données, elles restent
[12.38s - 13.18s]  dans l'entreprise,
[13.58s - 15.64s]  que nous, en tant que fournisseurs d'AI,
[15.78s - 17.00s]  on n'est pas à voir ces données-là.
[17.30s - 18.38s]  Ça permet...

[0.00s - 3.96s]  justement d'avoir le niveau de sécurité, le niveau de gouvernance dont tu as besoin sur les données.
[4.32s - 5.78s]  Et donc progressivement...

[0.00s - 1.46s]  on va résoudre ce problème.

[0.00s - 2.50s]  Et nous, je dirais que c'est un des problèmes essentiels qu'on cherche à résoudre,
[2.56s - 3.26s]  de faire en sorte que...

[0.00s - 2.04s]  l'IT dans les entreprises soit confortable
[2.04s - 3.90s]  pour amener le char à tous leurs employés
[3.90s - 5.02s]  et qu'ils arrêtent d'être frustrés.

[0.00s - 1.96s]  les exemples d'outils que tu donnais,
[2.32s - 3.94s]  il y a un truc qui revient,
[4.10s - 6.10s]  qu'on n'a pas explicité, mais qui est
[6.10s - 7.54s]  super important, c'est le

[0.00s - 1.12s]  la notion d'objectif,
[1.34s - 3.04s]  d'avoir des modèles qui sont capables

[0.00s - 3.06s]  d'effectuer des tâches et sur la route, d'arriver à créer

[0.00s - 0.84s]  des étapes
[0.84s - 1.60s]  et à appeler
[1.60s - 2.40s]  les bons outils

[0.00s - 1.70s]  comme le ferait un bon stagiaire.
[2.02s - 4.82s]  Tu n'as pas nécessairement à lui expliquer l'ensemble des étapes qu'il doit faire.
[5.54s - 9.20s]  Tu lui dis « Regarde les prochains vols pour New York et prends-en un ».

[0.00s - 2.12s]  Tu n'as pas besoin de lui expliquer étape par étape,
[2.66s - 4.34s]  seconde par seconde, ce qu'il doit effectuer.

[0.00s - 2.54s]  On a des modèles qui peuvent commencer à appeler des outils,

[0.00s - 4.38s]  qu'on sent un peu limité dans leur capacité à en utiliser plusieurs d'affilés notamment.
[4.88s - 6.38s]  Des trucs vraiment utiles, vraiment stylés.
[6.54s - 8.32s]  Comment tu penses que ça va évoluer ?
[8.32s - 9.86s]  Est-ce que c'est une...

[0.00s - 2.32s]  une frontière qui peut être bientôt franchie ?
[2.32s - 4.00s]  Est-ce que l'année prochaine, on aura résolu ce problème
[4.00s - 6.36s]  et on pourra faire 20 étapes avec beaucoup de fiabilité ?
[6.36s - 7.86s]  Ou est-ce qu'on est encore loin d'y arriver ?

[0.00s - 1.08s]  Je pense que c'est la frontière.
[2.08s - 3.72s]  Tout le monde essaie de la pousser,
[3.92s - 5.16s]  ça ne va pas se débloquer d'un coup.
[5.40s - 7.92s]  Parce qu'en fait, maîtriser un outil,
[8.20s - 9.28s]  ça prend du temps à un humain,
[9.38s - 10.28s]  ça prend aussi du temps à un modèle.
[10.28s - 12.04s]  Il faut des démonstrations,
[12.74s - 13.36s]  il faut du feedback,
[13.48s - 15.16s]  parce que la première fois, il va se tromper.
[15.82s - 17.82s]  Et une notion d'expertise qu'il faut distiller
[17.82s - 19.24s]  de l'entreprise vers les systèmes d'AI.
[19.62s - 22.12s]  Et ça, ça ne va pas se faire de manière magique.
[22.54s - 24.32s]  Il faut tous les systèmes en place.
[24.70s - 25.50s]  Il faut les métasystèmes,
[25.62s - 27.60s]  c'est-à-dire qu'il faut que les employés
[27.60s - 29.16s]  dans les entreprises soient capables de...

[0.00s - 3.24s]  de fournir du signal supplémentaire au système D.I. pour qu'il s'améliore.
[3.50s - 6.88s]  Donc ça va progresser, on va avoir de plus en plus d'outils utilisables en même temps
[6.88s - 8.62s]  et des modèles qui peuvent résonner de plus en plus.

[0.00s - 1.92s]  ça va être progressif.
[2.18s - 3.74s]  Et pour que ça marche vraiment très bien,
[3.92s - 5.50s]  il faut y mettre du sien, il faut investir dès maintenant.

[0.00s - 4.50s]  Pour illustrer ça, on voit que OpenAI, dans leur dernier modèle, dans les O1 et compagnie,

[0.00s - 3.00s]  Ce ne sont plus des améliorations significatives sur le modèle en lui-même,
[3.00s - 5.84s]  mais il tente des trucs de le faire boucler sur lui-même,
[5.84s - 7.96s]  faire des chaînes de pensée, je ne sais pas comment on dit en français.
[7.96s - 9.10s]  Chaîne de pensée, oui.

[0.00s - 0.30s]  ...

[0.00s - 1.00s]  c'est pas bien non ?
[1.00s - 1.64s]  ah c'est gros ça

[0.00s - 2.46s]  Est-ce que, selon toi, c'est un peu un signe que...

[0.00s - 1.82s]  on a atteint une sorte de plafond.
[2.08s - 5.10s]  C'est-à-dire que, justement, sur cette évolution exponentielle,
[5.28s - 8.38s]  on a bien optimisé par rapport à leur taille,
[8.84s - 9.76s]  la manière dont marchent les modèles.
[9.88s - 11.32s]  Maintenant, justement, il faut trouver autre chose.

[0.00s - 2.80s]  C'est un paradigme qui est de plus en plus saturé,
[2.84s - 3.88s]  je pense qu'il n'est pas encore saturé,
[3.96s - 5.44s]  qui est ce qu'on appelle le préentraînement,
[5.54s - 6.88s]  donc la compression de la connaissance humaine.

[0.00s - 2.72s]  D'une certaine manière, tu as une connaissance disponible

[0.00s - 1.68s]  humaine qui a une certaine taille,
[1.74s - 2.00s]  et à un moment,
[2.04s - 2.86s]  tu as fini de la compresser.
[3.16s - 4.56s]  Et c'est là où il faut aller chercher...

[0.00s - 7.74s]  du signal supplémentaire. Du coup, chaîne de pensée, utilisation de plusieurs outils, utilisation de signal experts dans les entreprises.
[8.30s - 11.92s]  Donc, il n'y a pas de saturation du système. On sait comment aller à l'étape suivante.
[12.14s - 16.70s]  Mais sur l'aspect pré-entraînement, oui, on commence à savoir bien le faire collectivement.
[16.86s - 20.98s]  Tout le monde sait à peu près faire la même chose. Et donc, ce n'est plus tellement là où est la compétition.

[0.00s - 5.48s]  La compétition, elle est sur les interfaces et la compétition, elle est sur avoir des modèles qui tournent pendant plus longtemps.

[0.00s - 0.22s]  Ok.

[0.00s - 2.64s]  Je trouve que c'est un peu dur de se faire un avis dessus, justement, quand on est...

[0.00s - 2.88s]  On ne maîtrise pas la stack scientifique
[2.88s - 4.78s]  derrière les transformeurs et compagnie.

[0.00s - 3.00s]  J'ai l'impression qu'il y a un peu ce débat entre est-ce que c'est juste une question
[3.00s - 8.26s]  de compute, de données qui va repousser cette barrière d'autonomie ?

[0.00s - 4.40s]  Est-ce que c'est vraiment un problème intrinsèque à la manière dont le modèle est designé ?
[4.40s - 11.20s]  Et que juste le fait que ce soit de la prédiction du prochain token qui peut avoir un petit pourcentage de partir en cacahuètes à chaque fois,
[11.56s - 15.80s]  ça rend nécessairement trop compliqué, trop difficile la planification long terme.
[16.08s - 20.18s]  Je sais que par exemple, il y a des gens comme Yann Lecay, on en parle souvent, qui sont un peu défenseurs de cette vision-là,
[20.22s - 23.14s]  que l'AGI, ou je ne sais pas comment on l'appelle, elle est cachée encore derrière...

[0.00s - 1.16s]  des découvertes scientifiques.

[0.00s - 8.90s]  Oui, c'est une bonne question. Ce qui est vrai, c'est que travailler sur des architectures qui induisent des biais de réflexion humaine, c'est souvent utile.
[9.34s - 19.78s]  Ça a été utile pendant les 12 dernières années de se dire comment nous on réfléchit, essayons d'écrire ça en mathématiques et de faire en sorte que les modèles copient un peu ce qu'on sait faire.

[0.00s - 4.60s]  Ce qu'on observe aussi, c'est que toute l'intelligence qu'on peut mettre dans les architectures...

[0.00s - 2.06s]  Il suffit de mettre deux fois plus de compute et ça disparaît.
[2.50s - 5.60s]  Donc, en fait, le paradigme qu'on a suivi dans les cinq dernières années,
[5.70s - 7.98s]  c'est plutôt de se dire, prenez une architecture extrêmement simple
[7.98s - 9.50s]  qui prédit des séquences...

[0.00s - 3.16s]  et passons là à l'échelle, allons chercher le plus de données possibles,
[3.44s - 6.34s]  allons chercher les données multimodales, allons chercher de l'audio, ce genre de choses.

[0.00s - 0.70s]  Et...

[0.00s - 1.68s]  Et passons-la à l'échelle et voyons ce que ça donne.
[2.02s - 5.12s]  Et en fait, ce que ça donne, c'est que c'était en tout cas plus intelligent,
[5.42s - 8.24s]  en termes d'allocation de ressources, de travailler sur un passage à l'échelle
[8.24s - 10.32s]  que de travailler sur des architectures subtiles.

[0.00s - 2.00s]  Alors est-ce que c'est toujours le cas maintenant,
[2.00s - 4.84s]  comment ça va avoir saturé la quantité de données qu'on s'est compressé ?

[0.00s - 0.96s]  Je pense que la question est ouverte.

[0.00s - 1.76s]  le sujet, ce n'est plus tellement une question d'architecture,
[1.96s - 3.14s]  c'est plutôt une question d'orchestration.
[3.32s - 3.70s]  C'est-à-dire...

[0.00s - 2.28s]  Comment on fait pour que les modèles se rappellent eux-mêmes ?

[0.00s - 1.42s]  qu'ils interagissent avec des outils,
[1.54s - 5.86s]  qu'ils durent longtemps, qu'ils fassent du raisonnement en plusieurs étapes.

[0.00s - 2.66s]  Et ça, ça reste les mêmes modèles au fond.

[0.00s - 3.50s]  C'est la brique de base, mais le système complet, ce n'est pas juste le modèle.
[3.62s - 6.50s]  C'est le modèle qui sait se rappeler lui-même, qui sait réfléchir,
[6.58s - 9.36s]  qui sait interagir avec tout son environnement, qui sait interagir avec les humains.
[10.00s - 12.78s]  Donc la complexité des systèmes, elle devient beaucoup plus grande
[12.78s - 15.06s]  que juste un simple modèle de génération de séquences.

[0.00s - 2.58s]  Ça reste le moteur, mais c'est pas du tout toute la voiture.

[0.00s - 2.66s]  Mais donc, tu es plutôt optimiste sur le fait que ce soit le bon moteur.
[2.94s - 3.76s]  C'est le bon moteur.
[4.06s - 6.94s]  Après, il y a une règle en machine learning qui dit
[6.94s - 9.22s]  essentiellement, augmenter la capacité de calcul,
[9.38s - 12.38s]  ça augmente la qualité des systèmes.
[12.80s - 13.90s]  Et tu as deux solutions pour le faire.
[14.00s - 16.32s]  Soit tu compresses de la donnée, soit tu fais de la recherche.
[16.50s - 17.64s]  C'est-à-dire que tu vas échantillonner,
[18.06s - 19.84s]  tu vas demander au modèle de tester mille trucs
[19.84s - 22.26s]  et de sélectionner l'échantillon qui marche le mieux
[22.26s - 24.10s]  et tu vas le renforcer là-dessus.

[0.00s - 2.42s]  et donc là on commence de plus en plus à basculer
[2.42s - 4.24s]  dans le mode recherche plutôt que dans le mode compression
[4.24s - 5.88s]  la personne qui a dit ça c'est
[5.88s - 7.88s]  Richard Sutton dans un

[0.00s - 2.32s]  blog post que je vous invite à lire qui s'appelle The Bitter Lesson.

[0.00s - 1.82s]  Est-ce que toi, il y a une démo

[0.00s - 0.92s]  un peu de bout en bout,

[0.00s - 1.14s]  d'un truc qui a

[0.00s - 1.90s]  même si parfois ça ne marche pas,
[1.90s - 3.76s]  mais d'un truc où tu as été impressionné,
[3.76s - 5.48s]  où ça a vraiment très bien marché,

[0.00s - 4.86s]  d'une suite d'étapes, un truc qui t'a fait un peu sentir comme Iron Man avec Jarvis ?

[0.00s - 3.12s]  Oui, avec le chat, on a connecté les API ouvertes de Spotify.
[3.50s - 6.90s]  Et donc, tu peux lui parler, lui demander des playlists et décrire ta playlist.
[7.08s - 8.90s]  Puis, ça te crée ta playlist et ça la joue pour toi.
[9.12s - 10.46s]  Donc, ça fait des choses intéressantes.
[10.54s - 11.52s]  Alors, c'est juste un seul outil, ça.
[12.04s - 14.06s]  Non, là, on a vu des choses très intéressantes.
[14.24s - 18.00s]  C'est une fois qu'on a connecté le web, ça te permet d'avoir toutes les informations en live.

[0.00s - 4.60s]  Très vite, tu peux te créer tes mémos pour savoir ce qu'il faut aller dire à tel client en fonction des informations qu'il a eues.

[0.00s - 5.48s]  Et donc la combinaison des outils, ensemble, ça fait émerger des cas d'usage que tu n'avais pas forcément prévus.
[5.70s - 9.28s]  Si tu as connecté le web, si tu connectes ton mail, tu peux faire plein de choses en même temps.
[9.70s - 14.40s]  Et si tu connectes ta connaissance interne et le web, tu peux combiner ces informations.

[0.00s - 1.74s]  de manière un peu imprévisible ?

[0.00s - 2.32s]  Et donc la quantité de cas d'usage que tu couvres

[0.00s - 3.30s]  et assez exponentielle au nombre d'outils.

[0.00s - 1.26s]  Et donc ça, c'est assez magique, je veux dire.

[0.00s - 2.66s]  Moi effectivement je trouve qu'il y a un côté un peu vertigineux
[2.66s - 4.56s]  On va pouvoir construire
[4.56s - 5.08s]  Des trucs de fou

[0.00s - 3.92s]  Mais du coup, ça fait que c'est un peu dur de s'imaginer, de se dire ça va ressembler
[3.92s - 6.26s]  à quoi concrètement, genre le métier de développeur de

[0.00s - 3.50s]  de quelqu'un qui doit faire des scénarios de LLM, ça ressemble à quoi ?

[0.00s - 4.76s]  Je dirais que c'est un outil qui augmente le niveau d'abstraction requis par les humains.
[5.64s - 11.04s]  En tant que développeur, tu vas continuer à réfléchir aux problèmes que tu cherches à résoudre pour tes utilisateurs.
[11.48s - 18.32s]  Tu vas continuer à réfléchir aux architectures au niveau qui remplissent tes contraintes, ton cahier des charges.
[18.84s - 22.44s]  Après, est-ce que tu vas continuer à coder tes applications en JavaScript ?
[22.44s - 25.38s]  Vrasemblablement, non, parce que les modèles arrivent bien à générer...

[0.00s - 2.90s]  des applications simples et des applications de plus en plus compliquées.
[3.40s - 7.52s]  Donc tous les sujets très abstraits qui vont nécessiter de la communication avec des humains.
[7.84s - 9.60s]  Le métier d'ingénieur, c'est aussi un métier de communication.
[9.78s - 11.54s]  Il faut aussi comprendre quelles sont les contraintes de chacun.
[12.14s - 14.10s]  Ça, ça ne va pas être facilement remplaçable.

[0.00s - 4.50s]  Mais en revanche, tout l'aspect « je t'aide à faire tes tests unitaires »,
[4.50s - 8.16s]  « je te fais ton application Pixel Perfect », à partir d'un design,

[0.00s - 6.98s]  Je pense que ça devait devenir de plus en plus automatisable pour juste coller au développeur.

[0.00s - 0.88s]  pour tous les métiers.

[0.00s - 3.44s]  Est-ce qu'on a une intuition de comment ça se fait que les modèles sont aussi sensibles

[0.00s - 0.36s]  au code,
[0.72s - 1.30s]  c'est pour se dire ?

[0.00s - 2.44s]  Par exemple, je veux un modèle qui est super fort en français et en anglais.

[0.00s - 3.04s]  Bon, qu'ils sachent, le Python et le JavaScript, a priori, ça n'est pas utile.
[3.44s - 5.46s]  Or, ce n'est pas du tout ce qu'on observe, de ce que j'ai compris.

[0.00s - 1.18s]  C'est une très bonne question.
[27.64s - 28.38s]  C'est vrai qu'on observe un genre de transfert. Entraîner son modèle sur beaucoup de codes, ça lui permet de raisonner mieux. Je ne suis pas le mieux placé pour en parler, il faudrait que ça soit plutôt Guillaume, mais la réalité, c'est que le code, ça a plus d'informations que le langage. Il y a plus de réflexion qui est passée dans le langage, c'est plus structuré. S'entraîner à générer du code, ça force le modèle à raisononner à plus haut niveau que l'entraîner à générer du texte.
[29.64s - 31.66s]  Et donc, il sait résonner sur du code, et donc quand il voit du texte,
[31.72s - 32.90s]  il sait aussi résonner sur du texte.
[33.32s - 35.54s]  Et c'est vrai qu'il y a ce transfert un peu magique,
[36.00s - 37.66s]  qui est, je pense, une des raisons pour lesquelles les modèles sont devenus
[37.66s - 38.96s]  largement meilleurs dans les deux dernières années.
[39.24s - 40.62s]  Ça sert aussi parce que

[0.00s - 0.48s]  En fait, le...

[0.00s - 2.26s]  Tu as beaucoup plus de codebase
[2.26s - 4.22s]  qui sont plus longs qu'un livre.
[4.34s - 5.84s]  Comprendre une codebase, c'est plus long que lire un livre.

[0.00s - 2.96s]  et donc un peu le maximum sur lequel tu peux t'entraîner

[0.00s - 3.48s]  pour faire un modèle qui comprend le contexte long

[0.00s - 2.22s]  c'est des livres du 19e siècle

[0.00s - 8.34s]  Et le maximum sur lequel tu peux t'entraîner pour faire du code, c'est des millions de lignes de projets open source.
[8.80s - 11.54s]  Et donc, c'est plus long et ton modèle peut résonner plus longtemps.

[0.00s - 1.04s]  C'est des millions de lignes.
[1.94s - 2.40s]  Oui, c'est ça.

[0.00s - 1.50s]  je pense que c'est une des intuitions

[0.00s - 1.90s]  Je te propose de parler maintenant un petit peu de

[0.00s - 0.36s]  T'allons.

[0.00s - 3.20s]  et des gens qui font que, justement, vous faites ce que vous faites.
[3.40s - 6.92s]  Déjà, pourquoi est-ce que vous avez décidé, à la base, de mettre Mistral à Paris ?

[0.00s - 0.78s]  Aujourd'hui, ça peut pas...

[0.00s - 1.40s]  paraître un peu plus évident,
[1.48s - 3.90s]  on sait que l'écosystème est super vivant,
[3.96s - 4.80s]  on va en parler,
[5.50s - 6.78s]  mais est-ce que c'était une décision,
[7.10s - 9.62s]  à cette époque-là, une décision évidente
[9.62s - 11.40s]  entre ce maître-là ou à San Francisco,
[11.78s - 14.54s]  même avec une boîte française ?

[0.00s - 2.12s]  On ne s'est même pas posé la question en réalité.
[2.40s - 4.58s]  Moi, je n'avais aucune envie de partir de Paris.
[4.86s - 7.36s]  Ma compagne est fonctionnaire, donc elle a quelques contraintes.
[7.84s - 11.50s]  Timothée, il n'avait aucune envie de partir de France aussi, et Guillaume non plus.
[12.00s - 16.56s]  Donc je pense qu'en réalité, si j'ai réfléchi, je crois qu'on ne s'est jamais posé la question.
[16.98s - 19.78s]  On savait aussi que les gens dans notre réseau, en fait, c'était des Parisiens.
[20.18s - 21.42s]  Des Parisiens, des gens à Londres aussi.
[21.72s - 24.10s]  Ces personnes qu'on pouvait recruter, c'était des gens locaux.
[24.34s - 26.70s]  Donc c'était une évidence de démarrer à Paris.
[27.30s - 27.50s]  Comment ça se fait ?

[0.00s - 0.24s]  que...

[0.00s - 0.26s]  Bye.

[0.00s - 4.66s]  Marie, particulièrement, fourmille autant de bons ingés et scientifiques ?

[0.00s - 1.94s]  Je pense parce qu'il y a un écosystème en machine learning.
[2.14s - 5.40s]  Il y a un écosystème avec l'INRIA, avec...

[0.00s - 2.74s]  l'académique d'un côté, et puis les laboratoires privés
[2.74s - 4.38s]  que Yann Lequin a contribué à créer.
[4.72s - 8.94s]  Un laboratoire FAIR, historiquement, qui a été créé en 2015, je pense.

[0.00s - 8.18s]  T'as DeepMind qui en réaction s'était installé là-bas et qui avait un énorme centre de compétences à Londres grâce à DeepMind.
[8.66s - 15.12s]  Et donc en fait ce qui fait les talents dans la tech c'est le fait que t'es une entreprise qui soit déjà passée par là avant, qui a grossi.

[0.00s - 3.88s]  et des gens qui ont appris dans cette entreprise là et qui ensuite essaiment.

[0.00s - 6.28s]  Et donc on a bénéficié de ça, on a bénéficié des entrepreneurs qui voulaient bien nous rejoindre et qui se sont formés là-bas.

[0.00s - 3.56s]  Très bon écosystème privé et très bon écosystème public aussi,
[4.02s - 8.78s]  parce qu'il y a beaucoup de nos chercheurs qui ont fait des thèses dans l'académique en France aussi.

[0.00s - 3.32s]  le résultat est assez dingue quand même parce que dans les conventions d'IA,
[3.52s - 7.52s]  j'ai vu Oliyama par exemple qui a organisé un meetup ou un truc comme ça et tu vois les images,

[0.00s - 1.86s]  On se dit pas que ça apparaît, quoi.
[1.98s - 3.70s]  Et en fait, ça bouge de fou, quoi.

[0.00s - 2.34s]  Il y a plein de gens dans ce domaine en particulier qui sont des Français.
[2.88s - 4.48s]  Yann Lequin est un Français, mais...

[0.00s - 26.62s]  En dessous des gens plus jeunes que lui, il y a beaucoup de gens qui ont fait... On a fait tout le mal, Sialon par exemple. Oui, par exemple. À Meta, Paris, il y a aussi des gens très forts. À DeepMind, Paris, il y a des... Je ne vais pas encore aussi débaucher pour l'instant. J'ai plein d'amis... Je pense qu'on a débauché les gens qu'on pouvait. Ah, c'est terminé, assez ! J'ai plein d'amis qui sont français et qui sont encore là-bas. Ils finiront peut-être par créer leur boîte. De manière générale, l'Europe et la France en particulier a les bonnes compétences.
[26.62s - 30.62s]  Il y a les bonnes écoles, il faut être fort en maths et fort en informatique pour faire

[0.00s - 0.28s]  C'est un truc de russes.

[0.00s - 0.44s]  Je suis plein d'eux.

[0.00s - 0.70s]  C'est plus cool.

[0.00s - 1.74s]  Ah c'est un idée assez

[0.00s - 2.28s]  scientifiques en intelligence artificielle
[2.28s - 4.40s]  et de fait on a les bons tuyaux
[4.40s - 4.80s]  de formation

[0.00s - 1.38s]  Et c'est quoi les arguments aujourd'hui ?
[1.38s - 3.60s]  Moi, je viens de sortir de ma thèse en ML.
[4.34s - 5.84s]  Qu'est-ce qui fait, d'après toi,
[6.28s - 8.00s]  que je vais plutôt décider d'avenir chez Mistral
[8.00s - 11.36s]  versus chez Meta ou chez DeepMind ?

[0.00s - 0.12s]  ...

[0.00s - 3.68s]  En sortant de master, à Paris, je pense qu'on est de loin le meilleur centre de formation.

[0.00s - 3.40s]  pour faire le cœur de la science en intelligence artificielle.

[0.00s - 2.24s]  Il n'y a pas de structure équivalente, même en Europe.

[0.00s - 6.00s]  sur ce qu'on fait. Il y a 50-60 scientifiques chez nous qui sont tous extrêmement bien formés.
[6.52s - 11.62s]  Et donc, en sortant de Master, je trouve que c'est le meilleur endroit parce qu'en six mois, ils sont formés.
[11.92s - 14.92s]  C'est aussi un excellent endroit pour nous parce qu'on va récupérer des gens juniors.
[15.24s - 18.62s]  On a une bonne marque avec eux, on leur propose une excellente formation.

[0.00s - 2.02s]  Ils nous rejoignent et en fait, ils sont très performants au bout de 6 mois.
[2.30s - 6.24s]  Et ils ont plein d'idées, ils sont extrêmement créatifs, ils ont 23 ans, ils ont toute l'énergie qu'il faut.
[6.72s - 11.92s]  Et donc, on les utilise, on doit beaucoup de ce qu'on a proposé à des gens qui sont très jeunes.

[0.00s - 2.18s]  et donc on encourage tous les gens en master
[2.18s - 4.34s]  à venir nous rejoindre, on a plein de places
[4.34s - 5.18s]  et on adore les former

[0.00s - 4.92s]  J'ai appris tout à l'heure d'ailleurs qu'après notre émission qu'on avait faite sur Mistral, il y a eu une recrue.
[5.22s - 7.06s]  Tu te souviens ? Tu ne sais pas quel genre de profil ?
[7.06s - 8.06s]  Si, si, si, c'est un ingénieur.

[0.00s - 0.66s]  Ce n'est pas un certain...

[0.00s - 1.14s]  tu fixer un ingénieur.
[1.16s - 1.60s]  Si tout va bien,
[1.72s - 2.02s]  on n'a pas un peu.
[2.02s - 2.96s]  Oui, très content.
[3.18s - 3.70s]  Ok, parfait.
[3.96s - 4.82s]  Tu conseillerais quoi, toi,
[4.94s - 6.16s]  à des gens qui sont peut-être
[6.16s - 7.22s]  moins avancés
[7.22s - 8.78s]  ou qui réfléchissent
[8.78s - 9.78s]  à ce qu'ils veulent faire,
[9.86s - 10.40s]  qui voient bien
[10.40s - 10.94s]  qu'il y a quelque chose
[10.94s - 12.32s]  qui se passe dans l'IA ?
[12.32s - 13.30s]  Est-ce que, d'après toi,
[13.42s - 14.62s]  c'est trop tard, entre guillemets,
[14.68s - 15.42s]  dans le sens où...

[0.00s - 0.52s]  Ça va ?

[0.00s - 0.98s]  C'est quoi ?

[0.00s - 2.70s]  ça va être un écosystème très saturé ?
[2.70s - 4.14s]  Ou est-ce que tu penses que c'est encore possible ?
[4.14s - 7.26s]  Et qu'est-ce que tu conseillerais comme choix d'études,
[7.36s - 8.46s]  de trucs à explorer ?
[8.46s - 10.40s]  Est-ce qu'il faut faire des modèles de langage comme tout le monde ?
[10.40s - 11.30s]  Est-ce que justement...

[0.00s - 2.34s]  il faut essayer de sortir un petit peu de la bulle.
[2.72s - 3.26s]  C'est quand on...

[0.00s - 0.10s]  de

[0.00s - 2.52s]  Je pense qu'il faut réfléchir aux systèmes qu'on crée avec.
[25.96s - 28.46s]  Nous, on crée des systèmes de gestion de la connaissance, mais il y a plein de systèmes verticalisés dans les sciences de la vie, dans l'architecture, dans la conception par ordinateur, qui ne demandent qu'à être pris avec des nouveaux systèmes. Le montage vidéo aussi. Et donc, se dire que la technologie va continuer à avancer, les technologies horizontales vont être de plus en plus performantes. Ça ouvre de nouvelles opportunités d'automatisation, de nouvelles opportunités
[28.46s - 29.80s]  de création de logiciels
[29.80s - 32.02s]  intéressants, de création de services
[32.02s - 34.14s]  complètement différents pour les utilisateurs.
[34.48s - 35.94s]  Et donc partir des besoins d'utilisateurs,
[36.06s - 36.58s]  partir de...

[0.00s - 2.38s]  un peu du rêve du logiciel de demain,
[2.98s - 4.72s]  la machine auquel on va parler
[4.72s - 6.92s]  et qui va faire tout ce qu'on veut à sa place.
[7.44s - 8.54s]  Je pense que c'est un bon point de départ ?

[0.00s - 4.16s]  on choisit sa verticale et puis on se dit est-ce que les modèles aujourd'hui peuvent résoudre ce problème
[4.44s - 8.48s]  s'ils peuvent pas résoudre ce problème peut-être qu'il faut les personnaliser peut-être qu'il faut aller un petit peu plus profondément
[8.76s - 15.14s]  et là dessus on peut aider on peut aider à le faire on a tous les outils qui permettent de faire les personnalisations verticalisées. Nous on n'a pas

[0.00s - 3.24s]  On a parlé avec Microsoft des conséquences que ça avait dans la recherche de matériaux,
[3.24s - 4.24s]  par exemple.
[4.24s - 7.80s]  Et tu te dis, mais c'est quoi la base ? On parle de modèles qui génèrent des images,
[7.80s - 9.36s]  du texte, etc.

[0.00s - 1.72s]  Grâce à ça, entre autres, on découvre des matériaux.
[1.82s - 4.30s]  Est-ce qu'il y a des trucs comme ça, soit dans vos partenaires actuels ?

[0.00s - 2.86s]  ou dans les trucs que toi t'as vu qui t'ont aussi frappé ?

[0.00s - 2.08s]  La découverte de matériaux, c'est les mêmes techniques,
[2.20s - 2.88s]  ce n'est pas les mêmes modèles.
[3.18s - 4.78s]  Parce qu'en fait, ce qu'on fait, c'est qu'on séquence,
[4.90s - 6.22s]  de manière générale, il y a plein de problèmes
[6.22s - 9.58s]  qui se résolvent en séquençant, en sérialisme,
[9.58s - 11.72s]  de manière générale, le problème.
[12.04s - 14.10s]  Et la découverte de matériaux, c'est de la chimie,
[14.20s - 16.16s]  donc on peut sérialiser les molécules chimiques
[16.16s - 18.04s]  et demander au modèle de les prédire.
[18.34s - 21.62s]  Après, le paradigme, je prédis des séquences
[21.62s - 22.76s]  et j'ai plein de séquences sous la main
[22.76s - 24.66s]  de données particulières,
[25.10s - 27.66s]  ils marchent en chimie, ils marchent en médecine,

[0.00s - 1.70s]  Il marche certainement en sciences de la vie aussi.
[2.20s - 2.92s]  Non, on ne travaille pas là-dessus,
[3.02s - 5.48s]  mais on est ravis d'être partenaires
[5.48s - 7.24s]  avec des startups qui font ça, oui.

[0.00s - 4.68s]  Et là où, peut-être, comme tu disais, on arrive à une fin d'exponentiel éventuellement sur le texte,
[4.88s - 7.34s]  il y a peut-être plein d'autres domaines où, en fait...

[0.00s - 2.46s]  Le transfert, il fait que commencer quoi ?

[0.00s - 2.34s]  Sur le texte, on n'est pas du tout à saturation,
[2.48s - 6.44s]  puisque la prochaine étape, c'est d'avoir des modèles qui appellent plein d'outils.
[6.70s - 10.08s]  Du coup, ils deviennent beaucoup plus intelligents que juste des générateurs de texte.
[10.40s - 12.36s]  Et un des outils qu'ils peuvent appeler, c'est des simulateurs, par exemple.
[12.36s - 16.70s]  Donc la boucle de travail du concepteur de turbines,
[17.60s - 19.80s]  de la personne qui travaille sur un nouveau médicament,
[19.86s - 21.36s]  elle va complètement changer dans les années qui viennent.
[21.78s - 23.80s]  Et ça vient à la fois de modèles spécialisés,

[0.00s - 1.74s]  de prédiction de molécules par exemple,

[0.00s - 3.48s]  Mais ça vient aussi de l'interface avec la connaissance qu'on est en train de recréer.

[0.00s - 2.30s]  Tu vas pouvoir parler avec ton ordinateur et dire
[2.30s - 4.54s]  « Est-ce que tu peux me simuler une turbine qui ressemble à celle-là,
[4.60s - 6.24s]  mais un peu plus grande ? »

[0.00s - 1.48s]  et fais-moi plusieurs essais
[1.48s - 2.92s]  et dis-moi celle qui marcherait la mieux

[0.00s - 1.34s]  Et donc, la manière d'itérer,
[1.88s - 3.68s]  l'ingénierie, finalement, ça va aussi
[3.68s - 5.20s]  beaucoup changer de nature dans les années qui viennent, oui.

[0.00s - 2.94s]  C'est hyper intéressant parce qu'effectivement, les trucs auxquels on pense rapidement,
[3.16s - 5.32s]  c'est, tu parlais de simulateur, il y a le simulateur de code tout simplement.
[5.54s - 6.48s]  Ça, c'est des trucs qui se font un peu déjà.
[6.54s - 9.22s]  C'est le plus facile à simuler puisque ça reste au sein de la machine.
[9.66s - 10.54s]  Il n'y a pas de monde extérieur.
[11.00s - 12.56s]  Et donc, on voit déjà un peu le...

[0.00s - 2.20s]  une sandbox, par exemple, qui peut exécuter un bout de code
[2.20s - 3.92s]  et on voit si ça marche, si ça marche pas.
[4.20s - 4.84s]  Ça, c'est des trucs qui existent.

[0.00s - 3.62s]  Le raisonnement et le fait d'avoir un modèle qui génère du code qui est ensuite exécuté,
[3.86s - 5.12s]  on y travaille pour très prochainement.
[5.76s - 9.52s]  Et donc, c'est une grosse porte d'entrée vers des cas d'usage complexes.
[9.88s - 11.58s]  Tu as un espace ouvert d'outils à appeler maintenant,
[11.92s - 13.48s]  parce que tes outils, c'est tes librairies.
[13.86s - 15.78s]  Le niveau de contrôle aussi que tu dois mettre sur les modèles,
[16.22s - 18.68s]  ça devient plus compliqué à évaluer,
[18.80s - 20.24s]  parce que c'est plus ouvert, plus le monde est ouvert,
[20.38s - 21.36s]  plus c'est difficile à évaluer.
[21.68s - 22.58s]  Mais clairement, on va vers ça.
[22.88s - 24.98s]  Un modèle avec un exécuteur de code,
[25.20s - 27.74s]  c'est beaucoup plus performant qu'un modèle sans exécuteur de code.

[0.00s - 0.38s]  Dès qu'on crée,

[0.00s - 2.74s]  on commence à parler d'outils, d'exécuter des trucs du code.

[0.00s - 3.94s]  Moi, j'entends tout de suite d'autres profils dans l'écosystème de l'IA
[3.94s - 6.08s]  qui sont dans la frange très...

[0.00s - 1.90s]  très un peu alarmiste de

[0.00s - 1.66s]  de comment pourrait évoluer l'IA,
[2.14s - 5.28s]  et des possibilités que l'IA devienne rogue,
[5.78s - 6.86s]  et prenne son indépendance.
[7.00s - 8.34s]  Tu parles d'exécuter du code ?

[0.00s - 2.06s]  elle sortirait, on l'imagine sortir de la boîte.
[2.34s - 3.50s]  C'est quoi, toi, ton avis là-dessus ?

[0.00s - 1.92s]  Pour être caricatural, c'est un peu...

[0.00s - 2.94s]  un moyen de mettre en place des régulations pour...

[0.00s - 0.78s]  pour canasser tout ça,
[0.98s - 2.52s]  ou est-ce qu'il y a vraiment une des...

[0.00s - 0.88s]  des questions à se poser

[0.00s - 2.62s]  Il y a des questions à se poser sur comment on fait du nouveau logiciel
[2.62s - 5.26s]  avec des modèles qui sont fondamentalement...

[0.00s - 2.20s]  imprévisible, c'est-à-dire que par essence
[2.20s - 4.22s]  ce qu'il génère c'est aléatoire ?

[0.00s - 1.36s]  ça dépend de l'entrée

[0.00s - 2.42s]  Tu peux pas, en tant qu'humain, prédire ce que le modèle va sortir.

[0.00s - 1.24s]  Tu veux quand même faire du logiciel avec.
[1.90s - 3.20s]  Un logiciel, par essence,
[3.56s - 5.02s]  avant de le mettre à disposition du marché,
[5.12s - 7.02s]  tu veux vérifier qu'il couvre tous tes cas
[7.02s - 8.40s]  et qu'il n'y a pas de bug.
[8.92s - 11.62s]  Et donc la question de faire en sorte que ton système
[11.62s - 13.66s]  qui repose sur des LLM n'ait pas de bug,
[13.72s - 14.32s]  c'est une question difficile.

[0.00s - 2.00s]  C'est une question de contrôle, c'est une question d'évaluation.

[0.00s - 2.06s]  et finalement pour nous la question de sûreté

[0.00s - 1.20s]  en anglais c'est safety,
[1.72s - 3.00s]  c'est d'abord une question d'évaluation,
[3.52s - 5.08s]  c'est d'abord une question d'avoir les bons outils pour
[5.08s - 7.34s]  vérifier que ça fonctionne, et si ça
[7.34s - 9.00s]  fonctionne pas, avoir les bons outils pour
[9.00s - 11.04s]  corriger ça. Un des outils qu'on met à dispo,

[0.00s - 5.98s]  et qui est utile pour ça, c'est si tu veux contraindre l'espace des possibles de ton modèle et de ton système,
[6.38s - 8.08s]  tu contrains l'espace des entrées.

[0.00s - 3.84s]  Donc tu mets une modération sur le type de questions que l'utilisateur peut poser.
[4.10s - 5.48s]  Et comme ça, soudain tu passes de

[0.00s - 34.34s]  Un système qui peut répondre à toutes les questions, un système qui peut répondre que aux questions qui sont intéressantes pour le logiciel que tu crées. Pour nous, on voit vraiment l'aspect sûreté au niveau du système lui-même, et comme un problème d'intégration continue, comme un problème de test, il faut répondre à la question comment on construit des logiciels déterministes qui tournent sur des systèmes fondamentalement stochastiques. Ça, c'est la première chose. Ensuite, oui, il y a de la science-fiction, et puis, tu as quelques entreprises aux Etats-Unis qui ont un intérêt à dire aux régulateurs « Écoutez, cette technologie, elle est quand même un peu trop compliquée, un peu trop difficile à comprendre, un petit peu trop dangereuse.
[34.44s - 41.12s]  Imaginez que le truc devienne indépendant. » Bon, tu vas dire ça à des gens qui ne comprennent pas forcément exactement ce qui se passe. Ils peuvent se dire « Non, c'est pas mal. »

[0.00s - 3.36s]  Ah oui, peut-être que si on donnait ça à trois personnes...

[0.00s - 1.16s]  ou à deux personnes
[1.16s - 2.30s]  aux Etats-Unis
[2.30s - 3.14s]  on contrôlerait
[3.14s - 3.88s]  tout ce qui se passe
[3.88s - 4.76s]  et puis il n'y a pas de problème quoi

[0.00s - 1.72s]  Mais nous, on pense que c'est faux, c'est-à-dire que...

[0.00s - 4.34s]  avoir deux entités ou encore pire une entité qui contrôle tous
[5.04s - 9.80s]  tous les systèmes et qui ouvre sa porte à des auditeurs auxquels ils montrent ce qu'ils veulent
[9.96s - 12.48s]  on pense que c'est pas la bonne solution la bonne solution

[0.00s - 1.36s]  en sûreté logicielle.

[0.00s - 25.98s]  C'est l'open source de manière générale. On l'a montré en cyber, on l'a montré sur les systèmes les plus fiables aujourd'hui, les operating systems les plus fiables, c'est Linux. Le fait d'avoir le plus d'yeux possible sur une technologie, le fait de la distribuer le plus possible, c'est une manière de faire en sorte que le contrôle de cette technologie soit sous un contrôle démocratique. Et donc, nous c'est ce qu'on dit. Et puis quand on entend les doomers raconter autre chose, il y a des gens qui sont de bonne foi, il faut le reconnaître, qu'ils ont vraiment peur
[25.98s - 27.44s]  que ce sont des choses qui vont se passer.
[27.72s - 29.60s]  Et puis il y a surtout beaucoup de gens qui ne sont pas du tout de bonne foi.
[29.96s - 31.66s]  Je pense qu'il est important
[31.66s - 32.72s]  de vérifier

[0.00s - 15.82s]  Ça ne doit pas être simple parce qu'en face, l'argument, il est super facile à comprendre justement quand tu disais pour quelqu'un qui n'est pas forcément adepte du sujet, on te dit voilà un outil dangereux, est-ce qu'il ne faudrait pas éviter qu'on le mette dans trop de mains ? Tu pars déjà avec un truc un peu dur à défendre quoi.

[0.00s - 1.74s]  L'atout qu'on a, on a un atout historique.
[1.88s - 4.02s]  C'est-à-dire que ce n'est pas la première fois qu'on a ce débat.
[4.20s - 5.30s]  On a eu ce débat pour l'Internet.
[5.42s - 8.14s]  Internet, ça aurait pu être un truc contrôlé par trois entreprises
[8.14s - 9.86s]  qui auraient fait leur propre nette.

[0.00s - 0.72s]  Moi je trouve qu'on meurt.

[0.00s - 1.80s]  qui auraient refusé de standardiser les choses.
[2.28s - 4.60s]  Et en fait, finalement, il y a eu suffisamment de pression.
[5.08s - 6.72s]  À un moment donné, le régulateur s'est dit
[6.72s - 9.20s]  « Ouais, on va faire en sorte qu'elle soit standardisée. »
[9.20s - 10.72s]  Et donc, Internet, ça appartient à tout le monde maintenant.

[0.00s - 3.54s]  Il aurait suffi que des personnes différentes fassent des choix différents, quelques personnes,

[0.00s - 5.98s]  Et on serait dans une situation où, en fait, il y a trois wall gardens...

[0.00s - 0.94s]  non interopérables.

[0.00s - 2.60s]  Ça aurait pu être la même chose pour le end-to-end encryption.
[2.94s - 3.98s]  Ça, c'est un autre exemple.
[4.30s - 6.34s]  À une époque, c'était considéré comme une arme
[6.34s - 9.30s]  et il y avait un contrôle des exports des États-Unis.
[9.78s - 10.50s]  En fait, on est ce qu'il fallait.

[0.00s - 1.16s]  En fait, on se dit pas fou maintenant.

[0.00s - 2.08s]  Et en fait, on se pose la question maintenant sur les poids.
[2.68s - 5.52s]  Il y a parfois certains régulateurs qui se posent cette question-là.
[6.06s - 8.82s]  Mais ça paraît fou pour le end-to-end encryption.
[8.98s - 11.40s]  Nous, ce qu'on pense, c'est que dans dix ans, ça paraîtrait complètement fou pour des poids d'un modèle.
[11.76s - 16.20s]  Parce que c'est tellement infrastructurel, c'est tellement une ressource qui doit être partagée par tout le monde,
[16.56s - 18.68s]  cette compression de la connaissance et cette intelligence,
[19.18s - 22.98s]  que pour nous, c'est criminel de la laisser entre les mains de deux entités
[22.98s - 24.96s]  qui ne sont pas du tout sous contrôle démocratique.

[0.00s - 3.88s]  Et pour défendre cette vision-là que le contrôle doit avoir lieu un peu plus tard
[3.88s - 6.00s]  dans la chaîne, au moment de l'interface par exemple,
[6.00s - 8.50s]  ou par l'entreprise vis-à-vis de son client,

[0.00s - 0.98s]  Tu vas notamment au Sénat.

[0.00s - 3.48s]  On t'a vu sur YouTube parler au Sénat.
[3.98s - 4.94s]  Ça fait quoi de parler ?
[4.94s - 9.70s]  D'essayer d'expliquer ce que c'est un modèle, un dataset, un LLM à des sénateurs ?

[0.00s - 25.52s]  C'est intéressant. C'est un exercice. Il y avait des bonnes questions, peut-être posées par des gens qui comprenaient un peu moins la technologie, on va dire. Mais je pense que c'est important. De manière générale, c'est des représentants des citoyens et il faut qu'ils comprennent que c'est une technologie qui va affecter les citoyens. Donc nous, on est prêts à investir du temps parce que mieux c'est compris, plus on comprend que c'est aussi un enjeu de souveraineté, c'est aussi un enjeu culturel. C'est un enjeu d'avoir des acteurs comme nous,
[25.68s - 27.88s]  et pas que nous, mais des acteurs comme nous sur le sol européen.
[27.98s - 28.66s]  Parce que si ce n'est pas le cas,
[29.10s - 33.52s]  le sujet, c'est qu'on a une dépendance économique énorme aux États-Unis
[33.52s - 36.34s]  et ça, elle est très, très dommageable à long terme.

[0.00s - 10.50s]  Le fait d'aller parler aux gens qui font les lois, aux gens qui vont aussi parler avec leurs citoyens, comprennent leurs angoisses, c'est une manière de dédramatiser cette technologie.
[10.78s - 16.32s]  C'est une technologie qui va apporter beaucoup de bénéfices dans l'éducation, dans la santé, dans la manière dont on travaille.

[0.00s - 8.84s]  Et il faut que les représentants de la démocratie française, de la démocratie européenne, de la démocratie américaine, aient conscience, savent de quoi il s'agit.
[9.12s - 14.14s]  Moi, je dois dire que personnellement, je n'avais pas tellement prévu de faire ça en démarrant la boîte, mais il faut faire entendre savoir,
[14.30s - 20.66s]  parce que sinon, le vide est comblé par des gens qui n'ont pas forcément des intérêts alignés avec...

[0.00s - 3.26s]  la démocratie est certainement pas alignée avec ce que nous on essaie de faire.

[0.00s - 0.88s]  Si vous n'avez pas su,

[0.00s - 5.40s]  l'histoire du Vesuvius Challenge ou comment un papyrus a été décrypté par un étudiant en IA.
[5.40s - 7.60s]  Allez voir cette vidéo, c'était vraiment passionnant !

