**Transcriptions par locuteur**

[0.03s - 106.50s] SPEAKER_02 :  Aujourd'hui, on reçoit une légende de la tech française, Arthur Mench, cofondateur de Mistral AI, la seule entreprise d'Europe capable de tenir tête à OpenAI et aux GAFAM dans leur course à l'intelligence artificielle. En à peine un an, lui et ses deux associés ont réussi l'impossible, lever plus d'un milliard d'euros, développer des modèles d'IA qui rivalisent avec Chagipity et transformer leur start-up parisienne en une entreprise valorisée 6 milliards d'euros. Dans cet épisode exceptionnel, Arthur va nous dévoiler les secrets de cette success story, comment trois Français ont quitté leur job en or chez Google et Meta pour se lancer dans cette aventure folle, comment ils rivalisent avec des géants qui ont 100 fois plus de puissance de calcul qu'eux et surtout la guerre des talents qui fait rage en coulisses entre Mistral et les GAFAM pour attirer les meilleurs ingénieurs. On va aussi lui demander si d'après lui les modèles d'IA ont atteint un plafond et qu'est-ce qu'ils nous réservent pour la suite. Je suis très excité et honoré de pouvoir vous partager cette conversation avec Arthur Mech. Mais juste avant, j'ai un message pour tous ceux qui hésitent à prendre un abonnement chat GPT. Notre partenaire du jour, Mammouth Ayaï eu une idée assez brillante. Rassembler tous les meilleurs modèles d'IA dans une seule interface et derrière un unique abonnement. Pour 10 euros par mois, vous avez accès aux modèles de langages les plus récents, O1, Grock, DeepSync, et même des modèles de génération d'images comme Midjournay ou Flux. Quand on sait que cumuler tous ces abonnements, ça coûterait dans les 80-100 euros, c'est assez imbattable. Si vous avez besoin de générer beaucoup d'images par mois, ils ont aussi des plans un peu plus chers. Le truc cool, c'est qu'ils sont toujours à l'affût des nouvelles sorties. Par exemple, ils ont déjà flux pour la génération d'images. Et globalement, c'est juste agréable de ne pas avoir à changer d'interface tout le temps. Je vous mets le lien vers leurs différentes formules en description et on reprend. C'est quoi l'élément déclencheur déjà pour se dire « on va créer notre propre société face à ces géants » quand on est déjà bien installé.  confortable.
[106.58s - 155.75s] SPEAKER_00 :  Je pense qu'il y a deux conversations, une en septembre 2022 avec Timothée et une en novembre 2022 à NeurIPS, qui est la grosse conférence de machine learning avec Guillaume, où on s'est rendu compte qu'on avait des aspirations similaires de lancer une entreprise...  en France et qu'on connaissait pas mal de gens que ça intéresserait.  Et donc à partir de là, c'est un peu le début de l'engrenage. C'est-à-dire qu'au début, tu te dis « Ah, c'est peut-être une bonne idée ». Et puis au fur et à mesure, chaque jour qui passe, tu t'impliques de plus en plus émotionnellement dans cette idée. Puis à un moment donné, tu as un peu un point de non-retour parce qu'en fait, tu es plus dans l'idée que dans le travail dans ton entreprise actuelle.  Mais à partir de février, c'est vrai qu'on s'est dit, là on peut avoir 15 personnes, on peut aller vite, on sait le faire, on peut démontrer que l'Europe peut faire des choses intéressantes dans le domaine et peut reprendre une position de leadership. Et donc c'est comme ça que ça s'est fait, et à partir d'avril, on s'est lancé, quoi.
[156.24s - 170.79s] SPEAKER_02 :  Donc, Tau, il y a déjà cette idée que le projet, c'est de faire de l'IA très performante européenne  plus ça que  Juste, on se sent un petit peu peut-être ralenti par une grosse structure au-dessus de nous, donc Meta ou Google, et on pense aller plus vite tout seul. C'était quoi ?
[170.79s - 199.97s] SPEAKER_00 :  Non, tu avais les deux. En fait, Guillaume, Timothée et moi, on travaillait sur ce sujet depuis à peu près 2020. Et on a vu ce qu'on pouvait faire avec des petites équipes très concentrées. C'est vrai qu'en 2022, ces équipes sont devenues moins concentrées parce que c'est le moment où le monde a réalisé qu'il y avait une opportunité économique autour des modèles de langue. Et donc, on s'est dit qu'on pouvait bénéficier aussi de cet aspect de désorganisation pour nous être mieux organisés et fournir des choses plus rapidement.  Ça se passe comment le tout début ?
[200.46s - 205.74s] SPEAKER_02 :  Vous avez chacun une spécialité ? Comment vous vous organisez au tout début de la boîte ?
[205.89s - 207.21s] SPEAKER_00 :  On vient tous les trois du même...
[207.21s - 207.85s] SPEAKER_02 :  Même formation.
[207.54s - 253.26s] SPEAKER_00 :  De la même formation, on a fait la même chose, on a tous les trois des thèses en machine learning. C'est vrai qu'on s'est rapidement spécialisé avec Guillaume, qui est le scientifique le plus fort d'entre nous, qui a pris la partie scientifique. Timothée, qui est plus un ingénieur et qui s'est occupé de faire toute l'infrastructure et de monter l'équipe d'ingénieurs produits aussi. Et moi, j'ai assez vite fait l'aspect lever de fond, l'aspect parler avec des clients...  C'est des choses que j'aimais bien faire, on s'est répartis comme ça assez vite. Et pour revenir à comment ça démarre, ça démarre par une levée de fonds, parce qu'il faut la capacité de calcul et il faut la capacité humaine pour avancer assez vite. Et donc on a fait une levée de fonds en quelques semaines, et à partir de là on était partis pour faire le premier modèle en septembre.
[253.31s - 263.04s] SPEAKER_01 :  C'est-à-dire qu'il n'y a pas une seule ligne de code, en fait ?  Avant même de savoir que c'est bon, il y a une levée de fonds qui va se faire. C'est un domaine où il faut forcément attendre la levée de fonds. Si on veut commencer la première...
[263.60s - 266.07s] SPEAKER_00 :  On peut un peu paralléliser, commencer à faire du code.
[263.65s - 264.60s] SPEAKER_01 :  un peu paralléliser ?
[266.35s - 282.70s] SPEAKER_00 :  À trois, tu n'as pas beaucoup de levier. Il vaut mieux avoir une petite équipe d'une dizaine de personnes pour aller plus vite. On a commencé par la data, parce qu'il faut la donner pour entraîner les modèles. Il y a beaucoup de travail manuel là-dessus.  à faire et Guillaume, Timothée essentiellement avaient commencé  pendant qu'on finissait la levée.  OK.
[282.97s - 296.37s] SPEAKER_02 :  on parle des levées de fonds. Toi, tu as fait Polytechnique, Centra de Paris, l'UNS et un doctorat. Est-ce que ça aide quand même à lever des fonds alors que vous n'êtes que trois ? Ou alors, est-ce que c'est encore plus le non-méta Google ? Tu dirais que c'est quoi qui aide le plus ?
[296.83s - 323.37s] SPEAKER_00 :  Je pense que ce qui a aidé au démarrage, c'est qu'on était crédibles sur le domaine le plus chaud du moment en 2023 et qu'on avait plutôt des papiers qui étaient liés à ce domaine-là. C'est-à-dire que moi, j'étais dans l'équipe qui travaillait à DeepMind là-dessus. Guillaume et Timothée, ils étaient à Meta, c'est eux qui ont fait de la mâle le premier. Et donc cette crédibilité-là, ce n'est pas ce qu'on a fait dans notre jeunesse à l'école. C'est plutôt une crédibilité scientifique qu'on a construite dans notre première partie de carrière, on va dire.
[323.66s - 328.74s] SPEAKER_02 :  Un alignement de planète avec ce qui intéresse le plus et les meilleures personnes pour le développer, en fait.
[328.77s - 338.61s] SPEAKER_00 :  Oui, effectivement, notre crédibilité venait aussi du fait qu'on avait une excellente équipe de démarrage et qu'on pouvait démontrer qu'on saurait la recruter.
[339.07s - 364.75s] SPEAKER_02 :  Et il y a ce jour qui arrive, c'est le 27 septembre 2023, où vous postez un lien sur votre compte Twitter parfaitement inactif, et c'est votre premier modèle en fait. Donc Mistral 7B.  Le tweet est vu plus d'un million de fois. Vous êtes repris par tous les médias américains, tout le monde de l'IA.  et en effervescence et s'amuse avec le modèle. Il est téléchargé un million de fois, mais super vite.  Nous, on a vu ça de l'extérieur.  on a vu cet engouement. Vous,  De l'intérieur, c'était comment ?
[364.75s - 447.78s] SPEAKER_00 :  Le tweet, c'était une idée de Guillaume, le chief scientist, pour rendre à César ce qui appartient à César. Parce que vous ne le publiez pas comme les autres. On ne le publie pas comme les autres. Effectivement, on a mis à disposition un magnet link qui permet de le télécharger en BitTorrent. C'est comme ça qu'on a parlé la première fois et c'était une excellente idée. C'était une journée où on avait aussi prévu de faire de la communication plus habituelle. Moi, j'étais allé parler aux journalistes Figaro etc et donc il fallait mettre le torrent le matin et puis l'embargo était vers 16h donc il y avait cette période où en fait on avait rompu l'embargo mais bon les journalistes a priori allaient pas comprendre ce qui se passait donc ça se passait bien c'est moi qui postais, je crois c'était à 5h du mat donc j'avais mis un petit réveil parce que j'étais pas sûr du schedule send de Twitter ça s'appelait encore Twitter à l'époque et je l'ai mis et après je suis allé me recoucher et ensuite on a vu que ça avait bien démarré au démarrage c'est un truc où vous vous y attendiez un petit peu ou quand  On savait que le modèle était bon On savait qu'on était largement au-dessus Des meilleurs modèles open source Qu'on avait visé explicitement cette taille-là Parce qu'on savait que ça tournait sur des laptops aussi Donc ça voulait dire que Tous les hobbyistes allaient pouvoir jouer avec Et ça n'a pas manqué, ça a fonctionné Donc on se doutait qu'on allait être remarqué Ce qu'on ne se doutait pas C'est que les gens allaient le mettre dans des perroquies en peluche Et ce genre de choses en un mois La réception était plus grande que ce qu'on espérait On était très contents
[447.78s - 471.03s] SPEAKER_02 :  Il y a un autre truc qui s'est passé nécessairement en publiant des modèles avec des poids ouverts comme ça. C'est que ça laisse la porte à tout ce qui est entraînement, fine tuning. Et tout le monde s'en est donné à cœur joie. Je pense que c'était déjà un peu le cas sur les modèles de l'Yama. Mais je me souviens que c'est un modèle qui a été beaucoup, beaucoup réentraîné. C'est quoi les...  Les...  fine-tuning un peu étonnant ou curieux dont tu te souviens de ce modèle-là ou d'autres ? Il y a quelqu'un qui s'est
[471.03s - 499.97s] SPEAKER_00 :  qui nous avait entraîné ce modèle pour parler aux morts.  Je ne sais plus comment ça s'appelait mais il avait fait un fine tuning un petit peu ésotérique et le truc marchait relativement bien donc c'était assez marrant. C'est vrai que cette taille là c'est aussi une taille où tu peux fin tuner même sur des gros PC de gaming éventuellement et puis ça ne coûte pas très cher et ça permet de rentrer du style, ça permet de faire du roleplay et donc les gens s'en sont donné à coeur de joie effectivement.  pour expliquer.
[499.97s - 510.31s] SPEAKER_02 :  Il y a le modèle de fondation qui est lui le plus coûteux et le plus compliqué. Et j'imagine qu'il contient en gros l'information. Et après, le fine tuning, c'est conversationnel, c'est en faire un bon agent de discussion.
[510.45s - 525.97s] SPEAKER_00 :  Oui, il faut voir la première phase comme une compression de la connaissance humaine, et la deuxième phase comme une manière d'instruire le modèle à suivre ce qu'on lui demande de faire. Donc on le rend contrôlable, et une manière de le contrôler, c'est de le rendre...  de le rendre conversationnel.  Donc ces deux phases-là sont assez distinctes, effectivement.  Et est-ce que sur cette deuxième
[525.97s - 533.03s] SPEAKER_02 :  phase, il y a des trucs d'indépendants tout seuls qui ont testé des choses sur du fine tuning et ont découvert des bonnes personnes.
[533.03s - 577.12s] SPEAKER_00 :  technique ? Ouais, on a appris des trucs, je ne vais pas rentrer dans les détails, mais il y avait de direct preference optimization, c'est un peu du jargon, mais qu'on n'avait pas fait sur le premier modèle et on a vu des gens le faire, on s'est dit ça devrait bien marcher sur le deuxième modèle et ça a bien marché sur le deuxième modèle. Maintenant on fait d'autres choses, mais effectivement une des raisons pour lesquelles on a lancé la boîte au-delà de l'Europe, c'est aussi l'aspect ouvert et l'aspect contribution de la communauté. C'est  En fait, l'EI entre 2012 et 2022, ça s'est construit les uns par-dessus les autres pendant les conférences, les grosses boîtes par-dessus les grosses boîtes. Puis soudain, quand c'est devenu...  un modèle économique intéressant les gens ont arrêté les grosses entreprises ont arrêté et donc on a essayé de prolonger ça un peu avec ce qu'on a fait
[572.70s - 591.92s] SPEAKER_02 :  Réalisé.  Oui, aujourd'hui, tu as vraiment deux camps distincts. C'est assez particulier sur, d'un côté, les Anthropik, OpenAI et compagnie qui ne publient plus grand-chose. Google aussi, j'ai l'impression, a beaucoup ralenti les publications. Et de l'autre côté, les Chinois, bizarrement.
[586.07s - 586.23s] SPEAKER_00 :  Donc...
[592.19s - 596.06s] SPEAKER_02 :  Pourquoi les Chinois, ils sont autant à fond dans les modèles open source ? C'est curieux, non ?
[596.44s - 613.71s] SPEAKER_00 :  Je pense qu'ils sont en position de challenger. Est-ce que l'open source, c'est une bonne stratégie de challenger ? On en est, je pense, la bonne illustration. Je pense qu'ils ont des bonnes techniques, ils ont des bons renseignements aussi, c'est vrai.  Mais ils ont beaucoup fait avancer la science, les nouvelles techniques. C'est clairement ceux qui publient le plus, effectivement.
[613.79s - 618.85s] SPEAKER_01 :  Et tu parlais de la position de challenger, est-ce que Meta, quand il publie Yama pour la première fois, ils sont en position de challenger à ce moment-là ?
[619.04s - 643.58s] SPEAKER_00 :  C'est les Timothée et Guillaume. Je pense qu'ils sont dans la position de challenge parce qu'ils n'ont pas encore parlé.  Et je pense qu'avec le mouvement qu'on a perpétué avec nos modèles en septembre et en décembre en particulier, donc Mistral 7B, Mistral 8X 7B, je pense qu'on a lancé cette route l'open source. Et donc il y a aussi un peu une concurrence sur qui fait les meilleurs modèles open source. Je pense que ça bénéficie à tout le monde. Et donc on est ravi d'avoir bien participé à ça.
[642.01s - 644.44s] SPEAKER_01 :  On va nous avoir  Oui, c'est vrai.
[643.58s - 643.80s] SPEAKER_02 :  C'est...
[643.80s - 643.96s] SPEAKER_00 :  ...
[643.96s - 655.30s] SPEAKER_02 :  ...  La régalade. Qu'est-ce qui fait, tu penses qu'à ce moment-là, vous avez autant d'avance, vous ? Après, il y a un yo-yo avec tout le monde qui se produit, mais là, il y a une vraie avance sociale.  Indiscutable quoi.
[655.30s - 676.95s] SPEAKER_00 :  Je pense qu'on connaissait l'importance de la donnée et on a beaucoup travaillé là-dessus. On savait aussi comment entraîner le modèle de manière efficace parce qu'on avait trois ronds d'expérience chacun dans ce domaine.  des bonnes connaissances et on a insisté sur les aspects de l'entraînement qui ont le plus de leviers c'est-à-dire la qualité de la donnée.
[677.12s - 685.44s] SPEAKER_02 :  Effectivement, c'est derrière un peu tout...  l'évolution de la recherche, j'ai l'impression de temps vert. En fait, il n'y a que la donnée qui compte.  Grosse partie.
[684.94s - 687.37s] SPEAKER_00 :  la donnée et la quantité de calcul.
[687.59s - 720.83s] SPEAKER_02 :  Il y a aussi le compute, et c'est lié à un autre sujet très important, c'est  les fonds, tout simplement. En un an, vous avez en tout levé quand même un milliard d'euros, ce qui est vertigineux. Vous avez aussi sorti plein de nouveaux modèles, des Pixral, par exemple, des modèles un peu différents, multimodaux, etc. Comment vous approchez le fait que, justement, en termes de quantité de compute, par rapport à un méta, par exemple, qui a  qui aura à la fin de l'année 350 000 H...  100, c'est ça ? Oui. Si je ne dis pas de bêtises. En GPU. Est-ce que justement, il n'y a pas le choix que de passer par des très grosses levées de fonds ?  Mais après, comme on pérennise le truc, c'est quoi un peu ta vision du compute ?
[720.95s - 742.97s] SPEAKER_00 :  Nous, notre vision, c'est qu'on a besoin de compute, mais on n'a pas besoin de 350 000 à champs.  Et donc ça a été toujours notre test qu'on pouvait être plus efficace, qu'on pouvait en étant focalisé sur le fait de faire des excellents produits et ne pas faire plein d'autres choses à côté. Parce que nos concurrents américains ont tendance à faire beaucoup de choses à côté. L'allocation de ressources, c'est une question importante.
[742.97s - 751.17s] SPEAKER_02 :  constante chez nous. C'est un peu le nerf de l'aïr quoi. C'est arriver à tenir l'amélioration des modèles dans le temps versus le cramage du compute.
[751.44s - 770.41s] SPEAKER_00 :  Il faut gérer le budget, il faut être intelligent pour ne pas dépenser trop. Tout est une question de mettre le curseur au bon endroit et d'avoir les bons compromis. Ce n'est pas facile, mais je pense que pour le moment, on a bien réussi. On a réussi à avoir des modèles qui sont très performants, avec un niveau de dépense de capital qui est quand même très contrôlé.
[770.41s - 792.03s] SPEAKER_02 :  Ouais.  Est-ce que justement, j'ai vu que parmi vos investisseurs  dans les derniers rounds, je crois, il y a NVIDIA. Est-ce que ça passe par des acteurs qui, eux, ont un peu le contrôle sur le hardware, ou l'infrastructure, ou les data centers ? Il y a Microsoft aussi, je crois, avec qui vous avez bossé. Est-ce que ça passe aussi par ça, justement, s'entourer des bonnes personnes ?  Faut les bons partners
[792.03s - 816.97s] SPEAKER_00 :  Il faut les bons partenaires de distribution en particulier parce que le calcul passe souvent par le cloud. Et donc on a comme partenaire tous les fournisseurs de cloud américains parce que c'est quand même les plus gros. On a aussi des fournisseurs français, on a Outscale qui en travaille. Et puis Nvidia c'est un fournisseur quasiment de cloud aussi. Donc à ce titre on travaille avec eux. On a fait de la R&D avec un modèle qui s'appelle Mistral Nemo.  Imaginez, il y a des gens qui nous
[816.97s - 825.03s] SPEAKER_02 :  écoutes qui n'ont pas su. Est-ce que tu peux nous expliquer c'est quoi aujourd'hui la  des modèles qui sont à jour. Moi, j'ai vu que dans les derniers mises à jour récents, il y a le large 2.
[825.27s - 862.77s] SPEAKER_00 :  Oui, alors maintenant on les numérote comme Ubuntu, donc 24.11. Et donc celui-là, Mistral Large 24.11, il est très fort pour appeler des fonctions, orchestrer des choses. Parce qu'en fait les modèles ça génère du texte, c'est l'utilisation de base. Mais ce qui est intéressant c'est quand ils génèrent des appels à des outils et qu'on les utilise comme des orchestrateurs, comme un peu des operating systems. Et donc on travaille beaucoup sur le fait d'avoir des modèles qui puissent être connectés à plein d'outils différents  auxquelles on peut poser des questions, auxquelles on peut donner des tâches et qui vont réfléchir d'eux-mêmes aux outils qu'ils vont appeler. Et donc on insiste beaucoup là-dessus. Et donc la nouvelle version de Mistral Large, elle est particulièrement forte là-dessus.
[862.83s - 863.12s] SPEAKER_02 :  Oui.
[863.12s - 863.96s] SPEAKER_00 :  Après, il y a...
[863.96s - 870.48s] SPEAKER_02 :  Il y a eu des mixtrales aussi. Ça, pour comprendre, c'est plutôt pour servir, pour une entreprise, par exemple, pour servir beaucoup d'utilisateurs en même temps ?
[870.48s - 892.18s] SPEAKER_00 :  C'est un autre type d'architecture qui est particulièrement pertinent quand on a une forte charge. Beaucoup d'utilisateurs, c'est des choses que nous on utilise par exemple. C'est des mixtures. C'est des mixtures. Une sorte de cerbère à 8 têtes. C'est ça, c'est plusieurs modèles en même temps et chaque mot passe sur le modèle le plus adapté. Pour plusieurs raisons, ça permet de mieux utiliser les GPUs.
[879.47s - 882.37s] SPEAKER_02 :  En ligne  parce qu'en fait  sur le cerveau.
[892.43s - 949.05s] SPEAKER_00 :  Et derrière, il y a les plus petits. Il y a les petits modèles qui passent sur les laptops, qui passent sur les smartphones. Et ceux-là, ils sont particulièrement adaptés à des usages de hobbyistes parce qu'il n'y a pas besoin d'aller dans le cloud, on peut les modifier facilement. Et puis, très vite, c'est aussi pas mal focalisé sur cet aspect petit et rapide parce que c'est vraiment l'ADN de l'entreprise. Nous, aujourd'hui, le produit, ce n'est pas le modèle. Le produit, c'est la plateforme pour les développeurs.  Et donc, ils choisissent s'ils veulent aller vite et être moins intelligents ou aller lentement et être plus intelligents, essentiellement. Et puis, l'autre produit, c'est le chat. Donc, c'est une solution plus front-end qui permet aux entreprises de gérer leurs connaissances, d'automatiser des choses, qui permet à tous les utilisateurs, vous pouvez le tester aujourd'hui, d'accéder au web, de discuter de l'information, de générer du code, de générer des images, de créer des documents. On a un mode où l'interface évolue en fonction de l'intention de l'utilisateur. Donc ça, c'est une nouvelle interface home machine et on investit beaucoup là-dessus.
[922.49s - 922.55s] SPEAKER_01 :  ...
[949.11s - 959.97s] SPEAKER_00 :  Donc le produit, c'est  la plateforme pour construire des applications en tant que développeur, et là-dedans il y a des modèles.  et puis un ensemble d'applications  qui permettent de gagner en productivité. C'est un milieu qui est super
[959.97s - 988.59s] SPEAKER_02 :  compétitif.  Évidemment, que ce soit, on l'a dit, sur les modèles, mais aussi sur tout ce qu'il y a autour, sur comment améliorer l'expérience, les interfaces de chat, etc. On a vu les systèmes d'interface qui se bougent. Tout le monde est un peu en train d'essayer de trouver les meilleures solutions à ça, Anthropic, OpenAI, et vous, évidemment, en tant qu'outsider, c'est quoi votre cible précise à vous en termes de possibilités d'évolution, quand on a des aussi gros acteurs à côté ? C'est quoi, toi, tu penses, la direction où vous avez un edge ?
[989.09s - 1050.94s] SPEAKER_00 :  On a un fort edge dans le fait de découpler la question de l'infrastructure, de la question de l'interface. Notre solution peut être déployée partout. Elle peut être déployée dans le cloud, mais elle peut être déployée chez les entreprises qui ne sont pas dans le cloud. Elle peut être déployée sur des laptops.  C'est le edge qu'on a construit aussi au-dessus de l'aspect open source, que ça va assez bien avec. Que les poids des modèles soient accessibles, ça rend facile leur déploiement n'importe où. On a cet aspect portabilité qui est très important. C'est notre première différenciation qu'on a beaucoup utilisée cette année. La différenciation qu'on cherche tous, c'est d'avoir la meilleure interface utilisateur. Il y a plein de sujets qui ne sont pas résolus. Le fait d'utiliser plein d'outils en même temps, le fait d'avoir des agents qui tournent pendant longtemps et qui prennent le feedback des utilisateurs. C'est-à-dire qu'on peut les voir comme des stagiaires, des stagiaires auxquels il faut faire du feedback pour qu'ils deviennent de plus en plus performants. Et donc, on va aller vers ce genre de système de plus en plus autonome qui vont avoir besoin de plus en plus de feedback pour passer de 80% de performance à 100% de performance.
[1036.68s - 1038.97s] SPEAKER_01 :  des stagiaires des stagiaires en qualité
[1050.89s - 1054.13s] SPEAKER_02 :  Comment ça se dit ? Tu ne restes pas constamment derrière lui et attendre qu'il avance ?
[1054.13s - 1067.51s] SPEAKER_00 :  Tu lui donnes une tâche, tu regardes ce qu'il a fait, tu lui dis ce qu'il n'a pas bien fait,  J'espère que la prochaine fois, ils le fassent mieux. Mais en fait, c'est plein de questions scientifiques qu'il faut résoudre pour que ça fonctionne. Et d'interface. Et d'interface, oui. C'est pas du mail, en fait.
[1066.23s - 1076.84s] SPEAKER_02 :  C'est pas du mail en fait Est-ce qu'on va pas Pour l'instant C'est du chat En mode temps réel Est-ce qu'à terme On va envoyer un mail A notre assistante  Et juste, il nous ping quand il a fini.
[1076.84s - 1101.55s] SPEAKER_00 :  C'est une des formes, je pense que c'est plutôt l'assistant qui t'envoie un mail, en fait, à un moment donné. Tu y travailles, et puis toutes les deux heures, il te dit, voilà où j'en suis, etc. Donc oui, il y a un aspect passé du synchrone à la synchrone, qui est très important, et qui pose plein de questions d'interface, parce que  Bon, le mail, c'est peut-être pas la meilleure interface. Il y en a certainement d'autres qui sont plus intelligentes. La question de quelle est l'interface pour donner le feedback, quelle est l'interface pour sélectionner ce qui est préférable pour l'humain ?
[1093.83s - 1095.24s] SPEAKER_01 :  il y en a certainement d'autres
[1101.85s - 1103.69s] SPEAKER_00 :  ça c'est on y travaille
[1104.08s - 1107.37s] SPEAKER_01 :  Ouais.  J'allais me dire, je suis persuadé que le... Enfin, je sais pas, mais...
[1105.39s - 1105.58s] SPEAKER_00 :  Oui.
[1107.86s - 1114.54s] SPEAKER_01 :  quand on regarde le chat, la discussion,  Ce n'est pas forcément l'interface ultime pour dialoguer avec un LLM.
[1114.59s - 1148.02s] SPEAKER_00 :  Ça a beaucoup évolué maintenant. Tu peux te chatter avec le chat et il peut décider de te mettre dans un document. Et là, tu travailles avec lui sur la construction d'un document ?  Tu peux lui demander d'aller chercher des sources et tu vois les sources, tu peux retourner, tu peux aller regarder ce que des humains ont écrit et demander des résumés par exemple. Et donc en fait ce que ça crée, ce que ça permet, les AI génératifs, c'est une espèce de liquidité de ta manière d'accéder à la connaissance.  Donc tu peux regarder  tout un site web et tu peux dire condense moi ce site web en  en deux phrases  Et je pense qu'il reste encore beaucoup de choses  à faire pour que le modèle te permette d'apprendre beaucoup plus vite et de charger de la connaissance beaucoup plus vite.
[1148.24s - 1164.41s] SPEAKER_02 :  Je crois que c'est Vercel qui avait fait des démos assez marrantes de composants  Web ?  qui se construisait  En fonction de la nécessité ?  tu te poses une question, et il te générait, sur la météo par exemple, il te générait un composant d'interface graphique  À la volée quoi !
[1164.41s - 1194.41s] SPEAKER_00 :  Ils voient le budget. Ouais, c'est ça. En fait, la question, c'est une question en back-end et en fontaine. En back-end, c'est quel outil appeler pour aller chercher de l'information ou pour actionner des choses ?  Et en front-end, c'est quelle interface il faut montrer à l'utilisateur, étant donné son intention actuelle.  Et ce que ça veut dire, c'est que les gros logiciels avec 50 000 boutons, je pense notamment au montage, ça va progressivement disparaître parce que tu peux identifier son état d'esprit  au moment où il est en train de créer, et adapter les boutons, lui donner exactement ce dont il a besoin.  Et donc ça change vraiment complètement la manière dont les interfaces vont se comporter dans les années qui viennent.
[1194.63s - 1228.87s] SPEAKER_02 :  Justement, on parlait de cette interface, de communication,  comment on y accède. Tu parlais du fait que vous êtes déployable un peu de n'importe où. Il y a un truc que je constate en parlant avec des gens autour de moi, c'est qu'on a un peu une génération  d'employés d'entreprise frustrés actuellement parce que chez eux, ils peuvent utiliser des trucs incroyables genre les meilleurs modèles disponibles, ils vont sur OpenAI, etc. Une fois au travail, on leur interdit souvent l'utilisation des meilleurs outils et parfois ils se retrouvent avec des versions un peu bridées ou des copilotes. Ou avec rien du tout. Ou avec, pour la plupart du temps, rien du tout. Ça vient d'où ça ?
[1225.04s - 1264.58s] SPEAKER_00 :  Ou avec rien du tout.  Ça vient du fait que les systèmes d'AI génératifs, ça touche beaucoup aux données. Et les données dans les entreprises, c'est quand même assez important. Et donc, c'est là-dessus que nous, on a cherché à trouver des solutions. Alors, faire en sorte que les données, elles restent dans l'entreprise, que nous, en tant que fournisseurs d'AI, on n'est pas à voir ces données-là. Ça permet...  justement d'avoir le niveau de sécurité, le niveau de gouvernance dont tu as besoin sur les données. Et donc progressivement...  on va résoudre ce problème.  Et nous, je dirais que c'est un des problèmes essentiels qu'on cherche à résoudre, de faire en sorte que...  l'IT dans les entreprises soit confortable pour amener le char à tous leurs employés et qu'ils arrêtent d'être frustrés.
[1264.73s - 1318.96s] SPEAKER_02 :  les exemples d'outils que tu donnais, il y a un truc qui revient, qu'on n'a pas explicité, mais qui est super important, c'est le  la notion d'objectif, d'avoir des modèles qui sont capables  d'effectuer des tâches et sur la route, d'arriver à créer  des étapes et à appeler les bons outils  comme le ferait un bon stagiaire. Tu n'as pas nécessairement à lui expliquer l'ensemble des étapes qu'il doit faire. Tu lui dis « Regarde les prochains vols pour New York et prends-en un ».  Tu n'as pas besoin de lui expliquer étape par étape, seconde par seconde, ce qu'il doit effectuer.  On a des modèles qui peuvent commencer à appeler des outils,  qu'on sent un peu limité dans leur capacité à en utiliser plusieurs d'affilés notamment. Des trucs vraiment utiles, vraiment stylés. Comment tu penses que ça va évoluer ? Est-ce que c'est une...  une frontière qui peut être bientôt franchie ? Est-ce que l'année prochaine, on aura résolu ce problème et on pourra faire 20 étapes avec beaucoup de fiabilité ? Ou est-ce qu'on est encore loin d'y arriver ?
[1319.01s - 1363.77s] SPEAKER_00 :  Je pense que c'est la frontière. Tout le monde essaie de la pousser, ça ne va pas se débloquer d'un coup. Parce qu'en fait, maîtriser un outil, ça prend du temps à un humain, ça prend aussi du temps à un modèle. Il faut des démonstrations, il faut du feedback, parce que la première fois, il va se tromper. Et une notion d'expertise qu'il faut distiller de l'entreprise vers les systèmes d'AI. Et ça, ça ne va pas se faire de manière magique. Il faut tous les systèmes en place. Il faut les métasystèmes, c'est-à-dire qu'il faut que les employés dans les entreprises soient capables de...  de fournir du signal supplémentaire au système D.I. pour qu'il s'améliore. Donc ça va progresser, on va avoir de plus en plus d'outils utilisables en même temps et des modèles qui peuvent résonner de plus en plus.  ça va être progressif. Et pour que ça marche vraiment très bien, il faut y mettre du sien, il faut investir dès maintenant.
[1364.24s - 1378.50s] SPEAKER_02 :  Pour illustrer ça, on voit que OpenAI, dans leur dernier modèle, dans les O1 et compagnie,  Ce ne sont plus des améliorations significatives sur le modèle en lui-même, mais il tente des trucs de le faire boucler sur lui-même, faire des chaînes de pensée, je ne sais pas comment on dit en français. Chaîne de pensée, oui.
[1378.09s - 1380.14s] SPEAKER_01 :  ...  c'est pas bien non ? ah c'est gros ça
[1380.91s - 1394.88s] SPEAKER_02 :  Est-ce que, selon toi, c'est un peu un signe que...  on a atteint une sorte de plafond. C'est-à-dire que, justement, sur cette évolution exponentielle, on a bien optimisé par rapport à leur taille, la manière dont marchent les modèles. Maintenant, justement, il faut trouver autre chose.
[1395.12s - 1437.06s] SPEAKER_00 :  C'est un paradigme qui est de plus en plus saturé, je pense qu'il n'est pas encore saturé, qui est ce qu'on appelle le préentraînement, donc la compression de la connaissance humaine.  D'une certaine manière, tu as une connaissance disponible  humaine qui a une certaine taille, et à un moment, tu as fini de la compresser. Et c'est là où il faut aller chercher...  du signal supplémentaire. Du coup, chaîne de pensée, utilisation de plusieurs outils, utilisation de signal experts dans les entreprises. Donc, il n'y a pas de saturation du système. On sait comment aller à l'étape suivante. Mais sur l'aspect pré-entraînement, oui, on commence à savoir bien le faire collectivement. Tout le monde sait à peu près faire la même chose. Et donc, ce n'est plus tellement là où est la compétition.  La compétition, elle est sur les interfaces et la compétition, elle est sur avoir des modèles qui tournent pendant plus longtemps.
[1437.24s - 1481.77s] SPEAKER_02 :  Ok.  Je trouve que c'est un peu dur de se faire un avis dessus, justement, quand on est...  On ne maîtrise pas la stack scientifique derrière les transformeurs et compagnie.  J'ai l'impression qu'il y a un peu ce débat entre est-ce que c'est juste une question de compute, de données qui va repousser cette barrière d'autonomie ?  Est-ce que c'est vraiment un problème intrinsèque à la manière dont le modèle est designé ? Et que juste le fait que ce soit de la prédiction du prochain token qui peut avoir un petit pourcentage de partir en cacahuètes à chaque fois, ça rend nécessairement trop compliqué, trop difficile la planification long terme. Je sais que par exemple, il y a des gens comme Yann Lecay, on en parle souvent, qui sont un peu défenseurs de cette vision-là, que l'AGI, ou je ne sais pas comment on l'appelle, elle est cachée encore derrière...  des découvertes scientifiques.
[1481.81s - 1613.45s] SPEAKER_00 :  Oui, c'est une bonne question. Ce qui est vrai, c'est que travailler sur des architectures qui induisent des biais de réflexion humaine, c'est souvent utile. Ça a été utile pendant les 12 dernières années de se dire comment nous on réfléchit, essayons d'écrire ça en mathématiques et de faire en sorte que les modèles copient un peu ce qu'on sait faire.  Ce qu'on observe aussi, c'est que toute l'intelligence qu'on peut mettre dans les architectures...  Il suffit de mettre deux fois plus de compute et ça disparaît. Donc, en fait, le paradigme qu'on a suivi dans les cinq dernières années, c'est plutôt de se dire, prenez une architecture extrêmement simple qui prédit des séquences...  et passons là à l'échelle, allons chercher le plus de données possibles, allons chercher les données multimodales, allons chercher de l'audio, ce genre de choses.  Et...  Et passons-la à l'échelle et voyons ce que ça donne. Et en fait, ce que ça donne, c'est que c'était en tout cas plus intelligent, en termes d'allocation de ressources, de travailler sur un passage à l'échelle que de travailler sur des architectures subtiles.  Alors est-ce que c'est toujours le cas maintenant, comment ça va avoir saturé la quantité de données qu'on s'est compressé ?  Je pense que la question est ouverte.  le sujet, ce n'est plus tellement une question d'architecture, c'est plutôt une question d'orchestration. C'est-à-dire...  Comment on fait pour que les modèles se rappellent eux-mêmes ?  qu'ils interagissent avec des outils, qu'ils durent longtemps, qu'ils fassent du raisonnement en plusieurs étapes.  Et ça, ça reste les mêmes modèles au fond.  C'est la brique de base, mais le système complet, ce n'est pas juste le modèle. C'est le modèle qui sait se rappeler lui-même, qui sait réfléchir, qui sait interagir avec tout son environnement, qui sait interagir avec les humains. Donc la complexité des systèmes, elle devient beaucoup plus grande que juste un simple modèle de génération de séquences.  Ça reste le moteur, mais c'est pas du tout toute la voiture.  Mais donc, tu es plutôt optimiste sur le fait que ce soit le bon moteur. C'est le bon moteur. Après, il y a une règle en machine learning qui dit essentiellement, augmenter la capacité de calcul, ça augmente la qualité des systèmes. Et tu as deux solutions pour le faire. Soit tu compresses de la donnée, soit tu fais de la recherche. C'est-à-dire que tu vas échantillonner, tu vas demander au modèle de tester mille trucs et de sélectionner l'échantillon qui marche le mieux et tu vas le renforcer là-dessus.  et donc là on commence de plus en plus à basculer dans le mode recherche plutôt que dans le mode compression la personne qui a dit ça c'est Richard Sutton dans un  blog post que je vous invite à lire qui s'appelle The Bitter Lesson.
[1613.67s - 1629.80s] SPEAKER_02 :  Est-ce que toi, il y a une démo  un peu de bout en bout,  d'un truc qui a  même si parfois ça ne marche pas, mais d'un truc où tu as été impressionné, où ça a vraiment très bien marché,  d'une suite d'étapes, un truc qui t'a fait un peu sentir comme Iron Man avec Jarvis ?
[1629.84s - 1677.96s] SPEAKER_00 :  Oui, avec le chat, on a connecté les API ouvertes de Spotify. Et donc, tu peux lui parler, lui demander des playlists et décrire ta playlist. Puis, ça te crée ta playlist et ça la joue pour toi. Donc, ça fait des choses intéressantes. Alors, c'est juste un seul outil, ça. Non, là, on a vu des choses très intéressantes. C'est une fois qu'on a connecté le web, ça te permet d'avoir toutes les informations en live.  Très vite, tu peux te créer tes mémos pour savoir ce qu'il faut aller dire à tel client en fonction des informations qu'il a eues.  Et donc la combinaison des outils, ensemble, ça fait émerger des cas d'usage que tu n'avais pas forcément prévus. Si tu as connecté le web, si tu connectes ton mail, tu peux faire plein de choses en même temps. Et si tu connectes ta connaissance interne et le web, tu peux combiner ces informations.  de manière un peu imprévisible ?  Et donc la quantité de cas d'usage que tu couvres  et assez exponentielle au nombre d'outils.  Et donc ça, c'est assez magique, je veux dire.
[1677.96s - 1694.13s] SPEAKER_02 :  Moi effectivement je trouve qu'il y a un côté un peu vertigineux On va pouvoir construire Des trucs de fou  Mais du coup, ça fait que c'est un peu dur de s'imaginer, de se dire ça va ressembler à quoi concrètement, genre le métier de développeur de  de quelqu'un qui doit faire des scénarios de LLM, ça ressemble à quoi ?
[1694.64s - 1749.97s] SPEAKER_00 :  Je dirais que c'est un outil qui augmente le niveau d'abstraction requis par les humains. En tant que développeur, tu vas continuer à réfléchir aux problèmes que tu cherches à résoudre pour tes utilisateurs. Tu vas continuer à réfléchir aux architectures au niveau qui remplissent tes contraintes, ton cahier des charges. Après, est-ce que tu vas continuer à coder tes applications en JavaScript ? Vrasemblablement, non, parce que les modèles arrivent bien à générer...  des applications simples et des applications de plus en plus compliquées. Donc tous les sujets très abstraits qui vont nécessiter de la communication avec des humains. Le métier d'ingénieur, c'est aussi un métier de communication. Il faut aussi comprendre quelles sont les contraintes de chacun. Ça, ça ne va pas être facilement remplaçable.  Mais en revanche, tout l'aspect « je t'aide à faire tes tests unitaires », « je te fais ton application Pixel Perfect », à partir d'un design,  Je pense que ça devait devenir de plus en plus automatisable pour juste coller au développeur.
[1749.97s - 1765.83s] SPEAKER_02 :  pour tous les métiers.  Est-ce qu'on a une intuition de comment ça se fait que les modèles sont aussi sensibles  au code, c'est pour se dire ?  Par exemple, je veux un modèle qui est super fort en français et en anglais.  Bon, qu'ils sachent, le Python et le JavaScript, a priori, ça n'est pas utile. Or, ce n'est pas du tout ce qu'on observe, de ce que j'ai compris.
[1766.00s - 1835.61s] SPEAKER_00 :  C'est une très bonne question. C'est vrai qu'on observe un genre de transfert. Entraîner son modèle sur beaucoup de codes, ça lui permet de raisonner mieux. Je ne suis pas le mieux placé pour en parler, il faudrait que ça soit plutôt Guillaume, mais la réalité, c'est que le code, ça a plus d'informations que le langage. Il y a plus de réflexion qui est passée dans le langage, c'est plus structuré. S'entraîner à générer du code, ça force le modèle à raisononner à plus haut niveau que l'entraîner à générer du texte. Et donc, il sait résonner sur du code, et donc quand il voit du texte, il sait aussi résonner sur du texte. Et c'est vrai qu'il y a ce transfert un peu magique, qui est, je pense, une des raisons pour lesquelles les modèles sont devenus largement meilleurs dans les deux dernières années. Ça sert aussi parce que  En fait, le...  Tu as beaucoup plus de codebase qui sont plus longs qu'un livre. Comprendre une codebase, c'est plus long que lire un livre.  et donc un peu le maximum sur lequel tu peux t'entraîner  pour faire un modèle qui comprend le contexte long  c'est des livres du 19e siècle  Et le maximum sur lequel tu peux t'entraîner pour faire du code, c'est des millions de lignes de projets open source. Et donc, c'est plus long et ton modèle peut résonner plus longtemps.
[1828.96s - 1831.36s] SPEAKER_02 :  C'est des millions de lignes. Oui, c'est ça.
[1835.73s - 1837.23s] SPEAKER_00 :  je pense que c'est une des intuitions
[1837.57s - 1863.42s] SPEAKER_02 :  Je te propose de parler maintenant un petit peu de  T'allons.  et des gens qui font que, justement, vous faites ce que vous faites. Déjà, pourquoi est-ce que vous avez décidé, à la base, de mettre Mistral à Paris ?  Aujourd'hui, ça peut pas...  paraître un peu plus évident, on sait que l'écosystème est super vivant, on va en parler, mais est-ce que c'était une décision, à cette époque-là, une décision évidente entre ce maître-là ou à San Francisco, même avec une boîte française ?
[1863.47s - 1890.98s] SPEAKER_00 :  On ne s'est même pas posé la question en réalité. Moi, je n'avais aucune envie de partir de Paris. Ma compagne est fonctionnaire, donc elle a quelques contraintes. Timothée, il n'avait aucune envie de partir de France aussi, et Guillaume non plus. Donc je pense qu'en réalité, si j'ai réfléchi, je crois qu'on ne s'est jamais posé la question. On savait aussi que les gens dans notre réseau, en fait, c'était des Parisiens. Des Parisiens, des gens à Londres aussi. Ces personnes qu'on pouvait recruter, c'était des gens locaux. Donc c'était une évidence de démarrer à Paris. Comment ça se fait ?
[1891.33s - 1891.57s] SPEAKER_02 :  que...
[1891.70s - 1891.97s] SPEAKER_00 :  Bye.
